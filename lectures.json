[
  {
    "id": "1743060933.131187",
    "subject": "Машинное обучение и большие данные",
    "date": "2025-02-21",
    "title": "Лекция 21.02.25",
    "content": "## Меры центральной тенденции\n\n### **Пример**: \n\nРассмотрим показатели силы ветра по шкале Бофорта:  \n$[0, 2, 2, 1, 1, 3, 3, 1, 1, 0, 0, 1, 2]$ \n\nВариационный ряд по возрастанию:  \n$[0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 3, 3]$  \n\nМедиана (Me) = 1 – тихий ветер.\n\n| Сила ветра | Описание |\n|------------|----------|\n| 0 Галлов  | Штиль    |\n| 1 Галл    | Тихий ветер |\n| 2 Галла   | Лёгкий ветер |\n| 3 Галла   | Слабый ветер |\n| 4 Галла   | Умеренный ветер |\n| 5 Галлов  | Свежий ветер |\n| 6 Галлов  | Сильный ветер |\n| 7 Галлов  | Крепкий ветер |\n| 8 Галлов  | Очень крепкий ветер |\n| 9 Галлов  | Шторм |\n| 10 Галлов | Сильный шторм |\n| 11 Галлов | Жестокий шторм |\n| 12 Галлов | Ураган |\n\n---\n\n## Проблема выбросов\n\n**Выбросы** – это значения, которые значительно отличаются от остальных данных. Они могут негативно повлиять на среднее значение, но медиана более устойчива к выбросам.\n\n- **Выбросы** могут быть вызваны ошибками в данных или редкими событиями.\n- Для обнаружения выбросов можно использовать визуализацию или сравнение медианы и моды.\n\n---\n\n## Меры разброса (изменчивости)\n\nМеры разброса описывают, насколько данные отклоняются от центральной тенденции.\n\n- **Max** – максимальное значение.\n- **Min** – минимальное значение.\n- **Range (Размах)** = Max - Min.\n- **Midrange** = (Max + Min) / 2.\n- **Inter-Quartile Range (IQR)** – межквартильный размах.\n- **Дисперсия** и **Стандартное отклонение**.\n\n---\n\n### Размах и квартильный размах\n\n**Размах** – разность между максимальным и минимальным значениями:  \n$$ R = X_{\\text{max}} - X_{\\text{min}} $$\n\n**Квартильный размах** – разница между третьим и первым квартилями:  \n$$ IQR = Q_3 - Q_1 $$\n\nКвартили делят данные на четыре равные части:\n-  $(Q_1) \\text{ - 25\\% квартиль}$\n- $( Q_2 ) \\text{ - 50\\% квартиль}$\n- $( Q_1 ) \\text{ - 25\\% квартиль}$\n\n---\n\n## Дисперсия и стандартное отклонение\n\n**Дисперсия** измеряет отклонение значений от среднего:  \n$$ \\sigma^2 = \\frac{\\sum_{i=1}^n (x_i - \\overline{x})^2}{n} $$\n\n**Стандартное отклонение** – это квадратный корень из дисперсии:  \n$$ \\sigma = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\overline{x})^2}{n}} $$\n\nСтандартное отклонение выражает изменчивость в исходных единицах измерения.\n\n---\n\n## Нормальное распределение\n\nНормальное распределение (колоколообразная кривая) характеризуется следующими свойствами:\n- Около 68% данных лежат в пределах одного стандартного отклонения от среднего:  \n  $$ \\overline{x} \\pm 1\\sigma $$\n- Около 95% данных лежат в пределах двух стандартных отклонений:  \n  $$ \\overline{x} \\pm 2\\sigma $$\n- Около 99.7% данных лежат в пределах трех стандартных отклонений:  \n  $$ \\overline{x} \\pm 3\\sigma $$\n\n---\n\n## Нормализация и стандартизация данных\n\n**Нормализация** – преобразование данных к диапазону $[0, 1]$:  \n$$ X_{\\text{norm}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} $$\n\n**Стандартизация** – преобразование данных к среднему значению 0 и стандартному отклонению 1:  \n$$ Z = \\frac{x_i - \\overline{x}}{s} $$\n\n---\n\n## Визуализация данных\n\n### Гистограмма\nГистограмма показывает распределение данных по интервалам (бинам). Она используется для непрерывных переменных.\n\n### Диаграмма рассеяния (Scatter Plot)\nДиаграмма рассеяния показывает взаимосвязь между двумя переменными. Она помогает выявить корреляции.\n\n### Тепловая карта (Heat Map)\nТепловая карта используется для визуализации корреляций между переменными. Коэффициент корреляции находится в интервале $[-1, 1]$.\n\n---\n\n## Обработка пропущенных значений\n\n1. **Удаление пропущенных значений** – если пропусков мало.\n2. **Замена средним значением** – если нет выбросов.\n3. **Замена медианой** – если есть выбросы.\n4. **Замена модой** – для категориальных данных.\n5. **Регрессия** – для прогнозирования пропущенных значений.\n\n---\n\n## Ключевые идеи\n\n- **Среднее значение** – типичное значение в наборе данных, но не устойчиво к выбросам.\n- **Медиана** – центральное значение, устойчиво к выбросам.\n- **Мода** – наиболее часто встречающееся значение.\n- **Дисперсия** и **стандартное отклонение** – меры разброса данных.\n- **Нормализация** и **стандартизация** – важные этапы предобработки данных.\n- **Визуализация** – помогает понять структуру и взаимосвязи в данных.\n\n---\n\n## Инструменты визуализации\n\n- **Matplotlib** – библиотека для построения графиков.\n- **Seaborn** – библиотека для статистической визуализации.\n\n---\n\n## Пример кода для стандартизации и нормализации\n\n```python\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nimport numpy as np\n\n# Пример данных\ndata = np.array([[1, 2], [3, 4], [5, 6]])\n\n# Стандартизация\nscaler = StandardScaler()\nstandardized_data = scaler.fit_transform(data)\n\n# Нормализация\nminmax_scaler = MinMaxScaler()\nscaled_data = minmax_scaler.fit_transform(data)\n\nprint(\"Стандартизированные данные:\\n\", standardized_data)\nprint(\"Нормализованные данные:\\n\", scaled_data)",
    "created_at": "2025-03-27T13:35:33.131187"
  },
  {
    "id": "1743060984.569695",
    "subject": "Машинное обучение и большие данные",
    "date": "2025-03-14",
    "title": "Лекция 14.03.25",
    "content": "## Введение в линейную регрессию\n\nЛинейная регрессия — это метод моделирования зависимости между зависимой переменной (откликом) и одной или несколькими независимыми переменными (предикторами). \n\nОсновная цель — построить математическую модель, которая наилучшим образом описывает связь между переменными.\n\n### Общее уравнение линейной регрессии:\n$$ y = ax + b $$\n\nВ Data Science и машинном обучении это уравнение часто записывается как:\n$$ Y = b_0 + b_1 X $$\n\n### Описание параметров:\n- $y$ или $Y$ — переменная ответа (зависимая переменная).\n- $x$ или $X$ — предикторная переменная (независимая переменная).\n- $a$ и $b$ или $b_0$ и $b_1$ — коэффициенты регрессии:\n\t  - $b_0$ — пересечение (intercept).\n\t  - $b_1$ — наклон (slope).\n\n## Ключевые термины\n\n- **Отклик (Response)**: Переменная, которую пытаемся предсказать. Cинонимы: зависимая переменная, целевая переменная.\n  \n- **Независимая переменная (Independent Variable)**: Переменная, используемая для предсказания отклика. Синонимы: предиктор, признак.\n  \n- **Запись (Record)**: Вектор, состоящий из значений предикторов и значений отклика для одного элемента данных. Синонимы: строка, случай, пример\n\n- **Пересечение (Intercept)**: Предсказанное значение, когда $X = 0$. Синонимы: $b_0$, точка пересечения.\n  \n- **Коэффициент регрессии (Regression Coefficient)**: Наклон регрессионной прямой. Синонимы: наклон, $b_1$, вес.\n\n## Линейные модели регрессии\n\nЛинейные модели регрессии используются для прогнозирования непрерывных величин. В зависимости от количества признаков, выделяют:\n\n- **Парная регрессия** (однофакторная):\n  $$ y = w x + b $$\n\n- **Множественная регрессия** (многофакторная):\n  $$ y = w_1 x_1 + w_2 x_2 + \\ldots + w_n x_n + b $$\n\n### Постановка задачи регрессии\n\nДано: коллекция размеченных данных $\\{(x_i, y_i)\\}_{i=1}^N$, где $N$ — размер коллекции, $x_i$ — $D$-мерный вектор признаков образца $i = 1, ..., N$, $y_i$ — действительное целевое значение.\n\nТребуется: построить модель $f_{w,b}(x)$, которая является линейной комбинацией признаков:\n$$ f_{w,b}(x) = xw + b $$\n\nгде:\n- $w$ — вектор параметров (весов).\n- $b$ — свободный коэффициент (сдвиг).\n\n## Метод наименьших квадратов (МНК)\n\nМетод наименьших квадратов (МНК) используется для нахождения оптимальных параметров линейной регрессии, минимизирующих сумму квадратов ошибок (остатков):\n\n$$ \\min_{w, b} \\sum_{i=1}^n (w^T x_i + b - y_i)^2 $$\n\n### Функция потерь (MSE)\nСреднеквадратичная ошибка (MSE) вычисляется как:\n$$ MSE = \\frac{1}{m} \\sum_{i=1}^m (y_i - \\hat{y}_i)^2 $$\n\n### Коэффициент детерминации $R^2$\nКоэффициент детерминации $R^2$ показывает, насколько хорошо модель объясняет дисперсию данных:\n$$ R^2 = 1 - \\frac{\\sum_{i=1}^n (\\hat{y}_i - y_i)^2}{\\sum_{i=1}^n (\\bar{y}_i - y_i)^2} $$\n\n## Регуляризация\n\nРегуляризация используется для предотвращения переобучения модели. Основные виды регуляризации:\n\n1. **L1-регуляризация (Lasso)**:\n   $$ \\min_{w, b} \\left[ \\frac{1}{N} \\sum_{i=1}^N (y_i - f_{w,b}(X_i))^2 + \\lambda \\sum_{j=1}^D |w_j| \\right] $$\n\n2. **L2-регуляризация (Ridge)**:\n   $$ \\min_{w, b} \\left[ \\frac{1}{N} \\sum_{i=1}^N (y_i - f_{w,b}(X_i))^2 + \\lambda \\sum_{j=1}^D w_j^2 \\right] $$\n\n3. **Elastic Net** (комбинация L1 и L2):\n   $$ \\min_{w, b} \\left[ \\frac{1}{N} \\sum_{i=1}^N (y_i - f_{w,b}(X_i))^2 + \\lambda_1 \\sum_{j=1}^D |w_j| + \\lambda_2 \\sum_{j=1}^D w_j^2 \\right] $$\n\n## Метрики качества модели\n\nДля оценки качества модели регрессии используются следующие метрики:\n\n- **MAE (Mean Absolute Error)**:\n  $$ MAE = \\frac{1}{m} \\sum_{i=1}^m |y_i - \\hat{y}_i| $$\n\n- **RMSE (Root Mean Squared Error)**:\n  $$ RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2} $$\n\n- **MAPE (Mean Absolute Percentage Error)**:\n  $$ MAPE = \\frac{1}{n} \\sum_{i=1}^n \\frac{|y_i - \\hat{y}_i|}{|y_i|} \\times 100\\% $$\n",
    "created_at": "2025-03-27T13:36:24.569695"
  },
  {
    "id": "1743061009.628635",
    "subject": "Машинное обучение и большие данные",
    "date": "2025-02-14",
    "title": "Лекция 14.02.25",
    "content": "## Ключевые идеи\n- **EDA** — итеративный процесс, который может потребовать возвращения к предыдущим этапам по мере получения новых инсайтов из данных.\n  - Это означает, что EDA не является линейным процессом. По мере анализа данных могут возникать новые вопросы, которые потребуют повторного изучения уже пройденных этапов.\n- Основная цель EDA — лучше понять данные и подготовить их для последующего анализа или моделирования.\n  - EDA помогает выявить закономерности, аномалии и взаимосвязи в данных, что важно для построения точных моделей.\n---\n## Размер и размерность данных\n- **Размер данных** относится к числу объектов данных.\n  - Например, в датасете с информацией о студентах размер данных будет равен количеству студентов.\n- **Размерность данных** относится к числу атрибутов (признаков).\n  - Например, если у каждого студента есть возраст, пол и оценка, то размерность данных будет равна 3.\n---\n## Типы данных\n### Числовые (Numeric)\n- Выражены числами (например, возраст, вес, размер обуви).\n  - **Непрерывные**: неограниченное число вариантов (например, возраст, вес, давление).\n    - Эти данные могут принимать любое значение в пределах определенного диапазона.\n  - **Дискретные**: ограниченное число вариантов (например, размер обуви, количество учеников).\n    - Эти данные принимают только целые значения.\n\n### Категориальные\n- Выражены словами (например, цвет глаз, пол, группа крови).\n  - **Порядковые (Ordinal)**: имеют иерархию (например, настроение, оценка качества обслуживания).\n    - Эти данные можно упорядочить, но разница между значениями не количественная.\n  - **Номинальные (Nominal)**: не имеют иерархии (например, цвет глаз, группа крови).\n    - Эти данные представляют собой просто категории без какого-либо порядка.\n---\n## Порядковые данные (Ordinal Data)\n- Значения имеют осмысленный порядок (например, оценки: A, B, C, D; размеры порций: маленькие, средние, большие).\n  - Например, оценка \"A\" лучше, чем \"B\", но мы не можем точно сказать, насколько лучше.\n- Количественной разницы между двумя уровнями не обнаружено.\n  - Это означает, что разница между \"A\" и \"B\" может быть не такой же, как между \"B\" и \"C\".\n---\n## Выбор признаков\n- Удаление признаков, не имеющих отношения к задаче (например, Student ID).\n  - Например, если мы анализируем доходы студентов, то идентификатор студента (Student ID) не имеет значения для анализа.\n  - Удаление лишних признаков помогает упростить модель и улучшить её производительность.\n---\n## Основы описательной статистики\n### Меры центральной тенденции\n- Данные распределены вокруг этого «центра».\n  - Эти меры помогают понять, где находится \"центр\" данных.\n- Вычисляется для каждого атрибута.\n- Три распространенных типа:\n  - **Среднее**: сумма всех значений, деленная на их количество.\n  - **Мода**: наиболее часто встречающееся значение.\n  - **Медиана**: значение, которое находится в середине упорядоченного набора данных.\n- Эти меры не дают информации относительно экстремальных значений в распределении данных и разброса данных.\n  - Например, среднее может быть искажено выбросами.\n\n### Среднее арифметическое\n- Чувствительно к выбросам.\n  - Например, если в данных есть одно очень большое значение, среднее будет значительно выше, чем большинство значений в наборе данных.\n---\n## Этапы EDA\n1. **Изучение распределения данных.**\n   - Это помогает понять, как данные распределены и есть ли в них выбросы.\n2. **Обработка пропущенных значений** (наиболее распространенная проблема в каждом датасете).\n   - Пропущенные значения могут быть заполнены средним значением, медианой или удалены.\n3. **Обработка отклонений.**\n   - Выбросы могут быть удалены или заменены на более репрезентативные значения.\n4. **Удаление дубликатов данных.**\n   - Дубликаты могут исказить результаты анализа.\n5. **Кодирование категориальных переменных.**\n   - Категориальные данные могут быть преобразованы в числовые для использования в моделях.\n6. **Нормализация и масштабирование.**\n   - Это помогает привести данные к одному масштабу, что важно для многих алгоритмов машинного обучения.\n---\n## Двумерный анализ\n- Исследует отношения между двумя переменными.\n  - Например, можно исследовать связь между возрастом и доходом.\n- Помогает находить корреляции, отношения и зависимости между парами переменных.\n- Используемые методы:\n  - **Диаграммы рассеяния**: визуализация зависимости между двумя переменными.\n  - **Регрессионный анализ**: позволяет предсказать одну переменную на основе другой.\n  - **Корреляционный анализ**: измеряет силу и направление связи между переменными.\n---\n## Предварительная обработка данных\n- **Очистка данных**: удаление дубликатов, обработка пропущенных значений, исправление ошибок в данных.\n  - Это важный этап, так как качество данных напрямую влияет на качество анализа.\n- **Преобразование данных**: изменение формата данных, кодирование категориальных переменных, нормализация или стандартизация числовых признаков.\n  - Например, категориальные данные могут быть преобразованы в числовые с помощью one-hot encoding.\n---\n## Визуализация данных\n- Построение графиков и диаграмм для визуального представления данных.\n  - **Гистограммы**: показывают распределение данных.\n  - **Ящики с усами (Box Plots)**: показывают медиану, квартили и выбросы.\n  - **Диаграммы рассеяния (Scatter Plots)**: показывают связь между двумя переменными.\n  - **Тепловые карты (HeatMaps)**: показывают интенсивность значений в таблице данных.\n- Визуализация помогает выявить закономерности, аномалии и взаимосвязи между переменными.\n  - Например, тепловая карта может показать, какие переменные наиболее сильно коррелируют друг с другом.\n---\n",
    "created_at": "2025-03-27T13:36:49.628635"
  },
  {
    "id": "1743061044.507574",
    "subject": "Машинное обучение и большие данные",
    "date": "2025-03-07",
    "title": "Лекция 07.03.25",
    "content": "### Что такое машинное обучение?\nМашинное обучение (ML) — это процесс разработки алгоритмов или моделей, которые могут предсказывать значения целевой переменной (Y) для новых объектов (x) на основе обучающих данных.\n\n### Модель алгоритма\n- Модель алгоритма$a$— это параметрическое семейство функций$g : X \\to Y$или $g(x, \\theta)$, где $\\theta \\in \\Theta$— параметры.\n- Процесс подбора оптимальной функции$g$и параметров$\\theta$называется **настройкой** или **обучением** алгоритма.\n\n### Признаковое описание объекта\n- Признаки объекта $x$ можно записать в виде вектора: $x_1, ..., x_d$.\n- Признаковое пространство определяется количеством признаков. Например, три признака образуют трехмерное пространство.\n\n### Обучающая выборка\n- Обучающая выборка — это набор объектов $x$, для которых известны значения целевой переменной $y$.\n- Обучающая выборка может быть разделена на:\n  - Тренировочную выборку (для обучения модели).\n  - Валидационную выборку (для настройки гиперпараметров).\n  - Тестовую выборку (для оценки качества модели).\n\n---\n## Обучение с учителем (Supervised Learning)\n\n### Основные понятия\n- В обучении с учителем данные представлены в виде пар $(x_i, y_i)$, где $x_i$— вектор признаков, а $y_i$ — метка.\n- Метка $y_i$ может быть элементом конечного множества классов, вещественным числом или более сложной структурой.\n\n### Цель обучения с учителем\n- Цель — создать модель, которая принимает вектор признаков $x$ и возвращает информацию, позволяющую определить метку для этого вектора.\n- Пример: модель, предсказывающая вероятность заболевания раком на основе характеристик пациента.\n\n### Оценщики (Estimators)\n- В Scikit-Learn модели называются **оценщиками**.\n- Основные методы оценщиков:\n  - `fit(X, y)` — обучение модели.\n  - `predict(X)` — предсказание на новых данных.\n  - `transform(X)` — преобразование данных.\n\n---\n## Классификация и регрессия\n\n### Классификация (Classification)\n- **Целевая переменная (target)** — дискретная.\n- Прогнозируется метка класса из заранее определенного списка.\n- Примеры:\n  - Бинарная классификация: $Y = \\{0, 1\\}$ (да/нет, положительный/отрицательный).\n  - Многоклассовая классификация: $Y = \\{0, 1, 2, ...\\}$.\n\n### Регрессия (Regression)\n- **Целевая переменная (target)** — непрерывная.\n- Прогнозируется вещественное число.\n- Пример: предсказание стоимости квартиры на основе её характеристик.\n\n---\n## Процесс машинного обучения\n\n### Этапы машинного обучения\n1. **Обучение алгоритма**:\n   - Модель изучает данные с известными ответами и учится делать предсказания.\n   - Цель: минимизировать отклонение предсказаний $a(x)$ от правильных ответов $y$.\n\n2. **Применение алгоритма**:\n   - Обученная модель используется для предсказания на новых данных.\n   - Пример: предсказание стоимости квартир, для которых неизвестна цена.\n\n### Разделение данных\n- Данные делятся на:\n  - Обучающую выборку $(X_{train}, y_{train})$.\n  - Тестовую выборку $(X_{test}, y_{test})$.\n- Модель обучается на обучающей выборке и оценивается на тестовой.\n\n---\n## Гиперпараметры и проблемы машинного обучения\n\n### Гиперпараметры модели\n- **Гиперпараметры** — параметры, задаваемые до начала обучения (например, шаг градиентного спуска).\n- **Параметры модели** — параметры, оптимизируемые в процессе обучения (например, веса в нейронной сети).\n\n### Основные проблемы ML\n1. **Недостаточный объем обучающей выборки**.\n2. **Пропуски в данных**.\n3. **Переобучение (Overfitting)**:\n   - Модель слишком сложна и хорошо работает на обучающих данных, но плохо на новых.\n4. **Недобучение (Underfitting)**:\n   - Модель слишком проста и не может хорошо описать данные.\n\n---\n## Линейная регрессия\n\n### Понятие линейной регрессии\n- Линейная регрессия — это статистический метод, моделирующий линейную зависимость между зависимой переменной и одной или несколькими независимыми переменными.\n- Математически линейная зависимость представляет прямую линию на графике.\n\n### Регрессионный анализ\n- **Регрессионный анализ** — инструмент для установления модели отношений между переменными.\n- **Линейная регрессия**: зависимая и независимая переменные связаны через уравнение, где показатель степени равен 1.\n- **Нелинейная регрессия**: описывает нелинейные отношения, где показатель степени любой переменной не равен 1.\n\n### Типы регрессии\n- **Простая линейная регрессия**: одна независимая переменная.\n- **Множественная линейная регрессия**: несколько независимых переменных.\n- **Полиномиальная регрессия**: нелинейная зависимость, описываемая полиномом.\n\n---\n",
    "created_at": "2025-03-27T13:37:24.507574"
  },
  {
    "id": "1743061073.192123",
    "subject": "Машинное обучение и большие данные",
    "date": "2025-02-07",
    "title": "Лекция 07.02.25",
    "content": "# 1. Введение в ML\n\n### Что такое Искусственный Интеллект (ИИ)?\nИскусственный интеллект (ИИ) — это область компьютерных наук, которая занимается созданием систем, способных выполнять задачи, требующие человеческого интеллекта. Эти задачи включают в себя обучение, рассуждение, восприятие, понимание естественного языка и принятие решений.\n\n### Что такое Машинное Обучение (ML)?\nМашинное обучение (ML) — это подраздел ИИ, который фокусируется на разработке алгоритмов и моделей, позволяющих компьютерам обучаться на данных и делать прогнозы или принимать решения без явного программирования.\n\n### Типы ИИ:\n1. Слабый ИИ (Narrow AI): Системы, предназначенные для выполнения конкретных задач (например, распознавание лиц, голосовые помощники).\n2. Сильный ИИ (General AI): Гипотетические системы, способные выполнять любые интеллектуальные задачи, которые может выполнять человек.\n3. Супер-ИИ (Superintelligence): Гипотетические системы, превосходящие человеческий интеллект во всех аспектах.\n\n### Области применения ИИ:\n- Обработка естественного языка (NLP)\n- Компьютерное зрение\n- Робототехника\n- Рекомендательные системы\n- Автономные транспортные средства\n\n### Виды Машинного Обучения:\n4. Обучение с учителем (Supervised Learning): Модель обучается на размеченных данных, где каждому примеру соответствует правильный ответ.\n5. Обучение без учителя (Unsupervised Learning): Модель обучается на данных без меток, выявляя скрытые структуры или паттерны.\n6. Обучение с подкреплением (Reinforcement Learning): Модель обучается через взаимодействие с окружающей средой, получая обратную связь в виде наград или штрафов.\n\n# 2. Типы задач в ML\n\n### Классификация:\nЗадача, при которой модель должна отнести входные данные к одной из нескольких категорий. Например, распознавание спама в электронной почте.\n\n### Регрессия:\nЗадача, при которой модель должна предсказать непрерывное значение. Например, предсказание цены дома на основе его характеристик.\n\n### Кластеризация:\nЗадача, при которой модель группирует данные на основе их сходства. Например, сегментация клиентов по их покупательскому поведению.\n\n### Понижение размерности:\nЗадача, при которой модель уменьшает количество признаков в данных, сохраняя при этом важную информацию. Например, визуализация многомерных данных.\n\n### Генерация данных:\nЗадача, при которой модель создает новые данные, похожие на те, на которых она обучалась. Например, генерация изображений или текста.\n\n# 3. Основные понятия ML\n\n### Данные:\nДанные — это основа машинного обучения. Они могут быть структурированными (например, таблицы) или неструктурированными (например, текст, изображения).\n\n### Признаки (Features):\nПризнаки — это характеристики данных, которые используются для обучения модели. Например, для задачи классификации изображений признаками могут быть пиксели изображения.\n\n### Алгоритмы или модели:\nАлгоритмы — это математические методы, которые используются для обучения моделей. Модели — это результат обучения алгоритма на данных.\n\n### Прямоугольные данные:\nПрямоугольные данные — это данные, представленные в виде таблицы, где строки представляют собой примеры, а столбцы — признаки.\n\n### Кадр данных (DataFrame):\nКадр данных — это структура данных, используемая для работы с прямоугольными данными. Например, в библиотеке Pandas в Python используется DataFrame.\n\n### Схема процесса ML:\n7. Сбор данных: Сбор и подготовка данных для обучения.\n8. Предобработка данных: Очистка, нормализация и преобразование данных.\n9. Выбор модели: Выбор подходящего алгоритма для задачи.\n10. Обучение модели: Обучение модели на тренировочных данных.\n11. Оценка модели: Проверка качества модели на тестовых данных.\n12. Настройка модели: Оптимизация параметров модели для улучшения качества.\n13. Прогнозирование: Использование модели для предсказания на новых данных.\n",
    "created_at": "2025-03-27T13:37:53.192123"
  },
  {
    "id": "1743061354.170786",
    "subject": "Базы данных",
    "date": "2025-02-14",
    "title": "Введение",
    "content": "## Раздел 1.1\n\n**База данных** - совокупность специальным образом организованных данных, которая:\n1) Подлежит долговременному хранению памяти ЭВМ\n2) Содержит информацию о сравнительно небольшом (фиксированном) количестве классов объектов\n3) Все классы объектов относятся к одному предмету прикладной области\n4) Используется в одном или нескольких приложениях\n\n---\n**Жизненный цикл данных существенно больше жизненного цикла программ, которые с ними работают**\n\n---\n## Раздел 1.2 \n\n**Специфическое свойство баз данных, отличающее их от других инфо-систем**:\n1) В отличие от систем ИИ, данные в БД пассивны - они не содержат информации о том, как ими пользоваться\n2) Соединение без потери информации\n\n---\n## Раздел 1.3\n\n**База данных должна быть одна на логическом уровне**\n\n---\n## Раздел 2.1\n\n**Категории баз данных:**\n1) Физический уровень:\n\t1) Поле:\n\t     БД - наименьшая единица памяти с определенным адресом и размером\n\t2) Физическая запись:\n\t     Упорядоченная последовательность фиксированного количества полей\n\t     Две записи однотипны, если они состоят из одинаковых последовательностей полей\n\t3) Память: \n\t     Совокупность однотипных физических записей\n\t4) Блок:\n\t\t Объем информации, передаваемый из внешний памяти в оперативную за одно обращение\n\t5) Индексный файл:\n\t\t Структурированная организация физических записей, предназначенная для поиска данных и обеспечения ограничения целостности на данных\n\n2) Логический уровень:\n\t1) Атрибут (элементарная единица):\n\t\t Наименьший элемент информации с определенным типом и наименованием\n\t\t \n\t2) Логическая запись:\n\t\t Совокупность фиксированного количества элементов данных\n\t\t Обычно соответствует физической записи\n\t3) Отношение:\n\t\t Совокупность однотипных логических записей\n\t4) Схема отношений\n\t5) Схема БД:\n\t\t Совокупность схем отношений с установленными ограничениями целостности\n\t\t На ней подчеркнуты первичные ключи отношений\n\n---\n## Раздел 2.2\n\n**Первичный ключ (Primary Key)** - атрибут или совокупность, которая однозначна определяет логическую запись\n  \n*За одним сотрудником можно закрепить несколько единиц оборудования\nЗа одним оборудованием можно закрепить несколько сотрудников\n\n**Стрелки на схеме** - ссылочная целостность, которая обеспечивается Foreign Key\n\n---\n## Раздел 2.3 (Требования к БД)\n\n### 1. Не избыточность и непротиворечивость\n\n- **Дублирование данных:**\n\t  1) В индексных файлах.\n\t  2) На различных серверах (зеркала).\n- **Контроль избыточности:** За этим следит СУБД (Система Управления Базами Данных).\n\n---\n\n### 2. Защита от программных и аппаратных сбоев\n\n#### Типы сбоев:\n1. **Логический сбой:**\n   - Возникает, когда запись уже существует (нарушение ограничения сущности).\n   - СУБД должна отвергнуть такую операцию (ошибка 1 рода).\n2. **Физический сбой:**\n   - Возникает при удалении записи, на которую есть ссылки в других таблицах (нарушение ссылочной целостности).\n   - СУБД должна отвергнуть такую операцию (ошибка 2 рода).\n   - Происходит аварийное выключение без изменения структуры данных.\n\n#### Проблемы с цепочками записей:\n- Если записи в основном файле связаны в цепь, разрушение одного указателя приводит к потере всех последующих записей.\n\n#### Меры защиты:\n- **Ведение системного журнала:** перед выполнением операции СУБД помещает в журнал информацию, достаточную для завершения операции после повторного старта.\n- **Архивация данных:** cоздаются две копии текущего состояния БД:\n    1. Первая копия хранится в несгораемом сейфе на рабочем месте.\n    2. Вторая копия передается в секретные государственные или коммерческие службы (первый отдел).\n- **Триггеры:** используются для автоматического выполнения действий при определенных событиях в БД.\n\n---\n\n### 3. Независимость данных\n\n#### Прикладная программа:\n1. Программа, реализующая отдельную функцию и взаимодействующая с БД.\n2. **Мобильность:** Программа считается мобильной, если ее исходный код не зависит от операционной среды и оборудования.\n3. **Корректность:** Программа написана правильно, если она мобильна и не зависит от места и способа хранения данных.\n\n#### Реализация принципа независимости данных:\n- Необходима независимость от приложения, места и способа хранения данных.\n- Для реализации этого принципа рабочей группой **CODASYL** была предложена **трехуровневая модель** описания и представления данных.\n\n---\n",
    "created_at": "2025-03-27T13:42:34.170786"
  },
  {
    "id": "1743061580.762605",
    "subject": "Математическая логика и теория алгоритмов",
    "date": "2025-02-04",
    "title": "Алфавит и слова",
    "content": "## **Алфавит**\nАлфавит — любое непустое множество **Ā**.\n\n---\n\n## **Элементы алфавита**\nЭлементы алфавита называются **буквами** (или **символами**) алфавита **Ā**.\n\n---\n\n## **Слово (на алфавите)**\nСлово — любая последовательность букв алфавита. Обозначается как **w**.\n\n---\n\n## **Длина слова**\nДлина слова **w** обозначается как **L(w)**. Это число символов в слове.\n\n---\n\n## **Пустое слово**\nПустое слово обозначается как **∅**. Это последовательность нулевой длины, то есть **L(∅) = 0**.\n\n---\n\n## **Конкатенация**\nКонкатенация — бинарная алгебраическая операция **\\*** (звездочка), заданная на множестве **W(Ā)** всех слов на алфавите **Ā** следующим образом:  \n**W(Ā)^2 → W(Ā)**  \n\n1. Если **w ∈ W(Ā)** — слово на алфавите **Ā**, то:  \n   **∅ \\* w = w = w \\* ∅**  \n2. Если **w** и **v** — два непустых слова, то:  \n   **w \\* v = a₁a₂...aₙb₁b₂...bₘ**,  \n   где **a₁a₂...aₙ** — буквы слова **w**, а **b₁b₂...bₘ** — буквы слова **v**.\n\n---\n### Примеры\n1. Пусть алфавит **Ā = {a, b}**.  \n   - Слово **w = aab**.  \n   - Длина слова **L(w) = 3**.  \n   - Конкатенация слов **w = aab** и **v = ba**:  \n     **w \\* v = aabba**.\n\n2. Пустое слово **∅** при конкатенации:  \n   - **∅ \\* ab = ab**  \n   - **ab \\* ∅ = ab**.\n\n---\n## **Теорема**\nКонкатенация на множестве **W(Ā)** задает структуру моноида:\n- **Нейтральный элемент** — пустое слово **∅**.\n- **Ассоциативность**: для любых слов **u**, **v**, **w** выполняется **(u \\* v) \\* w = u \\* (v \\* w)**.\n- **Коммутативность отсутствует**, если **|Ā| > 1** (то есть алфавит содержит более одного символа).\n\n---\n\n## **Замечание**\n1. Каждая буква алфавита **Ā** — это слово длины **1**.\n2. Каждое слово — это конкатенация входящих в него букв.\n\n---\n\n## **Под-слово**\nСлово **θ** называется **под-словом** слова **w** (обозначается **θ ⏴ w**), если существуют такие слова **a** и **b** из **W(Ā)**, что:  **w = aθb**,  причем **a** и **b** могут быть **∅**.\n**∅** всегда является под-словом любого слова, включая само **∅**.\n\n---\n\n## **Вхождение под-слова**\n**Вхождение под-слова θ в слово w** — это упорядоченная тройка **ξ = ⟨a, θ, b⟩**, где: **w = aθb**.\n\n---\n\n## **Замена вхождения**\nПусть **θ** — под-слово слова **w**, а **ξ = (a, θ, b)** — некоторое его вхождение. Если **X** — слово на алфавите **Ā**, то слово $$w_{\\xi}[X] = aXb,$$\nполученное заменой под-слова θ, называется результатом замены вхождения ξ на X.\n\n---\n\n## **Замена нескольких вхождений**\nЕсли **w** — слово, $$a_1, \\dots, a_n - \\text{попарно различные буквы алфавита Ā} $$и $$X_1, \\dots, X_n - \\text{ слова из W(Ā),}$$то через $$w(a_1, \\dots, a_n)[X_1, \\dots, X_n] $$обозначается результат замены каждого вхождения **aᵢ** на слово **Xᵢ**.\n\n---\n",
    "created_at": "2025-03-27T13:46:20.762605"
  },
  {
    "id": "1743061595.459983",
    "subject": "Математическая логика и теория алгоритмов",
    "date": "2025-02-04",
    "title": "Формализованные языки",
    "content": "## **Формализованный язык**\nФормализованным языком на алфавите $\\bar{A}$ называется упорядоченная пара $\\mathfrak{L} = \\langle \\bar{A}, \\mathcal{E} \\rangle$, где $\\mathcal{E} \\subseteq W(\\bar{A})$ — множество выделенных слов на алфавите, называемое множеством выражений языка $\\mathfrak{L}$.\n\nЕсли $\\mathfrak{L}_1$ и $\\mathfrak{L}_2$ — два языка, то при $A_1 \\subseteq A_2$ и $\\mathcal{E}_1 \\subseteq \\mathcal{E}_2$, то $\\mathfrak{L}_2$ — расширение $\\mathfrak{L}_1$.\n\nПри этом, если $\\mathcal{E}_1 = \\mathcal{E}_2$, то расширение называется **алфавитным**.  \nИначе, если $A_1 = A_2$, то расширение называется **алфавитно-инвариантным**.\n\n---\n### Примечание:  \nЛюбой язык является несущественным расширением самого себя.\n\nЕсли $\\mathfrak{L}_2$ — расширение $\\mathfrak{L}_1$, то $\\mathfrak{L}_1 \\subseteq \\mathfrak{L}_2$.\n\n---\n## **Логическая структура**\n\n**Логическая сигнатура нулевого порядка** — упорядоченная тройка $\\Omega = \\langle L_2, L_1, L_0 \\rangle$, где $L_i$ — некоторые множества, такие что $\\text{smb}(\\Omega) = L_0 \\cup L_1 \\cup L_2 \\neq \\emptyset$, и их попарное пересечение = $\\emptyset$.  \nПри этом:\n- $L_0$ — множество логических констант,\n- $L_1$ — множество унарных логических связок,\n- $L_2$ — множество бинарных логических связок.\n\n### Замечание 1:  \nЕсли $L_0 = \\{K_1, \\dots, K_s\\}$, $L_1 = \\{\\square{_1}, \\dots, \\square{_n}\\}$, $L_2 = \\{\\nabla_1, \\dots, \\nabla_n\\}$, то $\\Omega = \\langle L_2, L_1, L_0 \\rangle$.\n\n### Замечание 2:  \nЕсли множество $L_i = \\emptyset$, то его либо пропускают, либо пишут вместо него $\\emptyset$.\n\n### Пример 1:  \nРасширенная сигнатура классического ИВ: $\\Omega^*_{c1} = \\langle L_2, L_1, L_0 \\rangle$, где\n- $L_2 = \\{\\lor, \\land, \\rightarrow, \\leftrightarrow, \\oplus, \\downarrow, \\uparrow\\}$,\n- $L_1 = \\{\\lnot\\}$,\n- $L_0 = \\{\\top, \\bot\\}$.\n\n### Пример 2:  \nСтандартная сигнатура классического ИВ: $\\Omega_{c1} = \\langle L_2, L_1, L_0 \\rangle$, где\n- $L_2 = \\{\\lor, \\land, \\rightarrow\\}$,\n- $L_1 = \\{\\lnot\\}$,\n- $L_0 = \\emptyset$.\n\n## Язык ИВ логической сигнатуры\nЯзыком ИВ логической сигнатуры $\\Omega = \\langle L_2, L_1, L_0 \\rangle$ на множестве переменных $V$ (не менее чем счетное) называется язык $L_\\Omega(V) = \\langle \\bar{A}_\\Omega(V), \\mathcal{E}_\\Omega(V) \\rangle$, где:  \n- $\\bar{A}_\\Omega(V) = V \\cup \\text{smb}(\\Omega) \\cup \\{(\\ , )\\ ,\\ \",\"\\}$ \n- $\\mathcal{E}_\\Omega(V)$ является наименьшим по вложению подмножеством множества слов $W(\\bar{A}_\\Omega(V))$ данного алфавита $\\bar{A}_\\Omega(V)$, удовлетворяющее следующим условиям:\n\t1) $V \\subseteq \\mathcal{E}_\\Omega(V)$, $L_0 \\subseteq \\mathcal{E}_\\Omega(V)$\n\t2) Если $f \\in \\mathcal{E}_\\Omega(V)$, а $\\square{} \\in L_1$ — унарная логическая связка, то $\\square{f} \\in \\mathcal{E}_\\Omega(V)$\n\t3) Если $f$ и $p \\in \\mathcal{E}_\\Omega(V)$, а $\\nabla \\in L_2$ — бинарная логическая связка, то $(f \\nabla p) \\in \\mathcal{E}_\\Omega(V)$.  \n* Часто скобки $( \\,)$ не пишут и вместо $(f \\nabla p)$ пишут просто $f \\nabla p$.",
    "created_at": "2025-03-27T13:46:35.459983"
  },
  {
    "id": "1743061700.143053",
    "subject": "Теория вероятностей и математическая статистика",
    "date": "2025-03-04",
    "title": "Лекция 04.03.2025",
    "content": "### Медианный интервал \n\nИнтервал, в котором накопленная сумма частот впервые достигает $1/2$.  \nВыборочной группированной медианой называется значение:\n\n$$\nm_e^* = x_e + h \\cdot \\frac{n / 2 - (n_1 + \\cdots + n_{m_e} - 1)}{n_{m_e}}\n$$\n\nгде:\n- $n$ – объем выборки,\n- $h$ – длина интервала группировки,\n- $x_e$ – левая граница медианного интервала,\n- $n_i$ – частота $i$-го интервала,\n- $n_{m_e}$ – частота медианного интервала.\n\n---\n### Приближенное значение медианы\n\n![[медиана геом.jpg]]\n\n\n---\n### Пример. \n\nДан группированный статистический ряд величины $X$:\n\n| $X$   | 1-5 | 5-9 | 9-13 | 13-17 |\n| ----- | --- | --- | ---- | ----- |\n| $n_i$ | 2   | 8   | 9    | 1     |\n\nНайти выборочную группированную медиану.\n\n**Решение.**  \n$n = 20$.\n\n| $X$     | 1-5 | 5-9 | 9-13 | 13-17 |\n| ------- | --- | --- | ---- | ----- |\n| $n_i$   | 2   | 8   | 9    | 1     |\n| $n_i/n$ | 0,1 | 0,4 | 0,45 | 0,05  |\n\n$$\nm_e' = 5 + 4 \\cdot \\frac{20/2 - 2}{8} = 9\n$$\n\n---\n### Интервал квантильного порядка $q$\n\nИнтервал, в котором сумма накопленных частот впервые достигает значения $q$. \nВыборочной группированной квантилью называется значение:\n\n$$\n\\lambda_q^* = X(q) + h \\cdot \\frac{nq - (n_1 + \\cdots + n_q-1)}{n(q)}\n$$\n\nгде:\n- $\\lambda(q)$ – левая граница квантильного интервала,\n- $n(q)$ – численность квантильного интервала,\n- $n_1, \\ldots, n_{(q)-1}$ – частоты интервалов, предшествующих квантильному.\n\n---\n### Пример. \n\nДан группированный статистический ряд величины $X$:\n\n| $X$     | **1-5** | 5-9 | 9-13 | 13-17 |\n| ------- | ------- | --- | ---- | ----- |\n| $n_i$   | 2       | 8   | 9    | 1     |\n| $n_i/n$ | 0,1     | 0,4 | 0,45 | 0,05  |\n\nНайти приближенно квантиль порядка 0,4.\n\n**Решение.**  \n$n = 20$.\n\nКвантильным интервалом является второй $\\Rightarrow X_{0,4} = 5$\n\n$$\nX_{0,4}^* = 5 + 4 \\cdot \\frac{20 \\cdot 0,4 - 2}{8} = 8\n$$\n\n---\n### Модальный интервал\n\nИнтервал, имеющий наибольшую численность.  \nВыборочной группированной модой называется значение:\n\n$$\nm_0^* = x_0 + h \\cdot \\frac{n_{m_0} - n_{m_0-1}}{2n_{m_0} - n_{m_0-1} - n_{m_0+1}}\n$$\n\nгде:\n- $x_0$ - левая граница модального интервала,\n- $n_{m_0}$ - частота модального интервала,\n- $n_{m_0-1}$; $n_{m_0+1}$ - частоты интервалов слева и справа от модального.\n\n---\n### Приближённое значение моды\n\n![[мода геом.jpg]]\n\n---\n### Пример.  \n\nДан группированный статистический ряд величины $X$:\n\n| Х    | 1-5   | 5-9   | 9-13   | 13-17 |\n|---|---|---|---|---|\n| $n_i$    | 2    | 8    | 9    | 1    |\n| $n_i/n$ | 0,1   | 0,4   | 0,45 | 0,05 |\n\nНайти выборочную группированную моду.\n\n**Решение.**  \n$n = 20$.\n\nМодальным интервалом является третий.\n\n$$\nm_0^* = 9 + 4 \\cdot \\frac{9 - 8}{2 \\cdot 9 - 8 - 1} = 9,4\n$$\n\n---\n## Введение в точечное статистическое оценивание\n\n1. Оценки параметров распределения.  \n2. Несмещенность оценки.  \n3. Состоятельность оценки.  \n4. Оптимальность оценок.  \n\nТочное статистическое оценивание — это мощный инструмент для анализа данных. Оно позволяет нам оценить неизвестные параметры распределения случайных величин, используя имеющиеся данные.\n\n> «Мораль здесь такова: позаботься о смысле, а слова позаботятся о себе самих»  \n> Льюис Кэрролл «Алиса в стране чудес»\n\n---\n\n## Оценки параметров распределения\n\nПусть имеется выборка $X_1, \\ldots, X_n$ объёма $n$, извлечённая из распределения $F_\\theta$, которое известным образом зависит от неизвестного параметра $\\theta$.\n\nИзмеримая функция $g(X_1, \\ldots, X_n)$, от наблюдений называется **статистикой**.\n\nЗадача оценивания: выбор такой статистики $g(X_1, \\ldots, X_n)$, значения которой при заданной реализации $(X_1, \\ldots, X_n)$ приближаются к значению параметра $\\theta$.\n\n---\n## Точечная оценка\n\nВыборочная числовая характеристика (статистика) $\\theta = g(X_1, \\ldots, X_n)$, применяемая для оценивания неизвестного параметра $\\theta$ генеральной совокупности, называется его **точечной оценкой**.\n\nОбозначения: $\\hat{\\theta}, \\hat{\\theta}_i, \\hat{\\theta}_j^*$.\n\n**Пример точечной оценки:**  \nПредставьте, что вы хотите узнать средний рост всех студентов в университете. Вы берете случайную выборку из 100 студентов и измеряете их рост. Средний рост этой выборки будет точечной оценкой среднего роста всех студентов.\n\n---\n## Свойства точечных оценок\n\n- Несмещенность\n- Состоятельность\n- Оптимальность\n- Эффективность\n\n---\n### Несмещенность оценки\n\nНесмещенной называют точечную оценку $\\hat\\theta$, математическое ожидание которой равно оцениваемому параметру при любом объеме выборки:\n$$\nM\\hat\\theta(X_1, \\ldots, X_n) = \\theta \\quad \\forall \\theta \\in \\Theta\n$$\n\n**Замечание.** Математическое ожидание находится в предположении, что верна модель $F_\\theta$, т. е. что параметр равен $\\theta$.\n\nЕсли $M\\hat\\theta \\neq \\theta$, то оценка называется смещенной, и её смещение равно $M\\hat\\theta - \\theta$.\n\n---\n\nСтатистика $\\hat{\\theta} = \\theta(X_{1}\\dots X_{n})$ называется **асимптотически несмещённой** оценкой параметра $\\theta$, если для любого $\\theta \\in \\Theta$ имеет место сходимость:\n$$ M\\hat\\theta \\to \\theta \\text{ при } n \\to \\infty. $$\n---\n### Состоятельность оценки\n\nСтатистика $\\hat{\\theta} = g(X_1, \\ldots, X_n)$ называется состоятельной оценкой $\\theta$, если она сходится по вероятности к оцениваемому параметру:\n\n$$\nP(|\\hat{\\theta}_n - \\theta| < \\epsilon) \\to 1 \\quad \\forall \\epsilon > 0, \\theta \\in \\Theta \\quad \\text{при } n \\to \\infty\n$$\n\nСвойство состоятельности означает, что оценка делается точнее при увеличении количества данных.\n\n---\n### Оптимальность оценок\n\nПусть выбран критерий близости оценки к неизвестному параметру $\\theta$.\n\nОценка $\\hat{\\theta}$ параметра $\\theta$ называется оптимальной по данному критерию в рассматриваемом классе оценок, если она минимизирует выбранный критерий.\n\n**Замечание.** За критерий близости оценки к параметру $\\theta$ можно взять:\n\n$$\nM(\\hat{\\theta} - \\theta)^2,\n$$\n\nгде $\\hat{\\theta} = g(X_1, \\ldots, X_n)$ — оценка $\\theta$.\n\nЕсли оценка $\\hat{\\theta}$ — несмещена, то:\n\n$$\nM(\\hat{\\theta} - \\theta)^2 = D\\hat{\\theta},\n$$\n\nгде наименьшая дисперсия соответствует наиболее устойчивой оценке, которая меньше других меняется от выборки к выборке.\n\n---\nНесмещенная оценка $\\hat{\\theta}$ параметра $\\theta$ называется **оптимальной оценкой**, если\n$$\nD\\hat{\\theta} \\leq D\\theta^*, \\, \\forall \\theta \\in \\Theta,\n$$\nгде $\\theta^*$ — произвольная несмещенная оценка $\\theta$.\n\n---\n## Теоремы\n### Теорема 1\nЕсли $M\\hat{\\theta} = \\theta$ и $D\\hat{\\theta} \\rightarrow 0$ при $n \\rightarrow \\infty$, то $\\hat{\\theta}$ — состоятельная оценка $\\theta$.\n\n### Теорема 2\nЕсли $M\\hat{\\theta} \\rightarrow \\theta$ при $n \\rightarrow \\infty$ и $D\\hat{\\theta} \\rightarrow 0$ при $n \\rightarrow \\infty$, то $\\hat{\\theta}$ — состоятельная оценка $\\theta$.\n\n### Теорема 3\nЕсли $\\hat{\\theta}$ — состоятельная оценка $\\theta$, а $f$ — непрерывная функция, то $f(\\hat{\\theta})$ — состоятельная оценка $f(\\theta)$.\n\n### Теорема 4\nЕсли $\\hat{\\theta}_1, \\hat{\\theta}_2$ — две оптимальные оценки $\\theta$, то $P(\\hat{\\theta}_1 = \\hat{\\theta}_2) = 1.$\n\n### Доказательство\nПусть есть третья оценка\n$$\n\\hat{\\theta} = \\frac{\\hat{\\theta}_1 + \\hat{\\theta}_2}{2}, \\quad M\\hat{\\theta} = \\theta.\n$$\n\nТогда\n$$\nD\\hat{\\theta} = \\frac{1}{4} D\\hat{\\theta}_1 + \\frac{1}{4} D\\hat{\\theta}_2 + \\frac{1}{2} \\text{cov}(\\hat{\\theta}_1, \\hat{\\theta}_2).\n$$\n\nПо неравенству Коши-Буняковского для любых случайных величин\n$$\n\\text{cov}(\\hat{\\theta}_1, \\hat{\\theta}_2) \\leq \\sqrt{D\\hat{\\theta}_1 \\cdot D\\hat{\\theta}_2}.\n$$\n\nРавенство достигается, только если $\\hat{\\theta}_1 = k\\hat{\\theta}_2$.\n\nТак как $\\hat{\\theta}_1$ и $\\hat{\\theta}_2$ — оптимальные оценки с минимальной дисперсией, обозначим\n$$\nD\\hat{\\theta}_1 = D\\hat{\\theta}_2 = \\inf_{\\theta} D\\hat{\\theta} = v.\n$$\n\nТогда\n$$\nD\\hat{\\theta} \\leq \\frac{1}{2}(v + \\text{cov}(\\hat{\\theta}_1, \\hat{\\theta}_2)) \\leq v.\n$$\n\nЕсли $D\\hat{\\theta} < v$, это противоречит условию оптимальности оценок $\\hat{\\theta}_1$ и $\\hat{\\theta}_2$. \n\nТаким образом, $\\text{cov}(\\hat{\\theta}_1, \\hat{\\theta}_2) = v \\Rightarrow \\hat{\\theta}_1 = k\\hat{\\theta}_2,$ при $k = 1$.\nОткуда, $\\hat{\\theta}_1 = \\hat{\\theta}_2$ с вероятностью 1.\n\n### Теорема 5\n\nПусть $T(x)$ – оптимальная оценка некоторой параметрической функции $\\tau(\\theta)$, а $H(x)$ такая статистика, что $M(H(x)) = 0, \\, \\forall \\theta \\in \\Theta.$\nТогда $\\text{cov}(T(x), H(x)) = 0 \\, \\forall \\theta \\in \\Theta.$\n### Доказательство\nРассмотрим\n$$\nT^*(x) = T(x) + \\lambda H(x), \\, \\lambda \\in \\mathbb{R}.\n$$\nТогда\n$$\nM(T^*(x)) = M(T(x)) + \\lambda M(H(x)) = M(T(x)) = \\tau(\\theta).\n$$\n$$\nD(T^*(x)) = D(T(x)) + \\lambda^2 D(H(x)) + 2 \\lambda \\text{cov}(T(x), H(x)) \\geq D(T(x)).\n$$\n$$\n\\lambda^2 D(H(x)) + 2 \\lambda \\text{cov}(T(x), H(x)) \\geq 0.\n$$\n$$\n\\lambda^2 D(H(x)) + 2 \\lambda \\text{cov}(T(x), H(x)) = 0.\n$$\nДискриминант уравнения:\n$$\n\\text{cov}^2(T(x), H(x)) = 0 \\Rightarrow \\text{cov}(T(x), H(x)) = 0.\n$$\n\n## Теорема 6\n\nЕсли $T_1(x), T_2(x)$ – оптимальные оценки некоторых параметрических функций $\\tau_1(\\theta)$ и $\\tau_2(\\theta)$ соответственно, то $T(x) = a_1 T_1(x) + a_2 T_2(x)$ - оптимальная оценка некоторой параметрической функции $\\tau(\\theta) = \\tau_1(\\theta) + \\tau_2(\\theta)$.\n### Доказательство\nРассмотрим $T(x) = a_1 T_1(x) + a_2 T_2(x)$:\n$$\nM(T(x)) = M(T_1(x)) + M(T_2(x)) = a_1 \\tau_1(\\theta) + a_2 \\tau_2(\\theta).\n$$\n\nПусть $S$ – произвольная несмещенная оценка функции $\\tau$. Рассмотрим разность $S - T$:\n$$\nM(S - T) = M S - M T = \\tau - \\tau = 0.\n$$\n\nТеперь рассмотрим ковариацию:\n$$\n\\text{cov}(T, S - T) = \\text{cov}(a_1 T_1(x) + a_2 T_2(x), S - T).\n$$\n\nПо теореме 5:\n$$\n\\text{cov}(T, S - T) = 0.\n$$\n\nС другой стороны,\n$$\n\\text{cov}(T, S - T) = \\text{cov}(T, S) - D T \\Rightarrow \\text{cov}(T, S) = D T.\n$$\n\nНо\n$$\n\\text{cov}(T, S) \\leq \\sqrt{D T \\cdot D S} \\Rightarrow D T \\leq \\sqrt{D T \\cdot D S} \\text{ или } D S \\geq D T.\n$$\n\nЭто верно для любой несмещенной оценки $S$, значит, $T$ – оптимальная оценка. \n\n---\n## Примеры\n\n### Пример 1: Несмещенная оценка среднего\n\nПусть $X$ — несмещенная оценка $\\mu$ в модели $(X, P_\\mu)$, где $P_\\mu$ — производное семейство распределений с неизвестным математическим ожиданием $\\mu$.\n\n$$\nMX = M \\left( \\frac{1}{n} \\sum_{i=1}^n X_i \\right) = \\frac{1}{n} \\sum_{i=1}^n MX_i = \\frac{1}{n} \\cdot n \\mu = \\mu\n$$\n---\n\n### Пример 2: Смещенная оценка дисперсии\n\nПусть $S^2$ — смещенная оценка $\\sigma^2$ в модели $(X, P(\\mu, \\sigma^2))$, где $P(\\mu, \\sigma^2)$ — производное семейство распределений с неизвестными математическим ожиданием $\\mu$ и дисперсией $\\sigma^2$.\n\n$$\nMS^2 = M \\left( \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X})^2 \\right) = \\frac{n-1}{n} \\sigma^2\n$$\n\nСмещение можно устранить, используя оценку дисперсии:\n\n$$\n\\overline{S}^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\overline{X})^2\n$$\n---\n\n### Пример 3: Состоятельность оценки\n\nПусть дана выборка $(X_1, \\ldots, X_n)$ из биномиального распределения $B(n, p)$. Исследуем на состоятельность оценки параметра $p$:\n\n1. $\\frac{X_1}{n}$\n2. $\\frac{2X_1 + 3X_2}{5n}$\n3. $\\frac{1}{5n} \\sum_{i=1}^5 X_i$\n\n**Решение:**\n\n1. Оценка $\\frac{X_1}{n}$ является состоятельной, так как при $n \\to \\infty$ она сходится по вероятности к $p$.\n2. Оценка $\\frac{2X_1 + 3X_2}{5n}$ также является состоятельной.\n3. Оценка $\\frac{1}{5n} \\sum_{i=1}^5 X_i$ является состоятельной.\n---\n## Оценки математического ожидания и дисперсии\n\n| Оценка | Формула |\n|--------|---------|\n| Несмещенная и состоятельная оценка математического ожидания – выборочная средняя | $\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i$ |\n| Смещенная и состоятельная оценка дисперсии – выборочная дисперсия | $S^2 = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X})^2$ |\n| Несмещенная и состоятельная оценка дисперсии – исправленная выборочная дисперсия | $\\bar{S}^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar{X})^2$ |\n",
    "created_at": "2025-03-27T13:48:20.143053"
  },
  {
    "id": "1743061721.940294",
    "subject": "Теория вероятностей и математическая статистика",
    "date": "2025-03-06",
    "title": "Лекция 06.03.2025",
    "content": "# Статистическое оценивание параметров\n---\n## Неравенство Рао – Крамера\n\n### Плотность распределения\nПлотность распределения $p(x, \\theta)$ может быть определена как:\n$$\np(x, \\theta) = \n\\begin{cases} \np(x, \\theta), & \\text{если распределение непрерывно,} \\\\\nP_\\theta (X = y), & \\text{если распределение дискретно.}\n\\end{cases}\n$$\n\n### Вклад выборки\nВклад выборки в информационное количество Фишера выражается через производную логарифма плотности распределения:\n$$\n\\frac{\\partial \\ln p(x_1, x_2, \\dots, x_n, \\theta)}{\\partial \\theta}\n$$\n### Вклад одного элемента выборки:\n$$\n\\frac{\\partial \\ln p(x,\\theta)}{\\partial \\theta}\n$$\n### Информационное количество Фишера о параметре $\\theta$:\n$$\nI = M \\left( \\frac{\\partial \\ln p(x_1, x_2, \\dots, x_n, \\theta)}{\\partial \\theta} \\right)^2\n$$\n\n### Условия регулярности\n1. Множество значений $x$, для которых $p(x; \\theta) \\neq 0$, не зависит от $\\theta$.\n2. $p(x; \\theta)$ дифференцируема по параметру $\\theta$.\n3. Оцениваемая функция $\\tau(\\theta)$ дифференцируема по параметру $\\theta$.\n\n### Неравенство Рао – Крамера\nПри выполнении условий регулярности для любой оценки выполняется неравенство:\n$$\nD\\hat\\theta \\geq \\frac{1}{I}\n$$\n### Доказательство\n\n1. По свойству плотности:\n\n$$\n\\int p \\, dx = 1\n$$\n\n2. Из несмещенности оценки:\n\n$$\n\\int \\hat{\\theta} p \\, dx = M \\hat{\\theta} = \\theta\n$$\n\n3. Продифференцируем (1) по параметру:\n\n$$\n\\int \\frac{\\partial p}{\\partial \\theta} \\, dx = 0\n$$\n\n4. Домножим (3) на $\\theta$:\n\n$$\n\\int \\theta \\frac{\\partial p}{\\partial \\theta} \\, dx = 0\n$$\n\n5. Продифференцируем (2) по параметру:\n\n$$\n\\int \\hat{\\theta}\\frac{\\partial p}{\\partial \\theta} \\, dx = 1\n$$\n\n6. Вычтем (5) - (4):\n\n$$\n\\int (\\hat{\\theta} - \\theta) \\frac{\\partial p}{\\partial \\theta} \\, dx = 1\n$$\n\n7. Подставим выражение для $\\partial p / \\partial \\theta$:\n\n$$\n\\int (\\hat{\\theta} - \\theta) \\frac{\\partial \\ln p}{\\partial \\theta} \\, p \\, dx = 1\n$$\n\n8. Из курса теории вероятностей известно:\n\n$$\nM [(\\varphi_1 - M \\varphi_1)(\\varphi_2 - M \\varphi_2)] = \\text{cov}(\\varphi_1, \\varphi_2)\n$$\n\n9. Из свойства коэффициента корреляции вытекает неравенство Коши – Буняковского:\n\n$$\n| \\text{cov} (\\varphi_1, \\varphi_2) | \\leq \\sqrt{D \\varphi_1 D \\varphi_2}\n$$\n\n$$\n1 \\leq \\sqrt{D \\hat\\theta D \\left( \\frac{\\partial \\ln p}{\\partial \\theta} \\right)} \\Rightarrow D \\hat{\\theta} \\geq \\frac{1}{D \\left( \\frac{\\partial \\ln p}{\\partial \\theta} \\right)}\n$$\n\n$$\nD \\left( \\frac{\\partial \\ln p}{\\partial \\theta} \\right) = M \\left( \\frac{\\partial \\ln p}{\\partial \\theta} \\right)^2 - M^2 \\left( \\frac{\\partial \\ln p}{\\partial \\theta} \\right) = M \\left( \\frac{\\partial \\ln p}{\\partial \\theta} \\right)^2 = I\n$$\n\n$$\n\\Rightarrow D \\hat{\\theta} \\geq \\frac{1}{I}\n$$\n\n---\n\n### Следствие: \nРавенство достигается, если $\\theta^*$ и $\\partial \\ln p / \\partial \\theta$ линейно зависимы.\n\n---\n## Формы информационного количества Фишера\n\n### Тождество для информационного количества:\n$$\n\\frac{\\partial \\ln p}{\\partial \\theta} = \\frac{\\partial p}{\\partial \\theta} \\frac{1}{p}\n$$\n\n### Вторая производная логарифма плотности:\n$$\n\\frac{\\partial^2 \\ln p}{\\partial \\theta^2} = -\\frac{1}{p^2} \\left( \\frac{\\partial p}{\\partial \\theta} \\right)^2 + \\frac{1}{p} \\frac{\\partial^2 p}{\\partial \\theta^2}\n$$\n\n### Информационное количество Фишера через вторую производную:\n$$\nI = -M \\left( \\frac{\\partial^2 \\ln p}{\\partial \\theta^2} \\right)\n$$\n\n### Формы для одномерной плотности $p(x, \\theta)$:\n$$\nI = nM \\left( \\frac{\\partial \\ln p(x, \\theta)}{\\partial \\theta} \\right)^2\n$$\n$$\nI = -nM \\left( \\frac{\\partial^2 \\ln p(x, \\theta)}{\\partial \\theta^2} \\right)\n$$\n---\n## Эффективные оценки\n\n### Определение эффективной оценки\nНесмещенная оценка $\\hat{\\theta}$ параметра $\\theta$ называется **эффективной**, если для любого $\\theta \\in \\Theta$ выполняется:\n$$\nD\\hat{\\theta} = \\frac{1}{I}\n$$\n### Замечание.\nЕсли оценка является эффективной, то она является оптимальной.\nОбратное не верно\n\n### Пример эффективной оценки\nДля распределения Пуассона $P_\\lambda$ оценка $\\hat{\\lambda} = \\bar{X}$ является эффективной, так как:\n$$\nD\\bar{X} = \\frac{\\lambda}{n} = \\frac{1}{I}\n$$\n### Показатель эффективности\nПоказатель эффективности несмещенной оценки $\\hat{\\theta}$ параметра $\\theta$ называется число:\n$$\ne(\\hat{\\theta}) = \\frac{1}{I \\cdot D\\hat{\\theta}},\\ 0<e(\\hat\\theta) \\leq 1\n$$\nДля эффективных оценок $e(\\hat{\\theta}) = 1$.\n\n---\n## Метод максимального правдоподобия\nПусть $\\xi$ - непрерывная случайная величина с плотностью $p(x, \\theta)$, где $\\theta$ - неизвестный параметр..\n\nТогда плотность распределения вектора $\\overline{X}$:\n$$\np(x_{1}\\dots x_{n},\\theta) = p(x_{1}, \\theta)* \\dots * p(x_{n}, \\theta)\n$$\n### Функция правдоподобия\nДля непрерывной случайной величины функция, рассматриваемая при фиксированных $(x_{1}\\dots x_{n})$ как функция параметра $\\theta$, называется функцией правдоподобия:\n$$\nL(x_1, \\dots, x_n, \\theta) = p(x_1, \\theta) \\cdot \\dots \\cdot p(x_n, \\theta)\n$$\n\nДля дискретной случайной величины:\n$$\nL(x_1, \\dots, x_n, \\theta) = P(\\xi = x_1) \\cdot \\dots \\cdot P(\\xi = x_n)\n$$\n### Суть метода\nМетод максимального правдоподобия заключается в нахождении значения параметра $\\theta$, которое максимизирует функцию правдоподобия $L(x_1, \\dots, x_n, \\theta)$.\n\n### Оценка максимального правдоподобия (о.м.п.)\nОценка $\\theta^*$, обеспечивающая по параметру $\\theta$ максимум функции правдоподобия, называется **оценкой максимального правдоподобия** параметра $\\theta$ (о.м.п.).\n\n### Уравнение правдоподобия\nДля нахождения оценки максимального правдоподобия решается уравнение:\n$$\n\\frac{\\partial \\ln L}{\\partial \\theta} = 0\n$$\n\nПосле нахождения критической точки необходимо проверить, что это точка максимума:\n$$\n\\frac{\\partial^2 \\ln L}{\\partial \\theta^2} < 0\n$$\n\n---\n### Пример (нерегулярная модель)\nНайти о.м.п. параметра $\\theta = (a, b)$ в равномерном распределении $R[a, b]$.\n\n### Решение\nФункция правдоподобия для равномерного распределения:\n$$\nL(X, \\theta) = \\prod_{i=1}^n \\frac{1}{b-a} = \\frac{1}{(b-a)^n}\n$$\n\nТак как функция $L$ монотонна по $a$ и $b$, наибольшее значение достигается при минимально возможном значении $b$ и максимально возможном значении $a$.\n\nТаким образом, оценки максимального правдоподобия для параметров $a$ и $b$:\n$$\n\\hat{a} = x_{max} = x^*_1, \\quad \\hat{b} = x_{min} = x^*_n\n$$\n\n---\n## Некоторые свойства оценок максимального правдоподобия\n\n### Cвойство 1\n\nДля нахождения оценки максимального правдоподобия (о.м.п.) можно выбирать наиболее удобную параметризацию, а о.м.п. получать затем с помощью соответствующих преобразований. Это свойство выражается следующим образом:\n\n$$\n\\overline{\\tau(\\theta)} = \\tau(\\overline{\\theta})\n$$\n\n### Пример:\nНайдем о.м.п. параметра $\\alpha^3$ в распределении $\\Gamma_{\\alpha,\\beta}$ при известном $\\beta$.\n\n**Решение:**\n\n$$\n\\alpha^3 = (\\hat{\\alpha})^3 = \\left( \\frac{\\beta}{\\overline{X}} \\right)^3\n$$\n\n### Свойство 2 \n\nОценки максимального правдоподобия обладают следующими асимптотическими свойствами:\n\n- **Асимптотическая несмещенность:** Оценки максимального правдоподобия становятся несмещенными при увеличении объема выборки.\n- **Состоятельность:** Оценки максимального правдоподобия сходятся по вероятности к истинному значению параметра при увеличении объема выборки.\n- **Асимптотическая нормальность:** При некоторых дополнительных условиях оценки максимального правдоподобия асимптотически нормальны.\n\n### Свойство 3\n\nЕсли оценки максимального правдоподобия асимптотически нормальны, то они также асимптотически эффективны, то есть:\n\n$$\nD\\hat{\\theta} \\rightarrow \\frac{1}{I}\n$$\n\nгде $I$ — информационное количество Фишера.\n\n---\n\n## Метод моментов\n\nМетод моментов заключается в приравнивании выборочных моментов к соответствующим теоретическим моментам распределения случайной величины $\\xi$.\n\n- Если распределение имеет **один параметр**, то составляют уравнение:\n\n$$\nM\\xi = \\bar{X}\n$$\n\n- Если распределение имеет **два параметра**, то составляют систему уравнений:\n\n$$\n\\begin{cases} \nM\\xi = \\bar{X}, \\\\ \nD\\xi = S^2\n\\end{cases}\n$$\n---\n## Пример: оценки параметров для распределений\n\n### 1. Показательное распределение\n\nДля показательного распределения:\n\n$$\nM\\xi = \\frac{1}{\\lambda} = \\bar{X} \\Rightarrow \\lambda = \\frac{1}{\\bar{X}}\n$$\n\n### 2. Равномерное распределение\n\nДля равномерного распределения:\n\n$$\n\\begin{cases}\nM\\xi = \\frac{a+b}{2} = \\bar{X}, \\\\\nD\\xi = \\frac{(b-a)^2}{12} = S^2\n\\end{cases}\n$$\n\n### 3. Нормальное распределение\n\nДля нормального распределения оценки параметров находятся аналогичным образом.\n\n---\n## Некоторые свойства оценок метода моментов\n\n### Теорема:\nПусть $\\hat{\\theta} = g(a_1, \\ldots, a_k)$ — оценка параметра $\\theta$, полученная по методу моментов, причем функция $g^{-1}$ непрерывна. Тогда $\\hat{\\theta}$ состоятельна.\n\n### Доказательство:\n\nЕсли $\\hat{\\theta} = g(a_1, \\ldots, a_k)$, то $\\theta = g(\\alpha_1, \\ldots, \\alpha_k)$.\n\nПо свойству выборочных моментов:\n\n$$\na_i \\xrightarrow{P} \\alpha_i \\quad \\text{при} \\quad n \\to \\infty\n$$\n\nТогда по теореме о сходимости по вероятности:\n\n$$\ng(a_1, \\ldots, a_k) \\xrightarrow{P} g(\\alpha_1, \\ldots, \\alpha_k)\n$$\n\nОткуда:\n\n$$\n\\hat{\\theta} \\xrightarrow{P} \\theta\n$$\n\n---\n\n## Метод наименьших\nОценка метода наименьших квадратов определяется из условия минимизации суммы квадратов отклонений выборочных данных от определяемой оценки.\n\n### Пример:\nНайти оценку метода наименьших квадратов $\\hat{\\theta}$ для генеральной средней $\\theta = \\bar{x}_0$.\n\n**Решение:**\n\nМинимизируем сумму квадратов отклонений:\n\n$$\nu = \\sum_{i=1}^n (x_i - \\theta)^2 \\to \\min\n$$\n\nПродифференцируем по $\\theta$ и приравняем к нулю:\n\n$$\n\\frac{du}{d\\theta} = -2\\sum_{i=1}^n (x_i - \\theta) = 0 \\implies \\sum_{i=1}^n x_i - \\theta n = 0 \\implies \\hat{\\theta} = \\frac{1}{n} \\sum_{i=1}^n x_i = \\bar{X}\n$$\n\nТаким образом, оценка метода наименьших квадратов для генеральной средней равна выборочному среднему $\\bar{X}$.",
    "created_at": "2025-03-27T13:48:41.940294"
  }
]