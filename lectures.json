[
  {
    "id": "физика-1742979969.377595",
    "subject": "Физика",
    "date": "2025-02-02",
    "title": "Лекция 2",
    "content": "# Статистическое оценивание параметров\n---\n## Неравенство Рао – Крамера\n\n### Плотность распределения\nПлотность распределения $p(x, \\theta)$ может быть определена как:\n$$\np(x, \\theta) = \n\\begin{cases} \np(x, \\theta), & \\text{если распределение непрерывно,} \\\\\nP_\\theta (X = y), & \\text{если распределение дискретно.}\n\\end{cases}\n$$\n\n### Вклад выборки\nВклад выборки в информационное количество Фишера выражается через производную логарифма плотности распределения:\n$$\n\\frac{\\partial \\ln p(x_1, x_2, \\dots, x_n, \\theta)}{\\partial \\theta}\n$$\n### Вклад одного элемента выборки:\n$$\n\\frac{\\partial \\ln p(x,\\theta)}{\\partial \\theta}\n$$\n### Информационное количество Фишера о параметре $\\theta$:\n$$\nI = M \\left( \\frac{\\partial \\ln p(x_1, x_2, \\dots, x_n, \\theta)}{\\partial \\theta} \\right)^2\n$$\n\n### Условия регулярности\n1. Множество значений $x$, для которых $p(x; \\theta) \\neq 0$, не зависит от $\\theta$.\n2. $p(x; \\theta)$ дифференцируема по параметру $\\theta$.\n3. Оцениваемая функция $\\tau(\\theta)$ дифференцируема по параметру $\\theta$.\n\n### Неравенство Рао – Крамера\nПри выполнении условий регулярности для любой оценки выполняется неравенство:\n$$\nD\\hat\\theta \\geq \\frac{1}{I}\n$$\n### Доказательство\n\n1. По свойству плотности:\n\n$$\n\\int p \\, dx = 1\n$$\n\n2. Из несмещенности оценки:\n\n$$\n\\int \\hat{\\theta} p \\, dx = M \\hat{\\theta} = \\theta\n$$\n\n3. Продифференцируем (1) по параметру:\n\n$$\n\\int \\frac{\\partial p}{\\partial \\theta} \\, dx = 0\n$$\n\n4. Домножим (3) на $\\theta$:\n\n$$\n\\int \\theta \\frac{\\partial p}{\\partial \\theta} \\, dx = 0\n$$\n\n5. Продифференцируем (2) по параметру:\n\n$$\n\\int \\hat{\\theta}\\frac{\\partial p}{\\partial \\theta} \\, dx = 1\n$$\n\n6. Вычтем (5) - (4):\n\n$$\n\\int (\\hat{\\theta} - \\theta) \\frac{\\partial p}{\\partial \\theta} \\, dx = 1\n$$\n\n7. Подставим выражение для $\\partial p / \\partial \\theta$:\n\n$$\n\\int (\\hat{\\theta} - \\theta) \\frac{\\partial \\ln p}{\\partial \\theta} \\, p \\, dx = 1\n$$\n\n8. Из курса теории вероятностей известно:\n\n$$\nM [(\\varphi_1 - M \\varphi_1)(\\varphi_2 - M \\varphi_2)] = \\text{cov}(\\varphi_1, \\varphi_2)\n$$\n\n9. Из свойства коэффициента корреляции вытекает неравенство Коши – Буняковского:\n\n$$\n| \\text{cov} (\\varphi_1, \\varphi_2) | \\leq \\sqrt{D \\varphi_1 D \\varphi_2}\n$$\n\n$$\n1 \\leq \\sqrt{D \\hat\\theta D \\left( \\frac{\\partial \\ln p}{\\partial \\theta} \\right)} \\Rightarrow D \\hat{\\theta} \\geq \\frac{1}{D \\left( \\frac{\\partial \\ln p}{\\partial \\theta} \\right)}\n$$\n\n$$\nD \\left( \\frac{\\partial \\ln p}{\\partial \\theta} \\right) = M \\left( \\frac{\\partial \\ln p}{\\partial \\theta} \\right)^2 - M^2 \\left( \\frac{\\partial \\ln p}{\\partial \\theta} \\right) = M \\left( \\frac{\\partial \\ln p}{\\partial \\theta} \\right)^2 = I\n$$\n\n$$\n\\Rightarrow D \\hat{\\theta} \\geq \\frac{1}{I}\n$$\n\n---\n\n### Следствие: \nРавенство достигается, если $\\theta^*$ и $\\partial \\ln p / \\partial \\theta$ линейно зависимы.\n\n---\n## Формы информационного количества Фишера\n\n### Тождество для информационного количества:\n$$\n\\frac{\\partial \\ln p}{\\partial \\theta} = \\frac{\\partial p}{\\partial \\theta} \\frac{1}{p}\n$$\n\n### Вторая производная логарифма плотности:\n$$\n\\frac{\\partial^2 \\ln p}{\\partial \\theta^2} = -\\frac{1}{p^2} \\left( \\frac{\\partial p}{\\partial \\theta} \\right)^2 + \\frac{1}{p} \\frac{\\partial^2 p}{\\partial \\theta^2}\n$$\n\n### Информационное количество Фишера через вторую производную:\n$$\nI = -M \\left( \\frac{\\partial^2 \\ln p}{\\partial \\theta^2} \\right)\n$$\n\n### Формы для одномерной плотности $p(x, \\theta)$:\n$$\nI = nM \\left( \\frac{\\partial \\ln p(x, \\theta)}{\\partial \\theta} \\right)^2\n$$\n$$\nI = -nM \\left( \\frac{\\partial^2 \\ln p(x, \\theta)}{\\partial \\theta^2} \\right)\n$$\n---\n## Эффективные оценки\n\n### Определение эффективной оценки\nНесмещенная оценка $\\hat{\\theta}$ параметра $\\theta$ называется **эффективной**, если для любого $\\theta \\in \\Theta$ выполняется:\n$$\nD\\hat{\\theta} = \\frac{1}{I}\n$$\n### Замечание.\nЕсли оценка является эффективной, то она является оптимальной.\nОбратное не верно\n\n### Пример эффективной оценки\nДля распределения Пуассона $P_\\lambda$ оценка $\\hat{\\lambda} = \\bar{X}$ является эффективной, так как:\n$$\nD\\bar{X} = \\frac{\\lambda}{n} = \\frac{1}{I}\n$$\n### Показатель эффективности\nПоказатель эффективности несмещенной оценки $\\hat{\\theta}$ параметра $\\theta$ называется число:\n$$\ne(\\hat{\\theta}) = \\frac{1}{I \\cdot D\\hat{\\theta}},\\ 0<e(\\hat\\theta) \\leq 1\n$$\nДля эффективных оценок $e(\\hat{\\theta}) = 1$.\n\n---\n## Метод максимального правдоподобия\nПусть $\\xi$ - непрерывная случайная величина с плотностью $p(x, \\theta)$, где $\\theta$ - неизвестный параметр..\n\nТогда плотность распределения вектора $\\overline{X}$:\n$$\np(x_{1}\\dots x_{n},\\theta) = p(x_{1}, \\theta)* \\dots * p(x_{n}, \\theta)\n$$\n### Функция правдоподобия\nДля непрерывной случайной величины функция, рассматриваемая при фиксированных $(x_{1}\\dots x_{n})$ как функция параметра $\\theta$, называется функцией правдоподобия:\n$$\nL(x_1, \\dots, x_n, \\theta) = p(x_1, \\theta) \\cdot \\dots \\cdot p(x_n, \\theta)\n$$\n\nДля дискретной случайной величины:\n$$\nL(x_1, \\dots, x_n, \\theta) = P(\\xi = x_1) \\cdot \\dots \\cdot P(\\xi = x_n)\n$$\n### Суть метода\nМетод максимального правдоподобия заключается в нахождении значения параметра $\\theta$, которое максимизирует функцию правдоподобия $L(x_1, \\dots, x_n, \\theta)$.\n\n### Оценка максимального правдоподобия (о.м.п.)\nОценка $\\theta^*$, обеспечивающая по параметру $\\theta$ максимум функции правдоподобия, называется **оценкой максимального правдоподобия** параметра $\\theta$ (о.м.п.).\n\n### Уравнение правдоподобия\nДля нахождения оценки максимального правдоподобия решается уравнение:\n$$\n\\frac{\\partial \\ln L}{\\partial \\theta} = 0\n$$\n\nПосле нахождения критической точки необходимо проверить, что это точка максимума:\n$$\n\\frac{\\partial^2 \\ln L}{\\partial \\theta^2} < 0\n$$\n\n---\n### Пример (нерегулярная модель)\nНайти о.м.п. параметра $\\theta = (a, b)$ в равномерном распределении $R[a, b]$.\n\n### Решение\nФункция правдоподобия для равномерного распределения:\n$$\nL(X, \\theta) = \\prod_{i=1}^n \\frac{1}{b-a} = \\frac{1}{(b-a)^n}\n$$\n\nТак как функция $L$ монотонна по $a$ и $b$, наибольшее значение достигается при минимально возможном значении $b$ и максимально возможном значении $a$.\n\nТаким образом, оценки максимального правдоподобия для параметров $a$ и $b$:\n$$\n\\hat{a} = x_{max} = x^*_1, \\quad \\hat{b} = x_{min} = x^*_n\n$$\n\n---\n## Некоторые свойства оценок максимального правдоподобия\n\n### Cвойство 1\n\nДля нахождения оценки максимального правдоподобия (о.м.п.) можно выбирать наиболее удобную параметризацию, а о.м.п. получать затем с помощью соответствующих преобразований. Это свойство выражается следующим образом:\n\n$$\n\\overline{\\tau(\\theta)} = \\tau(\\overline{\\theta})\n$$\n\n### Пример:\nНайдем о.м.п. параметра $\\alpha^3$ в распределении $\\Gamma_{\\alpha,\\beta}$ при известном $\\beta$.\n\n**Решение:**\n\n$$\n\\alpha^3 = (\\hat{\\alpha})^3 = \\left( \\frac{\\beta}{\\overline{X}} \\right)^3\n$$\n\n### Свойство 2 \n\nОценки максимального правдоподобия обладают следующими асимптотическими свойствами:\n\n- **Асимптотическая несмещенность:** Оценки максимального правдоподобия становятся несмещенными при увеличении объема выборки.\n- **Состоятельность:** Оценки максимального правдоподобия сходятся по вероятности к истинному значению параметра при увеличении объема выборки.\n- **Асимптотическая нормальность:** При некоторых дополнительных условиях оценки максимального правдоподобия асимптотически нормальны.\n\n### Свойство 3\n\nЕсли оценки максимального правдоподобия асимптотически нормальны, то они также асимптотически эффективны, то есть:\n\n$$\nD\\hat{\\theta} \\rightarrow \\frac{1}{I}\n$$\n\nгде $I$ — информационное количество Фишера.\n\n---\n\n## Метод моментов\n\nМетод моментов заключается в приравнивании выборочных моментов к соответствующим теоретическим моментам распределения случайной величины $\\xi$.\n\n- Если распределение имеет **один параметр**, то составляют уравнение:\n\n$$\nM\\xi = \\bar{X}\n$$\n\n- Если распределение имеет **два параметра**, то составляют систему уравнений:\n\n$$\n\\begin{cases} \nM\\xi = \\bar{X}, \\\\ \nD\\xi = S^2\n\\end{cases}\n$$\n---\n## Пример: оценки параметров для распределений\n\n### 1. Показательное распределение\n\nДля показательного распределения:\n\n$$\nM\\xi = \\frac{1}{\\lambda} = \\bar{X} \\Rightarrow \\lambda = \\frac{1}{\\bar{X}}\n$$\n\n### 2. Равномерное распределение\n\nДля равномерного распределения:\n\n$$\n\\begin{cases}\nM\\xi = \\frac{a+b}{2} = \\bar{X}, \\\\\nD\\xi = \\frac{(b-a)^2}{12} = S^2\n\\end{cases}\n$$\n\n### 3. Нормальное распределение\n\nДля нормального распределения оценки параметров находятся аналогичным образом.\n\n---\n## Некоторые свойства оценок метода моментов\n\n### Теорема:\nПусть $\\hat{\\theta} = g(a_1, \\ldots, a_k)$ — оценка параметра $\\theta$, полученная по методу моментов, причем функция $g^{-1}$ непрерывна. Тогда $\\hat{\\theta}$ состоятельна.\n\n### Доказательство:\n\nЕсли $\\hat{\\theta} = g(a_1, \\ldots, a_k)$, то $\\theta = g(\\alpha_1, \\ldots, \\alpha_k)$.\n\nПо свойству выборочных моментов:\n\n$$\na_i \\xrightarrow{P} \\alpha_i \\quad \\text{при} \\quad n \\to \\infty\n$$\n\nТогда по теореме о сходимости по вероятности:\n\n$$\ng(a_1, \\ldots, a_k) \\xrightarrow{P} g(\\alpha_1, \\ldots, \\alpha_k)\n$$\n\nОткуда:\n\n$$\n\\hat{\\theta} \\xrightarrow{P} \\theta\n$$\n\n---\n\n## Метод наименьших\nОценка метода наименьших квадратов определяется из условия минимизации суммы квадратов отклонений выборочных данных от определяемой оценки.\n\n### Пример:\nНайти оценку метода наименьших квадратов $\\hat{\\theta}$ для генеральной средней $\\theta = \\bar{x}_0$.\n\n**Решение:**\n\nМинимизируем сумму квадратов отклонений:\n\n$$\nu = \\sum_{i=1}^n (x_i - \\theta)^2 \\to \\min\n$$\n\nПродифференцируем по $\\theta$ и приравняем к нулю:\n\n$$\n\\frac{du}{d\\theta} = -2\\sum_{i=1}^n (x_i - \\theta) = 0 \\implies \\sum_{i=1}^n x_i - \\theta n = 0 \\implies \\hat{\\theta} = \\frac{1}{n} \\sum_{i=1}^n x_i = \\bar{X}\n$$\n\nТаким образом, оценка метода наименьших квадратов для генеральной средней равна выборочному среднему $\\bar{X}$.",
    "created_at": "2025-03-26T15:06:09.377595",
    "updated_at": "2025-03-26T15:06:09.377595"
  }
]