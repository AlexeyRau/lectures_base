[
  {
    "id": "1743060933.131187",
    "subject": "Машинное обучение и большие данные",
    "date": "2025-02-21",
    "title": "Лекция 21.02.25",
    "content": "## Меры центральной тенденции\n\n### **Пример**: \n\nРассмотрим показатели силы ветра по шкале Бофорта:  \n$[0, 2, 2, 1, 1, 3, 3, 1, 1, 0, 0, 1, 2]$ \n\nВариационный ряд по возрастанию:  \n$[0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 3, 3]$  \n\nМедиана (Me) = 1 – тихий ветер.\n\n| Сила ветра | Описание |\n|------------|----------|\n| 0 Галлов  | Штиль    |\n| 1 Галл    | Тихий ветер |\n| 2 Галла   | Лёгкий ветер |\n| 3 Галла   | Слабый ветер |\n| 4 Галла   | Умеренный ветер |\n| 5 Галлов  | Свежий ветер |\n| 6 Галлов  | Сильный ветер |\n| 7 Галлов  | Крепкий ветер |\n| 8 Галлов  | Очень крепкий ветер |\n| 9 Галлов  | Шторм |\n| 10 Галлов | Сильный шторм |\n| 11 Галлов | Жестокий шторм |\n| 12 Галлов | Ураган |\n\n---\n\n## Проблема выбросов\n\n**Выбросы** – это значения, которые значительно отличаются от остальных данных. Они могут негативно повлиять на среднее значение, но медиана более устойчива к выбросам.\n\n- **Выбросы** могут быть вызваны ошибками в данных или редкими событиями.\n- Для обнаружения выбросов можно использовать визуализацию или сравнение медианы и моды.\n\n---\n\n## Меры разброса (изменчивости)\n\nМеры разброса описывают, насколько данные отклоняются от центральной тенденции.\n\n- **Max** – максимальное значение.\n- **Min** – минимальное значение.\n- **Range (Размах)** = Max - Min.\n- **Midrange** = (Max + Min) / 2.\n- **Inter-Quartile Range (IQR)** – межквартильный размах.\n- **Дисперсия** и **Стандартное отклонение**.\n\n---\n\n### Размах и квартильный размах\n\n**Размах** – разность между максимальным и минимальным значениями:  \n$$ R = X_{\\text{max}} - X_{\\text{min}} $$\n\n**Квартильный размах** – разница между третьим и первым квартилями:  \n$$ IQR = Q_3 - Q_1 $$\n\nКвартили делят данные на четыре равные части:\n-  $(Q_1) \\text{ - 25\\% квартиль}$\n- $( Q_2 ) \\text{ - 50\\% квартиль}$\n- $( Q_1 ) \\text{ - 25\\% квартиль}$\n\n---\n\n## Дисперсия и стандартное отклонение\n\n**Дисперсия** измеряет отклонение значений от среднего:  \n$$ \\sigma^2 = \\frac{\\sum_{i=1}^n (x_i - \\overline{x})^2}{n} $$\n\n**Стандартное отклонение** – это квадратный корень из дисперсии:  \n$$ \\sigma = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\overline{x})^2}{n}} $$\n\nСтандартное отклонение выражает изменчивость в исходных единицах измерения.\n\n---\n\n## Нормальное распределение\n\nНормальное распределение (колоколообразная кривая) характеризуется следующими свойствами:\n- Около 68% данных лежат в пределах одного стандартного отклонения от среднего:  \n  $$ \\overline{x} \\pm 1\\sigma $$\n- Около 95% данных лежат в пределах двух стандартных отклонений:  \n  $$ \\overline{x} \\pm 2\\sigma $$\n- Около 99.7% данных лежат в пределах трех стандартных отклонений:  \n  $$ \\overline{x} \\pm 3\\sigma $$\n\n---\n\n## Нормализация и стандартизация данных\n\n**Нормализация** – преобразование данных к диапазону $[0, 1]$:  \n$$ X_{\\text{norm}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} $$\n\n**Стандартизация** – преобразование данных к среднему значению 0 и стандартному отклонению 1:  \n$$ Z = \\frac{x_i - \\overline{x}}{s} $$\n\n---\n\n## Визуализация данных\n\n### Гистограмма\nГистограмма показывает распределение данных по интервалам (бинам). Она используется для непрерывных переменных.\n\n### Диаграмма рассеяния (Scatter Plot)\nДиаграмма рассеяния показывает взаимосвязь между двумя переменными. Она помогает выявить корреляции.\n\n### Тепловая карта (Heat Map)\nТепловая карта используется для визуализации корреляций между переменными. Коэффициент корреляции находится в интервале $[-1, 1]$.\n\n---\n\n## Обработка пропущенных значений\n\n1. **Удаление пропущенных значений** – если пропусков мало.\n2. **Замена средним значением** – если нет выбросов.\n3. **Замена медианой** – если есть выбросы.\n4. **Замена модой** – для категориальных данных.\n5. **Регрессия** – для прогнозирования пропущенных значений.\n\n---\n\n## Ключевые идеи\n\n- **Среднее значение** – типичное значение в наборе данных, но не устойчиво к выбросам.\n- **Медиана** – центральное значение, устойчиво к выбросам.\n- **Мода** – наиболее часто встречающееся значение.\n- **Дисперсия** и **стандартное отклонение** – меры разброса данных.\n- **Нормализация** и **стандартизация** – важные этапы предобработки данных.\n- **Визуализация** – помогает понять структуру и взаимосвязи в данных.\n\n---\n\n## Инструменты визуализации\n\n- **Matplotlib** – библиотека для построения графиков.\n- **Seaborn** – библиотека для статистической визуализации.\n\n---\n\n## Пример кода для стандартизации и нормализации\n\n```python\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nimport numpy as np\n\n# Пример данных\ndata = np.array([[1, 2], [3, 4], [5, 6]])\n\n# Стандартизация\nscaler = StandardScaler()\nstandardized_data = scaler.fit_transform(data)\n\n# Нормализация\nminmax_scaler = MinMaxScaler()\nscaled_data = minmax_scaler.fit_transform(data)\n\nprint(\"Стандартизированные данные:\\n\", standardized_data)\nprint(\"Нормализованные данные:\\n\", scaled_data)",
    "created_at": "2025-03-27T13:35:33.131187"
  },
  {
    "id": "1743060984.569695",
    "subject": "Машинное обучение и большие данные",
    "date": "2025-03-14",
    "title": "Лекция 14.03.25",
    "content": "## Введение в линейную регрессию\n\nЛинейная регрессия — это метод моделирования зависимости между зависимой переменной (откликом) и одной или несколькими независимыми переменными (предикторами). \n\nОсновная цель — построить математическую модель, которая наилучшим образом описывает связь между переменными.\n\n### Общее уравнение линейной регрессии:\n$$ y = ax + b $$\n\nВ Data Science и машинном обучении это уравнение часто записывается как:\n$$ Y = b_0 + b_1 X $$\n\n### Описание параметров:\n- $y$ или $Y$ — переменная ответа (зависимая переменная).\n- $x$ или $X$ — предикторная переменная (независимая переменная).\n- $a$ и $b$ или $b_0$ и $b_1$ — коэффициенты регрессии:\n\t  - $b_0$ — пересечение (intercept).\n\t  - $b_1$ — наклон (slope).\n\n## Ключевые термины\n\n- **Отклик (Response)**: Переменная, которую пытаемся предсказать. Cинонимы: зависимая переменная, целевая переменная.\n  \n- **Независимая переменная (Independent Variable)**: Переменная, используемая для предсказания отклика. Синонимы: предиктор, признак.\n  \n- **Запись (Record)**: Вектор, состоящий из значений предикторов и значений отклика для одного элемента данных. Синонимы: строка, случай, пример\n\n- **Пересечение (Intercept)**: Предсказанное значение, когда $X = 0$. Синонимы: $b_0$, точка пересечения.\n  \n- **Коэффициент регрессии (Regression Coefficient)**: Наклон регрессионной прямой. Синонимы: наклон, $b_1$, вес.\n\n## Линейные модели регрессии\n\nЛинейные модели регрессии используются для прогнозирования непрерывных величин. В зависимости от количества признаков, выделяют:\n\n- **Парная регрессия** (однофакторная):\n  $$ y = w x + b $$\n\n- **Множественная регрессия** (многофакторная):\n  $$ y = w_1 x_1 + w_2 x_2 + \\ldots + w_n x_n + b $$\n\n### Постановка задачи регрессии\n\nДано: коллекция размеченных данных $\\{(x_i, y_i)\\}_{i=1}^N$, где $N$ — размер коллекции, $x_i$ — $D$-мерный вектор признаков образца $i = 1, ..., N$, $y_i$ — действительное целевое значение.\n\nТребуется: построить модель $f_{w,b}(x)$, которая является линейной комбинацией признаков:\n$$ f_{w,b}(x) = xw + b $$\n\nгде:\n- $w$ — вектор параметров (весов).\n- $b$ — свободный коэффициент (сдвиг).\n\n## Метод наименьших квадратов (МНК)\n\nМетод наименьших квадратов (МНК) используется для нахождения оптимальных параметров линейной регрессии, минимизирующих сумму квадратов ошибок (остатков):\n\n$$ \\min_{w, b} \\sum_{i=1}^n (w^T x_i + b - y_i)^2 $$\n\n### Функция потерь (MSE)\nСреднеквадратичная ошибка (MSE) вычисляется как:\n$$ MSE = \\frac{1}{m} \\sum_{i=1}^m (y_i - \\hat{y}_i)^2 $$\n\n### Коэффициент детерминации $R^2$\nКоэффициент детерминации $R^2$ показывает, насколько хорошо модель объясняет дисперсию данных:\n$$ R^2 = 1 - \\frac{\\sum_{i=1}^n (\\hat{y}_i - y_i)^2}{\\sum_{i=1}^n (\\bar{y}_i - y_i)^2} $$\n\n## Регуляризация\n\nРегуляризация используется для предотвращения переобучения модели. Основные виды регуляризации:\n\n1. **L1-регуляризация (Lasso)**:\n   $$ \\min_{w, b} \\left[ \\frac{1}{N} \\sum_{i=1}^N (y_i - f_{w,b}(X_i))^2 + \\lambda \\sum_{j=1}^D |w_j| \\right] $$\n\n2. **L2-регуляризация (Ridge)**:\n   $$ \\min_{w, b} \\left[ \\frac{1}{N} \\sum_{i=1}^N (y_i - f_{w,b}(X_i))^2 + \\lambda \\sum_{j=1}^D w_j^2 \\right] $$\n\n3. **Elastic Net** (комбинация L1 и L2):\n   $$ \\min_{w, b} \\left[ \\frac{1}{N} \\sum_{i=1}^N (y_i - f_{w,b}(X_i))^2 + \\lambda_1 \\sum_{j=1}^D |w_j| + \\lambda_2 \\sum_{j=1}^D w_j^2 \\right] $$\n\n## Метрики качества модели\n\nДля оценки качества модели регрессии используются следующие метрики:\n\n- **MAE (Mean Absolute Error)**:\n  $$ MAE = \\frac{1}{m} \\sum_{i=1}^m |y_i - \\hat{y}_i| $$\n\n- **RMSE (Root Mean Squared Error)**:\n  $$ RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2} $$\n\n- **MAPE (Mean Absolute Percentage Error)**:\n  $$ MAPE = \\frac{1}{n} \\sum_{i=1}^n \\frac{|y_i - \\hat{y}_i|}{|y_i|} \\times 100\\% $$\n",
    "created_at": "2025-03-27T13:36:24.569695"
  },
  {
    "id": "1743061009.628635",
    "subject": "Машинное обучение и большие данные",
    "date": "2025-02-14",
    "title": "Лекция 14.02.25",
    "content": "## Ключевые идеи\n- **EDA** — итеративный процесс, который может потребовать возвращения к предыдущим этапам по мере получения новых инсайтов из данных.\n  - Это означает, что EDA не является линейным процессом. По мере анализа данных могут возникать новые вопросы, которые потребуют повторного изучения уже пройденных этапов.\n- Основная цель EDA — лучше понять данные и подготовить их для последующего анализа или моделирования.\n  - EDA помогает выявить закономерности, аномалии и взаимосвязи в данных, что важно для построения точных моделей.\n---\n## Размер и размерность данных\n- **Размер данных** относится к числу объектов данных.\n  - Например, в датасете с информацией о студентах размер данных будет равен количеству студентов.\n- **Размерность данных** относится к числу атрибутов (признаков).\n  - Например, если у каждого студента есть возраст, пол и оценка, то размерность данных будет равна 3.\n---\n## Типы данных\n### Числовые (Numeric)\n- Выражены числами (например, возраст, вес, размер обуви).\n  - **Непрерывные**: неограниченное число вариантов (например, возраст, вес, давление).\n    - Эти данные могут принимать любое значение в пределах определенного диапазона.\n  - **Дискретные**: ограниченное число вариантов (например, размер обуви, количество учеников).\n    - Эти данные принимают только целые значения.\n\n### Категориальные\n- Выражены словами (например, цвет глаз, пол, группа крови).\n  - **Порядковые (Ordinal)**: имеют иерархию (например, настроение, оценка качества обслуживания).\n    - Эти данные можно упорядочить, но разница между значениями не количественная.\n  - **Номинальные (Nominal)**: не имеют иерархии (например, цвет глаз, группа крови).\n    - Эти данные представляют собой просто категории без какого-либо порядка.\n---\n## Порядковые данные (Ordinal Data)\n- Значения имеют осмысленный порядок (например, оценки: A, B, C, D; размеры порций: маленькие, средние, большие).\n  - Например, оценка \"A\" лучше, чем \"B\", но мы не можем точно сказать, насколько лучше.\n- Количественной разницы между двумя уровнями не обнаружено.\n  - Это означает, что разница между \"A\" и \"B\" может быть не такой же, как между \"B\" и \"C\".\n---\n## Выбор признаков\n- Удаление признаков, не имеющих отношения к задаче (например, Student ID).\n  - Например, если мы анализируем доходы студентов, то идентификатор студента (Student ID) не имеет значения для анализа.\n  - Удаление лишних признаков помогает упростить модель и улучшить её производительность.\n---\n## Основы описательной статистики\n### Меры центральной тенденции\n- Данные распределены вокруг этого «центра».\n  - Эти меры помогают понять, где находится \"центр\" данных.\n- Вычисляется для каждого атрибута.\n- Три распространенных типа:\n  - **Среднее**: сумма всех значений, деленная на их количество.\n  - **Мода**: наиболее часто встречающееся значение.\n  - **Медиана**: значение, которое находится в середине упорядоченного набора данных.\n- Эти меры не дают информации относительно экстремальных значений в распределении данных и разброса данных.\n  - Например, среднее может быть искажено выбросами.\n\n### Среднее арифметическое\n- Чувствительно к выбросам.\n  - Например, если в данных есть одно очень большое значение, среднее будет значительно выше, чем большинство значений в наборе данных.\n---\n## Этапы EDA\n1. **Изучение распределения данных.**\n   - Это помогает понять, как данные распределены и есть ли в них выбросы.\n2. **Обработка пропущенных значений** (наиболее распространенная проблема в каждом датасете).\n   - Пропущенные значения могут быть заполнены средним значением, медианой или удалены.\n3. **Обработка отклонений.**\n   - Выбросы могут быть удалены или заменены на более репрезентативные значения.\n4. **Удаление дубликатов данных.**\n   - Дубликаты могут исказить результаты анализа.\n5. **Кодирование категориальных переменных.**\n   - Категориальные данные могут быть преобразованы в числовые для использования в моделях.\n6. **Нормализация и масштабирование.**\n   - Это помогает привести данные к одному масштабу, что важно для многих алгоритмов машинного обучения.\n---\n## Двумерный анализ\n- Исследует отношения между двумя переменными.\n  - Например, можно исследовать связь между возрастом и доходом.\n- Помогает находить корреляции, отношения и зависимости между парами переменных.\n- Используемые методы:\n  - **Диаграммы рассеяния**: визуализация зависимости между двумя переменными.\n  - **Регрессионный анализ**: позволяет предсказать одну переменную на основе другой.\n  - **Корреляционный анализ**: измеряет силу и направление связи между переменными.\n---\n## Предварительная обработка данных\n- **Очистка данных**: удаление дубликатов, обработка пропущенных значений, исправление ошибок в данных.\n  - Это важный этап, так как качество данных напрямую влияет на качество анализа.\n- **Преобразование данных**: изменение формата данных, кодирование категориальных переменных, нормализация или стандартизация числовых признаков.\n  - Например, категориальные данные могут быть преобразованы в числовые с помощью one-hot encoding.\n---\n## Визуализация данных\n- Построение графиков и диаграмм для визуального представления данных.\n  - **Гистограммы**: показывают распределение данных.\n  - **Ящики с усами (Box Plots)**: показывают медиану, квартили и выбросы.\n  - **Диаграммы рассеяния (Scatter Plots)**: показывают связь между двумя переменными.\n  - **Тепловые карты (HeatMaps)**: показывают интенсивность значений в таблице данных.\n- Визуализация помогает выявить закономерности, аномалии и взаимосвязи между переменными.\n  - Например, тепловая карта может показать, какие переменные наиболее сильно коррелируют друг с другом.\n---\n",
    "created_at": "2025-03-27T13:36:49.628635"
  },
  {
    "id": "1743061044.507574",
    "subject": "Машинное обучение и большие данные",
    "date": "2025-03-07",
    "title": "Лекция 07.03.25",
    "content": "### Что такое машинное обучение?\nМашинное обучение (ML) — это процесс разработки алгоритмов или моделей, которые могут предсказывать значения целевой переменной (Y) для новых объектов (x) на основе обучающих данных.\n\n### Модель алгоритма\n- Модель алгоритма$a$— это параметрическое семейство функций$g : X \\to Y$или $g(x, \\theta)$, где $\\theta \\in \\Theta$— параметры.\n- Процесс подбора оптимальной функции$g$и параметров$\\theta$называется **настройкой** или **обучением** алгоритма.\n\n### Признаковое описание объекта\n- Признаки объекта $x$ можно записать в виде вектора: $x_1, ..., x_d$.\n- Признаковое пространство определяется количеством признаков. Например, три признака образуют трехмерное пространство.\n\n### Обучающая выборка\n- Обучающая выборка — это набор объектов $x$, для которых известны значения целевой переменной $y$.\n- Обучающая выборка может быть разделена на:\n  - Тренировочную выборку (для обучения модели).\n  - Валидационную выборку (для настройки гиперпараметров).\n  - Тестовую выборку (для оценки качества модели).\n\n---\n## Обучение с учителем (Supervised Learning)\n\n### Основные понятия\n- В обучении с учителем данные представлены в виде пар $(x_i, y_i)$, где $x_i$— вектор признаков, а $y_i$ — метка.\n- Метка $y_i$ может быть элементом конечного множества классов, вещественным числом или более сложной структурой.\n\n### Цель обучения с учителем\n- Цель — создать модель, которая принимает вектор признаков $x$ и возвращает информацию, позволяющую определить метку для этого вектора.\n- Пример: модель, предсказывающая вероятность заболевания раком на основе характеристик пациента.\n\n### Оценщики (Estimators)\n- В Scikit-Learn модели называются **оценщиками**.\n- Основные методы оценщиков:\n  - `fit(X, y)` — обучение модели.\n  - `predict(X)` — предсказание на новых данных.\n  - `transform(X)` — преобразование данных.\n\n---\n## Классификация и регрессия\n\n### Классификация (Classification)\n- **Целевая переменная (target)** — дискретная.\n- Прогнозируется метка класса из заранее определенного списка.\n- Примеры:\n  - Бинарная классификация: $Y = \\{0, 1\\}$ (да/нет, положительный/отрицательный).\n  - Многоклассовая классификация: $Y = \\{0, 1, 2, ...\\}$.\n\n### Регрессия (Regression)\n- **Целевая переменная (target)** — непрерывная.\n- Прогнозируется вещественное число.\n- Пример: предсказание стоимости квартиры на основе её характеристик.\n\n---\n## Процесс машинного обучения\n\n### Этапы машинного обучения\n1. **Обучение алгоритма**:\n   - Модель изучает данные с известными ответами и учится делать предсказания.\n   - Цель: минимизировать отклонение предсказаний $a(x)$ от правильных ответов $y$.\n\n2. **Применение алгоритма**:\n   - Обученная модель используется для предсказания на новых данных.\n   - Пример: предсказание стоимости квартир, для которых неизвестна цена.\n\n### Разделение данных\n- Данные делятся на:\n  - Обучающую выборку $(X_{train}, y_{train})$.\n  - Тестовую выборку $(X_{test}, y_{test})$.\n- Модель обучается на обучающей выборке и оценивается на тестовой.\n\n---\n## Гиперпараметры и проблемы машинного обучения\n\n### Гиперпараметры модели\n- **Гиперпараметры** — параметры, задаваемые до начала обучения (например, шаг градиентного спуска).\n- **Параметры модели** — параметры, оптимизируемые в процессе обучения (например, веса в нейронной сети).\n\n### Основные проблемы ML\n1. **Недостаточный объем обучающей выборки**.\n2. **Пропуски в данных**.\n3. **Переобучение (Overfitting)**:\n   - Модель слишком сложна и хорошо работает на обучающих данных, но плохо на новых.\n4. **Недобучение (Underfitting)**:\n   - Модель слишком проста и не может хорошо описать данные.\n\n---\n## Линейная регрессия\n\n### Понятие линейной регрессии\n- Линейная регрессия — это статистический метод, моделирующий линейную зависимость между зависимой переменной и одной или несколькими независимыми переменными.\n- Математически линейная зависимость представляет прямую линию на графике.\n\n### Регрессионный анализ\n- **Регрессионный анализ** — инструмент для установления модели отношений между переменными.\n- **Линейная регрессия**: зависимая и независимая переменные связаны через уравнение, где показатель степени равен 1.\n- **Нелинейная регрессия**: описывает нелинейные отношения, где показатель степени любой переменной не равен 1.\n\n### Типы регрессии\n- **Простая линейная регрессия**: одна независимая переменная.\n- **Множественная линейная регрессия**: несколько независимых переменных.\n- **Полиномиальная регрессия**: нелинейная зависимость, описываемая полиномом.\n\n---\n",
    "created_at": "2025-03-27T13:37:24.507574"
  },
  {
    "id": "1743061073.192123",
    "subject": "Машинное обучение и большие данные",
    "date": "2025-02-07",
    "title": "Лекция 07.02.25",
    "content": "# 1. Введение в ML\n\n### Что такое Искусственный Интеллект (ИИ)?\nИскусственный интеллект (ИИ) — это область компьютерных наук, которая занимается созданием систем, способных выполнять задачи, требующие человеческого интеллекта. Эти задачи включают в себя обучение, рассуждение, восприятие, понимание естественного языка и принятие решений.\n\n### Что такое Машинное Обучение (ML)?\nМашинное обучение (ML) — это подраздел ИИ, который фокусируется на разработке алгоритмов и моделей, позволяющих компьютерам обучаться на данных и делать прогнозы или принимать решения без явного программирования.\n\n### Типы ИИ:\n1. Слабый ИИ (Narrow AI): Системы, предназначенные для выполнения конкретных задач (например, распознавание лиц, голосовые помощники).\n2. Сильный ИИ (General AI): Гипотетические системы, способные выполнять любые интеллектуальные задачи, которые может выполнять человек.\n3. Супер-ИИ (Superintelligence): Гипотетические системы, превосходящие человеческий интеллект во всех аспектах.\n\n### Области применения ИИ:\n- Обработка естественного языка (NLP)\n- Компьютерное зрение\n- Робототехника\n- Рекомендательные системы\n- Автономные транспортные средства\n\n### Виды Машинного Обучения:\n4. Обучение с учителем (Supervised Learning): Модель обучается на размеченных данных, где каждому примеру соответствует правильный ответ.\n5. Обучение без учителя (Unsupervised Learning): Модель обучается на данных без меток, выявляя скрытые структуры или паттерны.\n6. Обучение с подкреплением (Reinforcement Learning): Модель обучается через взаимодействие с окружающей средой, получая обратную связь в виде наград или штрафов.\n\n# 2. Типы задач в ML\n\n### Классификация:\nЗадача, при которой модель должна отнести входные данные к одной из нескольких категорий. Например, распознавание спама в электронной почте.\n\n### Регрессия:\nЗадача, при которой модель должна предсказать непрерывное значение. Например, предсказание цены дома на основе его характеристик.\n\n### Кластеризация:\nЗадача, при которой модель группирует данные на основе их сходства. Например, сегментация клиентов по их покупательскому поведению.\n\n### Понижение размерности:\nЗадача, при которой модель уменьшает количество признаков в данных, сохраняя при этом важную информацию. Например, визуализация многомерных данных.\n\n### Генерация данных:\nЗадача, при которой модель создает новые данные, похожие на те, на которых она обучалась. Например, генерация изображений или текста.\n\n# 3. Основные понятия ML\n\n### Данные:\nДанные — это основа машинного обучения. Они могут быть структурированными (например, таблицы) или неструктурированными (например, текст, изображения).\n\n### Признаки (Features):\nПризнаки — это характеристики данных, которые используются для обучения модели. Например, для задачи классификации изображений признаками могут быть пиксели изображения.\n\n### Алгоритмы или модели:\nАлгоритмы — это математические методы, которые используются для обучения моделей. Модели — это результат обучения алгоритма на данных.\n\n### Прямоугольные данные:\nПрямоугольные данные — это данные, представленные в виде таблицы, где строки представляют собой примеры, а столбцы — признаки.\n\n### Кадр данных (DataFrame):\nКадр данных — это структура данных, используемая для работы с прямоугольными данными. Например, в библиотеке Pandas в Python используется DataFrame.\n\n### Схема процесса ML:\n7. Сбор данных: Сбор и подготовка данных для обучения.\n8. Предобработка данных: Очистка, нормализация и преобразование данных.\n9. Выбор модели: Выбор подходящего алгоритма для задачи.\n10. Обучение модели: Обучение модели на тренировочных данных.\n11. Оценка модели: Проверка качества модели на тестовых данных.\n12. Настройка модели: Оптимизация параметров модели для улучшения качества.\n13. Прогнозирование: Использование модели для предсказания на новых данных.\n",
    "created_at": "2025-03-27T13:37:53.192123"
  },
  {
    "id": "1743061354.170786",
    "subject": "Базы данных",
    "date": "2025-02-14",
    "title": "Введение",
    "content": "## Раздел 1.1\n\n**База данных** - совокупность специальным образом организованных данных, которая:\n1) Подлежит долговременному хранению памяти ЭВМ\n2) Содержит информацию о сравнительно небольшом (фиксированном) количестве классов объектов\n3) Все классы объектов относятся к одному предмету прикладной области\n4) Используется в одном или нескольких приложениях\n\n---\n**Жизненный цикл данных существенно больше жизненного цикла программ, которые с ними работают**\n\n---\n## Раздел 1.2 \n\n**Специфическое свойство баз данных, отличающее их от других инфо-систем**:\n1) В отличие от систем ИИ, данные в БД пассивны - они не содержат информации о том, как ими пользоваться\n2) Соединение без потери информации\n\n---\n## Раздел 1.3\n\n**База данных должна быть одна на логическом уровне**\n\n---\n## Раздел 2.1\n\n**Категории баз данных:**\n1) Физический уровень:\n\t1) Поле:\n\t     БД - наименьшая единица памяти с определенным адресом и размером\n\t2) Физическая запись:\n\t     Упорядоченная последовательность фиксированного количества полей\n\t     Две записи однотипны, если они состоят из одинаковых последовательностей полей\n\t3) Память: \n\t     Совокупность однотипных физических записей\n\t4) Блок:\n\t\t Объем информации, передаваемый из внешний памяти в оперативную за одно обращение\n\t5) Индексный файл:\n\t\t Структурированная организация физических записей, предназначенная для поиска данных и обеспечения ограничения целостности на данных\n\n2) Логический уровень:\n\t1) Атрибут (элементарная единица):\n\t\t Наименьший элемент информации с определенным типом и наименованием\n\t\t \n\t2) Логическая запись:\n\t\t Совокупность фиксированного количества элементов данных\n\t\t Обычно соответствует физической записи\n\t3) Отношение:\n\t\t Совокупность однотипных логических записей\n\t4) Схема отношений\n\t5) Схема БД:\n\t\t Совокупность схем отношений с установленными ограничениями целостности\n\t\t На ней подчеркнуты первичные ключи отношений\n\n---\n## Раздел 2.2\n\n**Первичный ключ (Primary Key)** - атрибут или совокупность, которая однозначна определяет логическую запись\n  \n*За одним сотрудником можно закрепить несколько единиц оборудования\nЗа одним оборудованием можно закрепить несколько сотрудников\n\n**Стрелки на схеме** - ссылочная целостность, которая обеспечивается Foreign Key\n\n---\n## Раздел 2.3 (Требования к БД)\n\n### 1. Не избыточность и непротиворечивость\n\n- **Дублирование данных:**\n\t  1) В индексных файлах.\n\t  2) На различных серверах (зеркала).\n- **Контроль избыточности:** За этим следит СУБД (Система Управления Базами Данных).\n\n---\n\n### 2. Защита от программных и аппаратных сбоев\n\n#### Типы сбоев:\n1. **Логический сбой:**\n   - Возникает, когда запись уже существует (нарушение ограничения сущности).\n   - СУБД должна отвергнуть такую операцию (ошибка 1 рода).\n2. **Физический сбой:**\n   - Возникает при удалении записи, на которую есть ссылки в других таблицах (нарушение ссылочной целостности).\n   - СУБД должна отвергнуть такую операцию (ошибка 2 рода).\n   - Происходит аварийное выключение без изменения структуры данных.\n\n#### Проблемы с цепочками записей:\n- Если записи в основном файле связаны в цепь, разрушение одного указателя приводит к потере всех последующих записей.\n\n#### Меры защиты:\n- **Ведение системного журнала:** перед выполнением операции СУБД помещает в журнал информацию, достаточную для завершения операции после повторного старта.\n- **Архивация данных:** cоздаются две копии текущего состояния БД:\n    1. Первая копия хранится в несгораемом сейфе на рабочем месте.\n    2. Вторая копия передается в секретные государственные или коммерческие службы (первый отдел).\n- **Триггеры:** используются для автоматического выполнения действий при определенных событиях в БД.\n\n---\n\n### 3. Независимость данных\n\n#### Прикладная программа:\n1. Программа, реализующая отдельную функцию и взаимодействующая с БД.\n2. **Мобильность:** Программа считается мобильной, если ее исходный код не зависит от операционной среды и оборудования.\n3. **Корректность:** Программа написана правильно, если она мобильна и не зависит от места и способа хранения данных.\n\n#### Реализация принципа независимости данных:\n- Необходима независимость от приложения, места и способа хранения данных.\n- Для реализации этого принципа рабочей группой **CODASYL** была предложена **трехуровневая модель** описания и представления данных.\n\n---\n",
    "created_at": "2025-03-27T13:42:34.170786"
  },
  {
    "id": "1743061580.762605",
    "subject": "Математическая логика и теория алгоритмов",
    "date": "2025-02-04",
    "title": "Алфавит и слова",
    "content": "## **Алфавит**\nАлфавит — любое непустое множество **Ā**.\n\n---\n\n## **Элементы алфавита**\nЭлементы алфавита называются **буквами** (или **символами**) алфавита **Ā**.\n\n---\n\n## **Слово (на алфавите)**\nСлово — любая последовательность букв алфавита. Обозначается как **w**.\n\n---\n\n## **Длина слова**\nДлина слова **w** обозначается как **L(w)**. Это число символов в слове.\n\n---\n\n## **Пустое слово**\nПустое слово обозначается как **∅**. Это последовательность нулевой длины, то есть **L(∅) = 0**.\n\n---\n\n## **Конкатенация**\nКонкатенация — бинарная алгебраическая операция **\\*** (звездочка), заданная на множестве **W(Ā)** всех слов на алфавите **Ā** следующим образом:  \n**W(Ā)^2 → W(Ā)**  \n\n1. Если **w ∈ W(Ā)** — слово на алфавите **Ā**, то:  \n   **∅ \\* w = w = w \\* ∅**  \n2. Если **w** и **v** — два непустых слова, то:  \n   **w \\* v = a₁a₂...aₙb₁b₂...bₘ**,  \n   где **a₁a₂...aₙ** — буквы слова **w**, а **b₁b₂...bₘ** — буквы слова **v**.\n\n---\n### Примеры\n1. Пусть алфавит **Ā = {a, b}**.  \n   - Слово **w = aab**.  \n   - Длина слова **L(w) = 3**.  \n   - Конкатенация слов **w = aab** и **v = ba**:  \n     **w \\* v = aabba**.\n\n2. Пустое слово **∅** при конкатенации:  \n   - **∅ \\* ab = ab**  \n   - **ab \\* ∅ = ab**.\n\n---\n## **Теорема**\nКонкатенация на множестве **W(Ā)** задает структуру моноида:\n- **Нейтральный элемент** — пустое слово **∅**.\n- **Ассоциативность**: для любых слов **u**, **v**, **w** выполняется **(u \\* v) \\* w = u \\* (v \\* w)**.\n- **Коммутативность отсутствует**, если **|Ā| > 1** (то есть алфавит содержит более одного символа).\n\n---\n\n## **Замечание**\n1. Каждая буква алфавита **Ā** — это слово длины **1**.\n2. Каждое слово — это конкатенация входящих в него букв.\n\n---\n\n## **Под-слово**\nСлово **θ** называется **под-словом** слова **w** (обозначается **θ ⏴ w**), если существуют такие слова **a** и **b** из **W(Ā)**, что:  **w = aθb**,  причем **a** и **b** могут быть **∅**.\n**∅** всегда является под-словом любого слова, включая само **∅**.\n\n---\n\n## **Вхождение под-слова**\n**Вхождение под-слова θ в слово w** — это упорядоченная тройка **ξ = ⟨a, θ, b⟩**, где: **w = aθb**.\n\n---\n\n## **Замена вхождения**\nПусть **θ** — под-слово слова **w**, а **ξ = (a, θ, b)** — некоторое его вхождение. Если **X** — слово на алфавите **Ā**, то слово $$w_{\\xi}[X] = aXb,$$\nполученное заменой под-слова θ, называется результатом замены вхождения ξ на X.\n\n---\n\n## **Замена нескольких вхождений**\nЕсли **w** — слово, $$a_1, \\dots, a_n - \\text{попарно различные буквы алфавита Ā} $$и $$X_1, \\dots, X_n - \\text{ слова из W(Ā),}$$то через $$w(a_1, \\dots, a_n)[X_1, \\dots, X_n] $$обозначается результат замены каждого вхождения **aᵢ** на слово **Xᵢ**.\n\n---\n",
    "created_at": "2025-03-27T13:46:20.762605"
  },
  {
    "id": "1743061595.459983",
    "subject": "Математическая логика и теория алгоритмов",
    "date": "2025-02-04",
    "title": "Формализованные языки",
    "content": "## **Формализованный язык**\nФормализованным языком на алфавите $\\bar{A}$ называется упорядоченная пара $\\mathfrak{L} = \\langle \\bar{A}, \\mathcal{E} \\rangle$, где $\\mathcal{E} \\subseteq W(\\bar{A})$ — множество выделенных слов на алфавите, называемое множеством выражений языка $\\mathfrak{L}$.\n\nЕсли $\\mathfrak{L}_1$ и $\\mathfrak{L}_2$ — два языка, то при $A_1 \\subseteq A_2$ и $\\mathcal{E}_1 \\subseteq \\mathcal{E}_2$, то $\\mathfrak{L}_2$ — расширение $\\mathfrak{L}_1$.\n\nПри этом, если $\\mathcal{E}_1 = \\mathcal{E}_2$, то расширение называется **алфавитным**.  \nИначе, если $A_1 = A_2$, то расширение называется **алфавитно-инвариантным**.\n\n---\n### Примечание:  \nЛюбой язык является несущественным расширением самого себя.\n\nЕсли $\\mathfrak{L}_2$ — расширение $\\mathfrak{L}_1$, то $\\mathfrak{L}_1 \\subseteq \\mathfrak{L}_2$.\n\n---\n## **Логическая структура**\n\n**Логическая сигнатура нулевого порядка** — упорядоченная тройка $\\Omega = \\langle L_2, L_1, L_0 \\rangle$, где $L_i$ — некоторые множества, такие что $\\text{smb}(\\Omega) = L_0 \\cup L_1 \\cup L_2 \\neq \\emptyset$, и их попарное пересечение = $\\emptyset$.  \nПри этом:\n- $L_0$ — множество логических констант,\n- $L_1$ — множество унарных логических связок,\n- $L_2$ — множество бинарных логических связок.\n\n### Замечание 1:  \nЕсли $L_0 = \\{K_1, \\dots, K_s\\}$, $L_1 = \\{\\square{_1}, \\dots, \\square{_n}\\}$, $L_2 = \\{\\nabla_1, \\dots, \\nabla_n\\}$, то $\\Omega = \\langle L_2, L_1, L_0 \\rangle$.\n\n### Замечание 2:  \nЕсли множество $L_i = \\emptyset$, то его либо пропускают, либо пишут вместо него $\\emptyset$.\n\n### Пример 1:  \nРасширенная сигнатура классического ИВ: $\\Omega^*_{c1} = \\langle L_2, L_1, L_0 \\rangle$, где\n- $L_2 = \\{\\lor, \\land, \\rightarrow, \\leftrightarrow, \\oplus, \\downarrow, \\uparrow\\}$,\n- $L_1 = \\{\\lnot\\}$,\n- $L_0 = \\{\\top, \\bot\\}$.\n\n### Пример 2:  \nСтандартная сигнатура классического ИВ: $\\Omega_{c1} = \\langle L_2, L_1, L_0 \\rangle$, где\n- $L_2 = \\{\\lor, \\land, \\rightarrow\\}$,\n- $L_1 = \\{\\lnot\\}$,\n- $L_0 = \\emptyset$.\n\n## Язык ИВ логической сигнатуры\nЯзыком ИВ логической сигнатуры $\\Omega = \\langle L_2, L_1, L_0 \\rangle$ на множестве переменных $V$ (не менее чем счетное) называется язык $L_\\Omega(V) = \\langle \\bar{A}_\\Omega(V), \\mathcal{E}_\\Omega(V) \\rangle$, где:  \n- $\\bar{A}_\\Omega(V) = V \\cup \\text{smb}(\\Omega) \\cup \\{(\\ , )\\ ,\\ \",\"\\}$ \n- $\\mathcal{E}_\\Omega(V)$ является наименьшим по вложению подмножеством множества слов $W(\\bar{A}_\\Omega(V))$ данного алфавита $\\bar{A}_\\Omega(V)$, удовлетворяющее следующим условиям:\n\t1) $V \\subseteq \\mathcal{E}_\\Omega(V)$, $L_0 \\subseteq \\mathcal{E}_\\Omega(V)$\n\t2) Если $f \\in \\mathcal{E}_\\Omega(V)$, а $\\square{} \\in L_1$ — унарная логическая связка, то $\\square{f} \\in \\mathcal{E}_\\Omega(V)$\n\t3) Если $f$ и $p \\in \\mathcal{E}_\\Omega(V)$, а $\\nabla \\in L_2$ — бинарная логическая связка, то $(f \\nabla p) \\in \\mathcal{E}_\\Omega(V)$.  \n* Часто скобки $( \\,)$ не пишут и вместо $(f \\nabla p)$ пишут просто $f \\nabla p$.",
    "created_at": "2025-03-27T13:46:35.459983"
  },
  {
    "id": "1743061700.143053",
    "subject": "Теория вероятностей и математическая статистика",
    "date": "2025-03-04",
    "title": "Лекция 04.03.2025",
    "content": "### Медианный интервал \n\nИнтервал, в котором накопленная сумма частот впервые достигает $1/2$.  \nВыборочной группированной медианой называется значение:\n\n$$\nm_e^* = x_e + h \\cdot \\frac{n / 2 - (n_1 + \\cdots + n_{m_e} - 1)}{n_{m_e}}\n$$\n\nгде:\n- $n$ – объем выборки,\n- $h$ – длина интервала группировки,\n- $x_e$ – левая граница медианного интервала,\n- $n_i$ – частота $i$-го интервала,\n- $n_{m_e}$ – частота медианного интервала.\n\n---\n### Приближенное значение медианы\n\n![[медиана геом.jpg]]\n\n\n---\n### Пример. \n\nДан группированный статистический ряд величины $X$:\n\n| $X$   | 1-5 | 5-9 | 9-13 | 13-17 |\n| ----- | --- | --- | ---- | ----- |\n| $n_i$ | 2   | 8   | 9    | 1     |\n\nНайти выборочную группированную медиану.\n\n**Решение.**  \n$n = 20$.\n\n| $X$     | 1-5 | 5-9 | 9-13 | 13-17 |\n| ------- | --- | --- | ---- | ----- |\n| $n_i$   | 2   | 8   | 9    | 1     |\n| $n_i/n$ | 0,1 | 0,4 | 0,45 | 0,05  |\n\n$$\nm_e' = 5 + 4 \\cdot \\frac{20/2 - 2}{8} = 9\n$$\n\n---\n### Интервал квантильного порядка $q$\n\nИнтервал, в котором сумма накопленных частот впервые достигает значения $q$. \nВыборочной группированной квантилью называется значение:\n\n$$\n\\lambda_q^* = X(q) + h \\cdot \\frac{nq - (n_1 + \\cdots + n_q-1)}{n(q)}\n$$\n\nгде:\n- $\\lambda(q)$ – левая граница квантильного интервала,\n- $n(q)$ – численность квантильного интервала,\n- $n_1, \\ldots, n_{(q)-1}$ – частоты интервалов, предшествующих квантильному.\n\n---\n### Пример. \n\nДан группированный статистический ряд величины $X$:\n\n| $X$     | **1-5** | 5-9 | 9-13 | 13-17 |\n| ------- | ------- | --- | ---- | ----- |\n| $n_i$   | 2       | 8   | 9    | 1     |\n| $n_i/n$ | 0,1     | 0,4 | 0,45 | 0,05  |\n\nНайти приближенно квантиль порядка 0,4.\n\n**Решение.**  \n$n = 20$.\n\nКвантильным интервалом является второй $\\Rightarrow X_{0,4} = 5$\n\n$$\nX_{0,4}^* = 5 + 4 \\cdot \\frac{20 \\cdot 0,4 - 2}{8} = 8\n$$\n\n---\n### Модальный интервал\n\nИнтервал, имеющий наибольшую численность.  \nВыборочной группированной модой называется значение:\n\n$$\nm_0^* = x_0 + h \\cdot \\frac{n_{m_0} - n_{m_0-1}}{2n_{m_0} - n_{m_0-1} - n_{m_0+1}}\n$$\n\nгде:\n- $x_0$ - левая граница модального интервала,\n- $n_{m_0}$ - частота модального интервала,\n- $n_{m_0-1}$; $n_{m_0+1}$ - частоты интервалов слева и справа от модального.\n\n---\n### Приближённое значение моды\n\n![[мода геом.jpg]]\n\n---\n### Пример.  \n\nДан группированный статистический ряд величины $X$:\n\n| Х    | 1-5   | 5-9   | 9-13   | 13-17 |\n|---|---|---|---|---|\n| $n_i$    | 2    | 8    | 9    | 1    |\n| $n_i/n$ | 0,1   | 0,4   | 0,45 | 0,05 |\n\nНайти выборочную группированную моду.\n\n**Решение.**  \n$n = 20$.\n\nМодальным интервалом является третий.\n\n$$\nm_0^* = 9 + 4 \\cdot \\frac{9 - 8}{2 \\cdot 9 - 8 - 1} = 9,4\n$$\n\n---\n## Введение в точечное статистическое оценивание\n\n1. Оценки параметров распределения.  \n2. Несмещенность оценки.  \n3. Состоятельность оценки.  \n4. Оптимальность оценок.  \n\nТочное статистическое оценивание — это мощный инструмент для анализа данных. Оно позволяет нам оценить неизвестные параметры распределения случайных величин, используя имеющиеся данные.\n\n> «Мораль здесь такова: позаботься о смысле, а слова позаботятся о себе самих»  \n> Льюис Кэрролл «Алиса в стране чудес»\n\n---\n\n## Оценки параметров распределения\n\nПусть имеется выборка $X_1, \\ldots, X_n$ объёма $n$, извлечённая из распределения $F_\\theta$, которое известным образом зависит от неизвестного параметра $\\theta$.\n\nИзмеримая функция $g(X_1, \\ldots, X_n)$, от наблюдений называется **статистикой**.\n\nЗадача оценивания: выбор такой статистики $g(X_1, \\ldots, X_n)$, значения которой при заданной реализации $(X_1, \\ldots, X_n)$ приближаются к значению параметра $\\theta$.\n\n---\n## Точечная оценка\n\nВыборочная числовая характеристика (статистика) $\\theta = g(X_1, \\ldots, X_n)$, применяемая для оценивания неизвестного параметра $\\theta$ генеральной совокупности, называется его **точечной оценкой**.\n\nОбозначения: $\\hat{\\theta}, \\hat{\\theta}_i, \\hat{\\theta}_j^*$.\n\n**Пример точечной оценки:**  \nПредставьте, что вы хотите узнать средний рост всех студентов в университете. Вы берете случайную выборку из 100 студентов и измеряете их рост. Средний рост этой выборки будет точечной оценкой среднего роста всех студентов.\n\n---\n## Свойства точечных оценок\n\n- Несмещенность\n- Состоятельность\n- Оптимальность\n- Эффективность\n\n---\n### Несмещенность оценки\n\nНесмещенной называют точечную оценку $\\hat\\theta$, математическое ожидание которой равно оцениваемому параметру при любом объеме выборки:\n$$\nM\\hat\\theta(X_1, \\ldots, X_n) = \\theta \\quad \\forall \\theta \\in \\Theta\n$$\n\n**Замечание.** Математическое ожидание находится в предположении, что верна модель $F_\\theta$, т. е. что параметр равен $\\theta$.\n\nЕсли $M\\hat\\theta \\neq \\theta$, то оценка называется смещенной, и её смещение равно $M\\hat\\theta - \\theta$.\n\n---\n\nСтатистика $\\hat{\\theta} = \\theta(X_{1}\\dots X_{n})$ называется **асимптотически несмещённой** оценкой параметра $\\theta$, если для любого $\\theta \\in \\Theta$ имеет место сходимость:\n$$ M\\hat\\theta \\to \\theta \\text{ при } n \\to \\infty. $$\n---\n### Состоятельность оценки\n\nСтатистика $\\hat{\\theta} = g(X_1, \\ldots, X_n)$ называется состоятельной оценкой $\\theta$, если она сходится по вероятности к оцениваемому параметру:\n\n$$\nP(|\\hat{\\theta}_n - \\theta| < \\epsilon) \\to 1 \\quad \\forall \\epsilon > 0, \\theta \\in \\Theta \\quad \\text{при } n \\to \\infty\n$$\n\nСвойство состоятельности означает, что оценка делается точнее при увеличении количества данных.\n\n---\n### Оптимальность оценок\n\nПусть выбран критерий близости оценки к неизвестному параметру $\\theta$.\n\nОценка $\\hat{\\theta}$ параметра $\\theta$ называется оптимальной по данному критерию в рассматриваемом классе оценок, если она минимизирует выбранный критерий.\n\n**Замечание.** За критерий близости оценки к параметру $\\theta$ можно взять:\n\n$$\nM(\\hat{\\theta} - \\theta)^2,\n$$\n\nгде $\\hat{\\theta} = g(X_1, \\ldots, X_n)$ — оценка $\\theta$.\n\nЕсли оценка $\\hat{\\theta}$ — несмещена, то:\n\n$$\nM(\\hat{\\theta} - \\theta)^2 = D\\hat{\\theta},\n$$\n\nгде наименьшая дисперсия соответствует наиболее устойчивой оценке, которая меньше других меняется от выборки к выборке.\n\n---\nНесмещенная оценка $\\hat{\\theta}$ параметра $\\theta$ называется **оптимальной оценкой**, если\n$$\nD\\hat{\\theta} \\leq D\\theta^*, \\, \\forall \\theta \\in \\Theta,\n$$\nгде $\\theta^*$ — произвольная несмещенная оценка $\\theta$.\n\n---\n## Теоремы\n### Теорема 1\nЕсли $M\\hat{\\theta} = \\theta$ и $D\\hat{\\theta} \\rightarrow 0$ при $n \\rightarrow \\infty$, то $\\hat{\\theta}$ — состоятельная оценка $\\theta$.\n\n### Теорема 2\nЕсли $M\\hat{\\theta} \\rightarrow \\theta$ при $n \\rightarrow \\infty$ и $D\\hat{\\theta} \\rightarrow 0$ при $n \\rightarrow \\infty$, то $\\hat{\\theta}$ — состоятельная оценка $\\theta$.\n\n### Теорема 3\nЕсли $\\hat{\\theta}$ — состоятельная оценка $\\theta$, а $f$ — непрерывная функция, то $f(\\hat{\\theta})$ — состоятельная оценка $f(\\theta)$.\n\n### Теорема 4\nЕсли $\\hat{\\theta}_1, \\hat{\\theta}_2$ — две оптимальные оценки $\\theta$, то $P(\\hat{\\theta}_1 = \\hat{\\theta}_2) = 1.$\n\n### Доказательство\nПусть есть третья оценка\n$$\n\\hat{\\theta} = \\frac{\\hat{\\theta}_1 + \\hat{\\theta}_2}{2}, \\quad M\\hat{\\theta} = \\theta.\n$$\n\nТогда\n$$\nD\\hat{\\theta} = \\frac{1}{4} D\\hat{\\theta}_1 + \\frac{1}{4} D\\hat{\\theta}_2 + \\frac{1}{2} \\text{cov}(\\hat{\\theta}_1, \\hat{\\theta}_2).\n$$\n\nПо неравенству Коши-Буняковского для любых случайных величин\n$$\n\\text{cov}(\\hat{\\theta}_1, \\hat{\\theta}_2) \\leq \\sqrt{D\\hat{\\theta}_1 \\cdot D\\hat{\\theta}_2}.\n$$\n\nРавенство достигается, только если $\\hat{\\theta}_1 = k\\hat{\\theta}_2$.\n\nТак как $\\hat{\\theta}_1$ и $\\hat{\\theta}_2$ — оптимальные оценки с минимальной дисперсией, обозначим\n$$\nD\\hat{\\theta}_1 = D\\hat{\\theta}_2 = \\inf_{\\theta} D\\hat{\\theta} = v.\n$$\n\nТогда\n$$\nD\\hat{\\theta} \\leq \\frac{1}{2}(v + \\text{cov}(\\hat{\\theta}_1, \\hat{\\theta}_2)) \\leq v.\n$$\n\nЕсли $D\\hat{\\theta} < v$, это противоречит условию оптимальности оценок $\\hat{\\theta}_1$ и $\\hat{\\theta}_2$. \n\nТаким образом, $\\text{cov}(\\hat{\\theta}_1, \\hat{\\theta}_2) = v \\Rightarrow \\hat{\\theta}_1 = k\\hat{\\theta}_2,$ при $k = 1$.\nОткуда, $\\hat{\\theta}_1 = \\hat{\\theta}_2$ с вероятностью 1.\n\n### Теорема 5\n\nПусть $T(x)$ – оптимальная оценка некоторой параметрической функции $\\tau(\\theta)$, а $H(x)$ такая статистика, что $M(H(x)) = 0, \\, \\forall \\theta \\in \\Theta.$\nТогда $\\text{cov}(T(x), H(x)) = 0 \\, \\forall \\theta \\in \\Theta.$\n### Доказательство\nРассмотрим\n$$\nT^*(x) = T(x) + \\lambda H(x), \\, \\lambda \\in \\mathbb{R}.\n$$\nТогда\n$$\nM(T^*(x)) = M(T(x)) + \\lambda M(H(x)) = M(T(x)) = \\tau(\\theta).\n$$\n$$\nD(T^*(x)) = D(T(x)) + \\lambda^2 D(H(x)) + 2 \\lambda \\text{cov}(T(x), H(x)) \\geq D(T(x)).\n$$\n$$\n\\lambda^2 D(H(x)) + 2 \\lambda \\text{cov}(T(x), H(x)) \\geq 0.\n$$\n$$\n\\lambda^2 D(H(x)) + 2 \\lambda \\text{cov}(T(x), H(x)) = 0.\n$$\nДискриминант уравнения:\n$$\n\\text{cov}^2(T(x), H(x)) = 0 \\Rightarrow \\text{cov}(T(x), H(x)) = 0.\n$$\n\n## Теорема 6\n\nЕсли $T_1(x), T_2(x)$ – оптимальные оценки некоторых параметрических функций $\\tau_1(\\theta)$ и $\\tau_2(\\theta)$ соответственно, то $T(x) = a_1 T_1(x) + a_2 T_2(x)$ - оптимальная оценка некоторой параметрической функции $\\tau(\\theta) = \\tau_1(\\theta) + \\tau_2(\\theta)$.\n### Доказательство\nРассмотрим $T(x) = a_1 T_1(x) + a_2 T_2(x)$:\n$$\nM(T(x)) = M(T_1(x)) + M(T_2(x)) = a_1 \\tau_1(\\theta) + a_2 \\tau_2(\\theta).\n$$\n\nПусть $S$ – произвольная несмещенная оценка функции $\\tau$. Рассмотрим разность $S - T$:\n$$\nM(S - T) = M S - M T = \\tau - \\tau = 0.\n$$\n\nТеперь рассмотрим ковариацию:\n$$\n\\text{cov}(T, S - T) = \\text{cov}(a_1 T_1(x) + a_2 T_2(x), S - T).\n$$\n\nПо теореме 5:\n$$\n\\text{cov}(T, S - T) = 0.\n$$\n\nС другой стороны,\n$$\n\\text{cov}(T, S - T) = \\text{cov}(T, S) - D T \\Rightarrow \\text{cov}(T, S) = D T.\n$$\n\nНо\n$$\n\\text{cov}(T, S) \\leq \\sqrt{D T \\cdot D S} \\Rightarrow D T \\leq \\sqrt{D T \\cdot D S} \\text{ или } D S \\geq D T.\n$$\n\nЭто верно для любой несмещенной оценки $S$, значит, $T$ – оптимальная оценка. \n\n---\n## Примеры\n\n### Пример 1: Несмещенная оценка среднего\n\nПусть $X$ — несмещенная оценка $\\mu$ в модели $(X, P_\\mu)$, где $P_\\mu$ — производное семейство распределений с неизвестным математическим ожиданием $\\mu$.\n\n$$\nMX = M \\left( \\frac{1}{n} \\sum_{i=1}^n X_i \\right) = \\frac{1}{n} \\sum_{i=1}^n MX_i = \\frac{1}{n} \\cdot n \\mu = \\mu\n$$\n---\n\n### Пример 2: Смещенная оценка дисперсии\n\nПусть $S^2$ — смещенная оценка $\\sigma^2$ в модели $(X, P(\\mu, \\sigma^2))$, где $P(\\mu, \\sigma^2)$ — производное семейство распределений с неизвестными математическим ожиданием $\\mu$ и дисперсией $\\sigma^2$.\n\n$$\nMS^2 = M \\left( \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X})^2 \\right) = \\frac{n-1}{n} \\sigma^2\n$$\n\nСмещение можно устранить, используя оценку дисперсии:\n\n$$\n\\overline{S}^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\overline{X})^2\n$$\n---\n\n### Пример 3: Состоятельность оценки\n\nПусть дана выборка $(X_1, \\ldots, X_n)$ из биномиального распределения $B(n, p)$. Исследуем на состоятельность оценки параметра $p$:\n\n1. $\\frac{X_1}{n}$\n2. $\\frac{2X_1 + 3X_2}{5n}$\n3. $\\frac{1}{5n} \\sum_{i=1}^5 X_i$\n\n**Решение:**\n\n1. Оценка $\\frac{X_1}{n}$ является состоятельной, так как при $n \\to \\infty$ она сходится по вероятности к $p$.\n2. Оценка $\\frac{2X_1 + 3X_2}{5n}$ также является состоятельной.\n3. Оценка $\\frac{1}{5n} \\sum_{i=1}^5 X_i$ является состоятельной.\n---\n## Оценки математического ожидания и дисперсии\n\n| Оценка | Формула |\n|--------|---------|\n| Несмещенная и состоятельная оценка математического ожидания – выборочная средняя | $\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i$ |\n| Смещенная и состоятельная оценка дисперсии – выборочная дисперсия | $S^2 = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X})^2$ |\n| Несмещенная и состоятельная оценка дисперсии – исправленная выборочная дисперсия | $\\bar{S}^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar{X})^2$ |\n",
    "created_at": "2025-03-27T13:48:20.143053"
  },
  {
    "id": "1743061721.940294",
    "subject": "Теория вероятностей и математическая статистика",
    "date": "2025-03-06",
    "title": "Лекция 06.03.2025",
    "content": "# Статистическое оценивание параметров\n---\n## Неравенство Рао – Крамера\n\n### Плотность распределения\nПлотность распределения $p(x, \\theta)$ может быть определена как:\n$$\np(x, \\theta) = \n\\begin{cases} \np(x, \\theta), & \\text{если распределение непрерывно,} \\\\\nP_\\theta (X = y), & \\text{если распределение дискретно.}\n\\end{cases}\n$$\n\n### Вклад выборки\nВклад выборки в информационное количество Фишера выражается через производную логарифма плотности распределения:\n$$\n\\frac{\\partial \\ln p(x_1, x_2, \\dots, x_n, \\theta)}{\\partial \\theta}\n$$\n### Вклад одного элемента выборки:\n$$\n\\frac{\\partial \\ln p(x,\\theta)}{\\partial \\theta}\n$$\n### Информационное количество Фишера о параметре $\\theta$:\n$$\nI = M \\left( \\frac{\\partial \\ln p(x_1, x_2, \\dots, x_n, \\theta)}{\\partial \\theta} \\right)^2\n$$\n\n### Условия регулярности\n1. Множество значений $x$, для которых $p(x; \\theta) \\neq 0$, не зависит от $\\theta$.\n2. $p(x; \\theta)$ дифференцируема по параметру $\\theta$.\n3. Оцениваемая функция $\\tau(\\theta)$ дифференцируема по параметру $\\theta$.\n\n### Неравенство Рао – Крамера\nПри выполнении условий регулярности для любой оценки выполняется неравенство:\n$$\nD\\hat\\theta \\geq \\frac{1}{I}\n$$\n### Доказательство\n\n1. По свойству плотности:\n\n$$\n\\int p \\, dx = 1\n$$\n\n2. Из несмещенности оценки:\n\n$$\n\\int \\hat{\\theta} p \\, dx = M \\hat{\\theta} = \\theta\n$$\n\n3. Продифференцируем (1) по параметру:\n\n$$\n\\int \\frac{\\partial p}{\\partial \\theta} \\, dx = 0\n$$\n\n4. Домножим (3) на $\\theta$:\n\n$$\n\\int \\theta \\frac{\\partial p}{\\partial \\theta} \\, dx = 0\n$$\n\n5. Продифференцируем (2) по параметру:\n\n$$\n\\int \\hat{\\theta}\\frac{\\partial p}{\\partial \\theta} \\, dx = 1\n$$\n\n6. Вычтем (5) - (4):\n\n$$\n\\int (\\hat{\\theta} - \\theta) \\frac{\\partial p}{\\partial \\theta} \\, dx = 1\n$$\n\n7. Подставим выражение для $\\partial p / \\partial \\theta$:\n\n$$\n\\int (\\hat{\\theta} - \\theta) \\frac{\\partial \\ln p}{\\partial \\theta} \\, p \\, dx = 1\n$$\n\n8. Из курса теории вероятностей известно:\n\n$$\nM [(\\varphi_1 - M \\varphi_1)(\\varphi_2 - M \\varphi_2)] = \\text{cov}(\\varphi_1, \\varphi_2)\n$$\n\n9. Из свойства коэффициента корреляции вытекает неравенство Коши – Буняковского:\n\n$$\n| \\text{cov} (\\varphi_1, \\varphi_2) | \\leq \\sqrt{D \\varphi_1 D \\varphi_2}\n$$\n\n$$\n1 \\leq \\sqrt{D \\hat\\theta D \\left( \\frac{\\partial \\ln p}{\\partial \\theta} \\right)} \\Rightarrow D \\hat{\\theta} \\geq \\frac{1}{D \\left( \\frac{\\partial \\ln p}{\\partial \\theta} \\right)}\n$$\n\n$$\nD \\left( \\frac{\\partial \\ln p}{\\partial \\theta} \\right) = M \\left( \\frac{\\partial \\ln p}{\\partial \\theta} \\right)^2 - M^2 \\left( \\frac{\\partial \\ln p}{\\partial \\theta} \\right) = M \\left( \\frac{\\partial \\ln p}{\\partial \\theta} \\right)^2 = I\n$$\n\n$$\n\\Rightarrow D \\hat{\\theta} \\geq \\frac{1}{I}\n$$\n\n---\n\n### Следствие: \nРавенство достигается, если $\\theta^*$ и $\\partial \\ln p / \\partial \\theta$ линейно зависимы.\n\n---\n## Формы информационного количества Фишера\n\n### Тождество для информационного количества:\n$$\n\\frac{\\partial \\ln p}{\\partial \\theta} = \\frac{\\partial p}{\\partial \\theta} \\frac{1}{p}\n$$\n\n### Вторая производная логарифма плотности:\n$$\n\\frac{\\partial^2 \\ln p}{\\partial \\theta^2} = -\\frac{1}{p^2} \\left( \\frac{\\partial p}{\\partial \\theta} \\right)^2 + \\frac{1}{p} \\frac{\\partial^2 p}{\\partial \\theta^2}\n$$\n\n### Информационное количество Фишера через вторую производную:\n$$\nI = -M \\left( \\frac{\\partial^2 \\ln p}{\\partial \\theta^2} \\right)\n$$\n\n### Формы для одномерной плотности $p(x, \\theta)$:\n$$\nI = nM \\left( \\frac{\\partial \\ln p(x, \\theta)}{\\partial \\theta} \\right)^2\n$$\n$$\nI = -nM \\left( \\frac{\\partial^2 \\ln p(x, \\theta)}{\\partial \\theta^2} \\right)\n$$\n---\n## Эффективные оценки\n\n### Определение эффективной оценки\nНесмещенная оценка $\\hat{\\theta}$ параметра $\\theta$ называется **эффективной**, если для любого $\\theta \\in \\Theta$ выполняется:\n$$\nD\\hat{\\theta} = \\frac{1}{I}\n$$\n### Замечание.\nЕсли оценка является эффективной, то она является оптимальной.\nОбратное не верно\n\n### Пример эффективной оценки\nДля распределения Пуассона $P_\\lambda$ оценка $\\hat{\\lambda} = \\bar{X}$ является эффективной, так как:\n$$\nD\\bar{X} = \\frac{\\lambda}{n} = \\frac{1}{I}\n$$\n### Показатель эффективности\nПоказатель эффективности несмещенной оценки $\\hat{\\theta}$ параметра $\\theta$ называется число:\n$$\ne(\\hat{\\theta}) = \\frac{1}{I \\cdot D\\hat{\\theta}},\\ 0<e(\\hat\\theta) \\leq 1\n$$\nДля эффективных оценок $e(\\hat{\\theta}) = 1$.\n\n---\n## Метод максимального правдоподобия\nПусть $\\xi$ - непрерывная случайная величина с плотностью $p(x, \\theta)$, где $\\theta$ - неизвестный параметр..\n\nТогда плотность распределения вектора $\\overline{X}$:\n$$\np(x_{1}\\dots x_{n},\\theta) = p(x_{1}, \\theta)* \\dots * p(x_{n}, \\theta)\n$$\n### Функция правдоподобия\nДля непрерывной случайной величины функция, рассматриваемая при фиксированных $(x_{1}\\dots x_{n})$ как функция параметра $\\theta$, называется функцией правдоподобия:\n$$\nL(x_1, \\dots, x_n, \\theta) = p(x_1, \\theta) \\cdot \\dots \\cdot p(x_n, \\theta)\n$$\n\nДля дискретной случайной величины:\n$$\nL(x_1, \\dots, x_n, \\theta) = P(\\xi = x_1) \\cdot \\dots \\cdot P(\\xi = x_n)\n$$\n### Суть метода\nМетод максимального правдоподобия заключается в нахождении значения параметра $\\theta$, которое максимизирует функцию правдоподобия $L(x_1, \\dots, x_n, \\theta)$.\n\n### Оценка максимального правдоподобия (о.м.п.)\nОценка $\\theta^*$, обеспечивающая по параметру $\\theta$ максимум функции правдоподобия, называется **оценкой максимального правдоподобия** параметра $\\theta$ (о.м.п.).\n\n### Уравнение правдоподобия\nДля нахождения оценки максимального правдоподобия решается уравнение:\n$$\n\\frac{\\partial \\ln L}{\\partial \\theta} = 0\n$$\n\nПосле нахождения критической точки необходимо проверить, что это точка максимума:\n$$\n\\frac{\\partial^2 \\ln L}{\\partial \\theta^2} < 0\n$$\n\n---\n### Пример (нерегулярная модель)\nНайти о.м.п. параметра $\\theta = (a, b)$ в равномерном распределении $R[a, b]$.\n\n### Решение\nФункция правдоподобия для равномерного распределения:\n$$\nL(X, \\theta) = \\prod_{i=1}^n \\frac{1}{b-a} = \\frac{1}{(b-a)^n}\n$$\n\nТак как функция $L$ монотонна по $a$ и $b$, наибольшее значение достигается при минимально возможном значении $b$ и максимально возможном значении $a$.\n\nТаким образом, оценки максимального правдоподобия для параметров $a$ и $b$:\n$$\n\\hat{a} = x_{max} = x^*_1, \\quad \\hat{b} = x_{min} = x^*_n\n$$\n\n---\n## Некоторые свойства оценок максимального правдоподобия\n\n### Cвойство 1\n\nДля нахождения оценки максимального правдоподобия (о.м.п.) можно выбирать наиболее удобную параметризацию, а о.м.п. получать затем с помощью соответствующих преобразований. Это свойство выражается следующим образом:\n\n$$\n\\overline{\\tau(\\theta)} = \\tau(\\overline{\\theta})\n$$\n\n### Пример:\nНайдем о.м.п. параметра $\\alpha^3$ в распределении $\\Gamma_{\\alpha,\\beta}$ при известном $\\beta$.\n\n**Решение:**\n\n$$\n\\alpha^3 = (\\hat{\\alpha})^3 = \\left( \\frac{\\beta}{\\overline{X}} \\right)^3\n$$\n\n### Свойство 2 \n\nОценки максимального правдоподобия обладают следующими асимптотическими свойствами:\n\n- **Асимптотическая несмещенность:** Оценки максимального правдоподобия становятся несмещенными при увеличении объема выборки.\n- **Состоятельность:** Оценки максимального правдоподобия сходятся по вероятности к истинному значению параметра при увеличении объема выборки.\n- **Асимптотическая нормальность:** При некоторых дополнительных условиях оценки максимального правдоподобия асимптотически нормальны.\n\n### Свойство 3\n\nЕсли оценки максимального правдоподобия асимптотически нормальны, то они также асимптотически эффективны, то есть:\n\n$$\nD\\hat{\\theta} \\rightarrow \\frac{1}{I}\n$$\n\nгде $I$ — информационное количество Фишера.\n\n---\n\n## Метод моментов\n\nМетод моментов заключается в приравнивании выборочных моментов к соответствующим теоретическим моментам распределения случайной величины $\\xi$.\n\n- Если распределение имеет **один параметр**, то составляют уравнение:\n\n$$\nM\\xi = \\bar{X}\n$$\n\n- Если распределение имеет **два параметра**, то составляют систему уравнений:\n\n$$\n\\begin{cases} \nM\\xi = \\bar{X}, \\\\ \nD\\xi = S^2\n\\end{cases}\n$$\n---\n## Пример: оценки параметров для распределений\n\n### 1. Показательное распределение\n\nДля показательного распределения:\n\n$$\nM\\xi = \\frac{1}{\\lambda} = \\bar{X} \\Rightarrow \\lambda = \\frac{1}{\\bar{X}}\n$$\n\n### 2. Равномерное распределение\n\nДля равномерного распределения:\n\n$$\n\\begin{cases}\nM\\xi = \\frac{a+b}{2} = \\bar{X}, \\\\\nD\\xi = \\frac{(b-a)^2}{12} = S^2\n\\end{cases}\n$$\n\n### 3. Нормальное распределение\n\nДля нормального распределения оценки параметров находятся аналогичным образом.\n\n---\n## Некоторые свойства оценок метода моментов\n\n### Теорема:\nПусть $\\hat{\\theta} = g(a_1, \\ldots, a_k)$ — оценка параметра $\\theta$, полученная по методу моментов, причем функция $g^{-1}$ непрерывна. Тогда $\\hat{\\theta}$ состоятельна.\n\n### Доказательство:\n\nЕсли $\\hat{\\theta} = g(a_1, \\ldots, a_k)$, то $\\theta = g(\\alpha_1, \\ldots, \\alpha_k)$.\n\nПо свойству выборочных моментов:\n\n$$\na_i \\xrightarrow{P} \\alpha_i \\quad \\text{при} \\quad n \\to \\infty\n$$\n\nТогда по теореме о сходимости по вероятности:\n\n$$\ng(a_1, \\ldots, a_k) \\xrightarrow{P} g(\\alpha_1, \\ldots, \\alpha_k)\n$$\n\nОткуда:\n\n$$\n\\hat{\\theta} \\xrightarrow{P} \\theta\n$$\n\n---\n\n## Метод наименьших\nОценка метода наименьших квадратов определяется из условия минимизации суммы квадратов отклонений выборочных данных от определяемой оценки.\n\n### Пример:\nНайти оценку метода наименьших квадратов $\\hat{\\theta}$ для генеральной средней $\\theta = \\bar{x}_0$.\n\n**Решение:**\n\nМинимизируем сумму квадратов отклонений:\n\n$$\nu = \\sum_{i=1}^n (x_i - \\theta)^2 \\to \\min\n$$\n\nПродифференцируем по $\\theta$ и приравняем к нулю:\n\n$$\n\\frac{du}{d\\theta} = -2\\sum_{i=1}^n (x_i - \\theta) = 0 \\implies \\sum_{i=1}^n x_i - \\theta n = 0 \\implies \\hat{\\theta} = \\frac{1}{n} \\sum_{i=1}^n x_i = \\bar{X}\n$$\n\nТаким образом, оценка метода наименьших квадратов для генеральной средней равна выборочному среднему $\\bar{X}$.",
    "created_at": "2025-03-27T13:48:41.940294"
  },
  {
    "id": "1743061762.569782",
    "subject": "Теория вероятностей и математическая статистика",
    "date": "2025-02-13",
    "title": "Лекция 13.02.2025",
    "content": "# Математическая статистика\n\nМатематическая статистика — наука, занимающаяся разработкой методов получения, описания и обработки опытных данных с целью изучения закономерностей случайных массовых явлений.\n\n---\n\n## Цель:\nОписание, объяснение и предсказание явлений действительности на основе установленных законов.\n\n---\n\n## Предмет:\nИзучение случайных величин (или случайных событий) по результатам наблюдений (статистическим данным).\n\n---\n\n## Задача:\nСоздание методов сбора и обработки статистических данных для получения научных и практических выводов.\n\n---\n\n# Статистические методы в принятии решений\n\n1. **Анализ данных**  \n   Обработка больших объемов информации и выявление закономерностей, скрытых в данных.\n\n2. **Оценка рисков**  \n   Оценка вероятности различных исходов событий, что особенно важно при принятии решений в условиях неопределенности.\n\n3. **Тестирование гипотез**  \n   Определение насколько данные подтверждают или опровергают предположения о взаимосвязях между переменными.\n\n4. **Прогнозирование**  \n   Построение прогнозов на основе имеющихся данных.\n\n5. **Оптимизация процессов**  \n   Нахождение наилучших решений среди множества возможных вариантов.\n\n6. **Контроль качества**  \n   Отслеживание стабильности производственного процесса и своевременное выявление отклонения от нормы.\n\n7. **Принятие решений в медицине**  \n   Статистический анализ данных клинических испытаний.\n\n8. **Маркетинг и реклама**  \n   Анализ поведения потребителей, изучение рыночных тенденций и проведение маркетинговых исследований.\n\n---\n\n# Диаграмма Парето\n\nДиаграмма Парето представляет собой комбинацию значений и кумулятивного распределения покупательской способности жителей стран.\n\n---\n\n# Вероятностная интерпретация данных\n\n1. Вероятностная интерпретация данных  \n2. Регуляризация  \n3. Оценка параметров модели  \n4. Проверка гипотез  \n5. Кросс-валидация  \n6. Обработка пропущенных данных  \n7. Выбор признаков  \n8. Бустинг и ансамблевые методы  \n9. Байесовская сеть доверия  \n10. Рекомендательные системы  \n\n---\n\n# Использование коэффициента корреляции для восстановления данных\n\n| Грудь | Талия | Бёдра | Рост | Вес |\n|---|---|---|---|---|\n| Ж1    | 99   | 56    | 91    | 160   | 58   |\n| Ж2    | 89   | 58    | 89    | 157   | 48   |\n| Ж3    | 91   | 64    | 91    | 165   | 54   |\n| Ж4    | 91   | 51    | 91    | 170   | 54   |\n| Ж5    | 86   | 56    | 84    | 157   | 44   |\n| Ж6    | 97   | 53    | 86    | 175   | 56   |\n| Ж7    | ?    | 51    | 91    | 165   | 54   |\n\n---\n\n# Коэффициенты корреляции\n\n$$ r(\\text{Грудь}, \\text{Талия}) = -0,22 $$\n\n$$ r(\\text{Грудь}, \\text{Бедра}) = 0,34 $$\n\n$$ r(\\text{Грудь}, \\text{Рост}) = 0,46 $$\n\n$$ r(\\text{Грудь}, \\text{Вес}) = 0,91 $$\n\n---\n\n# Метод восстановления данных\n\nПусть  \n$P(A)$ — значение признака $P$ объекта $A$.  \n$\\overline{P}$ — среднее значение признака $P$.  \nТребуется определить $P(A)$ по столбцам признакам $P_1, P_2, \\ldots, P_m$:\n\n$$ P(A) = \\overline{P} + \\frac{\\sum_{i=1}^m r(P_i, P_j)(P_j(A) - \\overline{P})}{\\sum_{j=1}^m |r(P_i, P_j)|} $$\n\nПримечание: в формуле все величины вычисляются без учета строки объекта $A$.\n\n---\n\n# Метод главных компонент\n\nМетод главных компонент преобразует набор коррелирующих исходных переменных в другой набор — не коррелирующих переменных.\n\n**Суть метода:** преобразование информации, содержащейся в исходных данных.\n\n---\n# Выборочная совокупность и генеральная совокупность\n\n**Выборочная совокупность (выборка)** — совокупность случайно отобранных объектов.\n\n**Генеральная совокупность** — совокупность всех объектов, из которых производится выборка.\n\n**Объём совокупности** — число объектов этой совокупности.\n\n---\n\n# Репрезентативность выборки\n\nРепрезентативность выборки — соответствие характеристик выборки характеристикам генеральной совокупности.\n\nРепрезентативность обеспечивается простым случайным выбором.\n\n---\n\n# Методы отбора\n\n1. **Случайный отбор (random sampling)**  \n   Выемка элементов в выборку в произвольном порядке.\n\n2. **Стратифицированный отбор (stratified sampling)**  \n   Разделение генеральной совокупности на страты и случайный отбор элементов из каждой страты.\n\n3. **Простая случайная выборка (simple random sample)**  \n   Выборка, получаемая в результате случайного отбора без разбиения генеральной совокупности на страты.\n\n4. **Бутстраповская выборка (bootstrap sample)**  \n   Выборка, взятая с возвратом из набора наблюдаемых данных.\n\n5. **Повторный отбор (resampling)**  \n   Процесс многократного взятия выборок из наблюдаемых данных.\n\n---\n## Исходные статистические данные\n\nНа слайде с исходными данными представлены следующие ключевые моменты:\n\n1. **Случайная величина $\\xi$**:  \n   $\\xi : \\Omega \\rightarrow \\mathbb{R}$ — независимая случайная величина, наблюдаемая в случайном эксперименте.\n\n2. **Результат наблюдения $X = (X_1, \\ldots, X_n)$**:  \n   Это результат наблюдения некоторой конечной совокупности случайных величин $\\xi_i$, характеризующей исход изучаемого эксперимента.\n\n3. **Количество испытаний $n$**:  \n   $n$ — количество проведённых испытаний или экспериментов.\n---\n## До и после опыта\n\n- **До опыта**:  \n  $X_i$ — случайная величина, одинаково распределённая с $\\xi$.\n\n- **После опыта**:  \n  $X_i$ — результат $i$-го эксперимента, $i = 1, \\ldots, n$, то есть одно из возможных значений случайной величины $X_i$.\n---\n## Выборка\n\nВыборка $X = (X_1, \\ldots, X_n)$ объёма $n$ из распределения $F$ называется набор из $n$ независимых и одинаково распределённых случайных величин, имеющих распределение $F$.\n\n- $x = (x_1, \\ldots, x_n)$ — реализация выборки $X$.\n- **Выборочное пространство**: $S = \\{x\\}$ — множество всех возможных значений выборки $X$.\n---\n## Статистическая модель\n\nСтатистическая модель описывается парой $(S, F)$, где:\n\n- $F$ — семейство функций распределения, которому принадлежит неизвестная функция распределения выборки.\n  \n$$\nP(x_i < x) = F(x), \\quad i=1, \\ldots, n\n$$\n\nПусть компоненты выборки независимы и распределены так же, как некоторая случайная величина $\\xi$ с функцией распределения $F_{\\xi}(x) = F(x)$. Тогда:\n\n$$\nF_{x_1, \\ldots, x_n}(x_1, \\ldots, x_n) = F(x_1) \\cdots F(x_n).\n$$\n---\n## Генеральная совокупность\n\nМножество возможных значений $\\xi$ с распределением $F = F_{\\xi}(x)$ называется **генеральной совокупностью**, из которой производят случайную выборку.\n\n---\n## Простая и смещенная выборка\n\n- **Простая выборка**: компоненты $(X_1, \\ldots, X_n)$ независимы и распределены так же, как $\\xi$.\n  \n- **Смещенная выборка**: выборка, которая представляет генеральную совокупность в искажённом виде.\n---\n## Параметрическая модель\n\nЕсли $F = F_{\\xi}(x)$ зависит от неизвестного параметра $\\theta$, но известно множество возможных значений этого параметра $\\Theta$, то статистическая модель называется **параметрической**.\n\n---\n## Регулярная модель\n\nЕсли модель $F_{\\xi}(x)$ такова, что можно дифференцировать по $\\theta$ интегралы на $X$, меняя порядок дифференцирования и интегрирования, то она называется **регулярной**.\n\n**Условие регулярности**: выборочное пространство $X$ не должно зависеть от параметра $\\theta$.\n\n---\n# Наиболее известные параметрические статистические модели\n\n| Наименование модели   | Обозначение модели       | Функция плотности или распределение вероятности                          | Множество значений $\\Theta$               |\n| --------------------- | ------------------------ | ------------------------------------------------------------------------ | ----------------------------------------- |\n| Нормальная $\\theta_1$ | $N(\\theta, \\sigma)$      | $\\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x-\\theta)^2}{2\\sigma^2}}$        | $R$                                       |\n| Нормальная $\\theta_2$ | $N(a, \\theta)$           | $\\frac{1}{\\theta\\sqrt{2\\pi}} e^{-\\frac{(x-a)^2}{2\\theta^2}}$             | $R^+$                                     |\n| Общая нормальная      | $N(\\theta_1, \\theta_2)$  | $\\frac{1}{\\theta_2\\sqrt{2\\pi}} e^{-\\frac{(x-\\theta_1)^2}{2\\theta_2^2}}$  | $\\theta_1 \\in R; \\theta_2 \\in R^+$        |\n| Гамма                 | $\\Gamma_{\\theta,\\theta}$ | $\\frac{\\theta^2 x^{\\beta-1}}{\\Gamma(\\theta)} e^{-\\theta x}, \\, x \\geq 0$ | $R^+$                                     |\n| Равномерная           | $R(0, \\theta)$           | $\\frac{1}{\\theta_1}, \\, 0 \\leq x \\leq \\theta$                            | $R^+$                                     |\n| Общая равномерная     | $R(\\theta_1, \\theta_2)$  | $\\frac{1}{\\theta_2 - \\theta_1}, \\, \\theta_1 \\leq x \\leq \\theta_2$        | $-\\infty < \\theta_1 < \\theta_2 < +\\infty$ |\n| Коши                  | $K_\\theta$               | $\\frac{1}{\\pi} \\cdot \\frac{1}{1+(x-\\theta)^2}$                           | $R$                                       |\n| Биномиальная          | $B(N, \\theta)$           | $C_N^\\alpha \\theta^\\beta (1 - \\theta)^{N-x}, \\, x = 0, 1, \\ldots, N$     | $(0, 1)$                                  |\n| Пуассоновская         | $P_\\theta$               | $\\frac{\\theta^2 e^{-y}}{x!}$                                             | $R^+$                                     |\n\n---\n\n",
    "created_at": "2025-03-27T13:49:22.569782"
  },
  {
    "id": "1743061785.978124",
    "subject": "Теория вероятностей и математическая статистика",
    "date": "2025-03-13",
    "title": "Лекция 13.03.2025",
    "content": "# Достаточные статистики\n\n### Основные темы:\n1. Критерий достаточности статистики.\n2. Свойства достаточных статистик.\n3. Свойства оценок максимального правдоподобия.\n4. Достаточные статистики и оптимальные оценки.\n5. Полные статистики.\n\n---\n## Критерий достаточности статистики\n\nСтатистика $T = T(X)$ называется **достаточной** для параметра $\\theta$, если условное распределение (плотность $p_{X/T}(X) = t(x)$ или вероятность $P(X = x / T(X) = t)$) случайной величины $X = (X_1, ..., X_n)$ (выборки) при условии $T(X) = t$ не зависит от параметра $\\theta$.\n\n---\n\n### Рассмотрим дискретный случай\n\n$$\nP(X = x / T(X) = t) = \\frac{P(X = x, T(X) = t)}{P(T(X) = t)} = \n\\begin{cases} \n\\frac{P(X = x)}{P(T(X) = t)}, & x : T(X) = t, \\\\\n0, & x : T(X) \\neq t\n\\end{cases}\n$$\n\nВ дискретной модели статистика $T(X)$ достаточна, если\n\n$$\n\\frac{P(X = x)}{P(T(X) = t)}\n$$\n\nДля непрерывных случайных величин распределение задается плотностью. В этом случае достаточно показать, что:\n\n$$\n\\frac{p_X(x)}{p_T(t)}\n$$\n\nне зависит от параметра $\\theta$.\n\n---\n\n## Пример: Исследование на достаточность статистики\n\n### Пример: \nИсследовать на достаточность статистику $T = \\frac{1}{n} \\sum_{i=1}^n X_i$ в распределении Пуассона.\n### Решение:\n\n$$\nP(X = x) = P(X_1 = x_1, ..., X_n = x_n) = \\prod_{i=1}^n P(X_i = x_i) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{x_i}}{x_i!} = \\frac{e^{-\\lambda n} \\lambda^{\\sum x_i}}{\\prod_{i=1}^n (x_i!)}\n$$\n\nСлучайная величина $\\frac{1}{n} \\sum_{i=1}^n X_i$ имеет распределение $P_{\\lambda n}$, значит,\n\n$$\nP(T = t) = P\\left(\\sum_{i=1}^n X_i = tn\\right) = \\frac{e^{-\\lambda n} (n\\lambda)^{tn}}{(tn)!}\n$$\n\nТогда:\n\n$$\nP(X / T = t) = \\frac{P(X = x)}{P(T = t)} = \\frac{e^{-\\lambda n} \\lambda^{\\sum x_i} (tn)!}{\\prod_{i=1}^n (x_i!) e^{-\\lambda n} (n\\lambda)^{tn}} = \\frac{\\lambda^{\\sum x_i} (\\sum x_i)!}{\\prod_{i=1}^n (x_i!) (n\\lambda)^{\\sum x_i}} = \\frac{(\\sum x_i)!}{\\prod_{i=1}^n (x_i!) (n)^{\\sum x_i}}\n$$\n\nЭто выражение не зависит от $\\lambda$, следовательно, статистика $T$ является достаточной.\n\n---\n\n## Теорема факторизации Неймана — Фишера (критерий достаточности статистики)\n\nВ модели $F_\\theta$ статистика $T(X)$ является достаточной для параметра $\\theta$ тогда и только тогда, когда функция правдоподобия представима в виде произведения двух функций:\n\n$$\nL(x, \\theta) = q(T(X), \\theta) \\cdot h(x)\n$$\n\n**Замечание:** Представление $L(x, \\theta)$ в таком виде называется факторизацией распределения. Факторизация не единственна. При $h \\equiv 1$ говорят о тривиальной факторизации.\n\n---\n### Доказательство теоремы факторизации\n\n**Доказательство для дискретной модели:**\n\n1. $\\rightarrow$ Пусть $T(X)$ — достаточная статистика.\n\n   Возьмем $t$ такое, что $T(X) = t$.\n   $$L(x, \\theta) = P(X = x).$$\n\n   Поскольку $\\{X = x\\} \\subseteq \\{T(X) = t\\}$,\n   $$\n   L(x, \\theta) = P(X = x) = P(X = x, T(X) = t).\n   $$\n\n   По теореме умножения:\n   $$P(X = x, T(X) = t) = P(T(X) = t) \\cdot P(X = x / T(X) = t).$$\n\n   По определению достаточной статистики, $P(X = x / T(X) = t)$ не зависит от параметра, то есть:\n\n   $$\n   P(X = x / T(X) = t) = h_1(x, t) = h_1(x, T(X)) = h(x).\n   $$\n   $P(T(X) = t)$ зависит от параметра $\\theta$, так как вычисляется по закону распределения. Однако от $x$ она зависит только через $T(X)$, то есть:\n\n$$\nP(T(X) = t) = q(T(X), \\theta)\n$$\n\nТогда функция правдоподобия может быть представлена в виде:\n\n$$\nL(x, \\theta) = P(T(X) = t) \\cdot P(X = x / T(X) = t) = q(T(X), \\theta) \\cdot h(x)\n$$\n\nТаким образом, получена факторизация распределения.\n\n2. $\\leftarrow$ Пусть $L(x, \\theta) = q(T(X), \\theta) \\cdot h(x)$.\n\n   Тогда:\n   $$\n   P(X = x / T(X) = t) = \\frac{P(X = x, T(X) = t)}{P(T(X) = t)} = \\frac{P(X = x)}{P(T(X) = t)} = \\frac{L(x, \\theta)}{\\sum_{x:T(X) = t} L(x, \\theta)} = \\frac{q(T(x), \\theta)h(x)}{\\sum_{x:T(X) = t} q(T(x), \\theta)h(x)} = \\frac{q(t, \\theta)h(x)}{q(t, \\theta) \\sum_{x:T(X) = t} h(x)} = \\frac{h(x)}{\\sum_{x:T(X) = t} h(x)}\n   $$\n\n   Это выражение не зависит от $\\theta$, следовательно, $T(x)$ — достаточная статистика. \n\n---\n\n## Свойства достаточных статистик\n\n### Свойство 1\nВсякая эффективная оценка является достаточной статистикой.\n\n### Доказательство:\nПусть $T(x)$ — эффективная оценка параметрической функции $\\tau(\\theta)$. Тогда:\n\n$$\nT(x) = a(\\theta) \\frac{\\partial \\ln L(X)}{\\partial \\theta} + \\tau(\\theta),\n$$\n\nгде $a(\\theta)$ — некоторая функция от $\\theta$.\n\nТогда:\n\n$$\n\\frac{\\partial \\ln L(X)}{\\partial \\theta} = \\frac{T(x) - \\tau(\\theta)}{a(\\theta)}\n$$\n\nИнтегрируя, получаем:\n\n$$\n\\ln L(X) = \\int \\frac{T(x) - \\tau(\\theta)}{a(\\theta)} d\\theta\n$$\n\n$$\nL(X) = e^{\\int \\frac{T(x) - \\tau(\\theta)}{a(\\theta)} d\\theta}\n$$\n\nПри фиксированном $T(x) = t$:\n\n$$\nL(x, \\theta) = q(t, \\theta).\n$$\n\nПолучили тривиальную факторизацию. \n\n### Пример\nВ распределении Пуассона $\\bar{X}$ является эффективной оценкой $\\lambda$.\n\n---\n### Свойство 2\nЛюбая взаимно однозначная функция от достаточной статистики $T$ является достаточной статистикой.\n\n### Доказательство:\n\nПусть $H = \\varphi(T)$ и $T = \\varphi^{-1}(H)$. Тогда:\n\n$$\nL(x, \\theta) = q(T(X), \\theta) \\cdot h(x) = q(\\varphi^{-1}(H), \\theta) \\cdot h(x) = q_1(H, \\theta) \\cdot h(x).\n$$\n\nПо теореме факторизации $H = \\varphi(T)$ — достаточная статистика. \n\n### Пример\nВ распределении Пуассона $T(x) = \\sum x_i$ — достаточная статистика, и $H = \\bar{X}$ — достаточная статистика, так как $H(x)$ является взаимно однозначной функцией от $T(x)$.\n\n---\n## Минимальная достаточная статистика\n\nДостаточная статистика, являющаяся функцией любых других достаточных статистик, называется **минимальной**.",
    "created_at": "2025-03-27T13:49:45.978124"
  },
  {
    "id": "1743061803.657928",
    "subject": "Теория вероятностей и математическая статистика",
    "date": "2025-03-18",
    "title": "Лекция 18.03.2025",
    "content": "## Свойства оценок максимального правдоподобия\n\n### Свойство 1: Связь с эффективными оценками\nЕсли для скалярного параметра $\\theta$ существует эффективная оценка $T(x)$, то $T(x)$ совпадает с оценкой максимального правдоподобия (о.м.п.) $\\hat{\\theta}$.\n\n### Свойство 2: Связь с достаточными статистиками\nЕсли $T(x)$ – достаточная статистика, а о.м.п. $\\hat{\\theta}$ существует и единственна, то $\\hat{\\theta}$ является функцией от $T(x)$.\n\n### Следствие. \nЕсли о.м.п. $\\hat{\\theta}$ существует и единственна, то $\\hat{\\theta}$ является функцией минимальной достаточной статистики.\n\n---\n## Достаточные статистики и оптимальные оценки\n\n### Теорема Рао – Блекуэлла – Колмогорова\nПусть $T(x)$ – достаточная статистика, $d(X)$ – несмещенная оценка $\\theta$, $\\varphi(T) = M(d(X) | T(X) = t)$. Тогда\n$$\nD(d(X)) \\geq D(\\varphi(T)).\n$$\n### Доказательство\n\n1. **Покажем, что $\\varphi(T)$ не зависит от $\\theta$ и может служить оценкой $\\theta$.**\n   $$\n   \\varphi(t) = M(d(X) | T(X) = t) = \\int d(X) p_T(x) dx\n   $$\n   $p_T(x)$ не зависит от $\\theta$, так как $T(x)$ – достаточная статистика.\n\n2. Покажем, что $\\varphi(T)$ – несмещенная оценка $\\theta$.\n   $$\n   M(\\varphi(T)) = M_T(M_X(d(X) | T(X))) = M(d(X)) = \\theta.\n   $$\n\n3. Докажем, что $D(d(X)) \\geq D(\\varphi(T))$.\n   $$\n   D(d(X)) = M(d(X) - \\theta)^2 = M(d(X) - \\varphi(T) + \\varphi(T) - \\theta)^2\n   $$\n   $$\n   = M(d(X) - \\varphi(T))^2 + M(\\varphi(T) - \\theta)^2 + 2M(d(X) - \\varphi(T))(\\varphi(T) - \\theta).\n   $$\n   Рассмотрим последний член:\n   $$\n   M(d(X) - \\varphi(T))(\\varphi(T) - \\theta) = M[(d(X) - \\varphi(T))\\varphi(T)] - \\theta M(d(X) - \\varphi(T)).\n   $$\n   Поскольку $\\varphi(T)$ – несмещенная оценка, $M(d(X) - \\varphi(T)) = 0$, следовательно:\n   $$\n   M[(d(X) - \\varphi(T))\\varphi(T)] = 0.\n   $$\n   Таким образом:\n   $$\n   D(d(X)) = M(d(X) - \\varphi(T))^2 + M(\\varphi(T) - \\theta)^2 \\geq D(\\varphi(T)).\n   $$\n\n---\n## Теорема об оптимальной оценке\n\n### Теорема. \nОптимальная оценка, если она существует, необходимо является функцией от достаточной статистики.\n\n### Доказательство.\nЕсли это не так, можно получить оценку, у которой дисперсия не больше и которая является функцией от достаточной статистики. Она тоже будет оптимальной, но оптимальная оценка единственна.\n\n---\n## Полные статистики\n\nСтатистика $T$ называется **полной**, если для всякой (ограниченной) функции $\\varphi(T)$ из того, что $M\\varphi(T) = 0$ следует, что $\\varphi(T) \\equiv 0$.\n\n### Теорема единственности\nЕсли $T(x)$ полна, то для любой функции $\\tau(\\theta)$ существует единственная несмещенная оценка, зависящая от $T(x)$.\n\n### Доказательство.\nПусть $\\varphi_1(T)$ и $\\varphi_2(T)$ – несмещенные оценки $\\tau(\\theta)$. Тогда\n$$\nM\\varphi_1(T) = M\\varphi_2(T) = \\tau(\\theta).\n$$\nПо определению полной статистики:\n$$\n\\varphi_1(T) - \\varphi_2(T) \\equiv 0 \\Rightarrow \\varphi_1(T) \\equiv \\varphi_2(T).\n$$\n\n## Теорема\nВсякая полная достаточная статистика $S$ является минимальной достаточной статистикой.\n\n## Теорема\nЕсли существует полная достаточная статистика, то всякая функция от нее является оптимальной оценкой своего математического ожидания.\n\n---\n## Теорема Фишера\n\nПусть $X_1, \\ldots, X_n$ – выборка из распределения $N(a, \\sigma)$. Тогда:\n1. $\\frac{X_i - a}{\\sigma} \\in N(0, 1)$;\n2. $\\sum_{i=1}^n \\left( \\frac{X_i - a}{\\sigma} \\right)^2 \\in \\chi^2_n$;\n3. $\\bar{X} \\in N\\left( a, \\frac{\\sigma}{\\sqrt{n}} \\right)$;\n4. $\\frac{(\\bar{X} - a)\\sqrt{n}}{\\sigma} \\in N(0, 1)$.\n\n### Доказательство\n\n 1. Нормальное распределение инвариантно относительно линейного преобразования, следовательно, $\\frac{X_i - a}{\\sigma}$ – нормально распределена.\n\n\t- Математическое ожидание:\n  $$\n  M\\left( \\frac{X_i - a}{\\sigma} \\right) = \\frac{MX_i - a}{\\sigma} = \\frac{a - a}{\\sigma} = 0\n  $$\n\n\t- Дисперсия:\n  $$\n  D\\left( \\frac{X_i - a}{\\sigma} \\right) = \\frac{DX_i}{\\sigma^2} = \\frac{\\sigma^2}{\\sigma^2} = 1\n  $$\n\n2. По свойству распределения $\\chi^2$, сумма квадратов независимых случайных величин, распределенных по закону $N(0, 1)$, распределена по закону хи-квадрат с числом степеней свободы, равным числу слагаемых:\n$$\n\\sum_{i=1}^n \\left( \\frac{X_i - a}{\\sigma} \\right)^2 \\in \\chi_n^2\n$$\n\n3. Линейная комбинация нормальных величин есть нормальная величина. По свойствам среднего выборочного:\n\t- Математическое ожидание:\n  $$\n  M \\bar{X} = a\n  $$\n\t- Дисперсия:\n  $$\n  D \\bar{X} = \\frac{\\sigma^2}{n}\n  $$\n\n4. Очевидно:\n\t- Математическое ожидание:\n  $$\n  M\\left( \\frac{(\\bar{X} - a) \\sqrt{n}}{\\sigma} \\right) = 0\n  $$\n\t- Дисперсия:\n  $$\n  D\\left( \\frac{(\\bar{X} - a) \\sqrt{n}}{\\sigma} \\right) = 1\n  $$\n\n## Лемма 1\nПусть вектор $X = (X_1, \\ldots, X_n)^T$, где $X_i$ – независимы и $X_i \\sim N(0, \\sigma)$, $C$ – ортогональная матрица размерности $n \\times n$ и $Y = CX$. Тогда координаты вектора $Y = (Y_1, \\ldots, Y_n)^T$ независимы и $Y_i \\sim N(0, \\sigma)$, причем\n$$\n\\sum_{i=1}^n X_i^2 = \\sum_{i=1}^n Y_i^2.\n$$\n\n## Лемма 2 (Лемма Фишера)\nПусть вектор $X = (X_1, \\ldots, X_n)^T$, где $X_i$ – независимы и $X_i \\sim N(0, \\sigma)$, $C$ – ортогональная матрица размерности $n \\times n$ и $Y = CX$. Тогда для любого $k = 1, \\ldots, n-1$ величина\n$$\nQ(X) = \\sum_{i=1}^n X_i^2 - Y_1^2 - \\cdots - Y_k^2\n$$\nне зависит от $Y_1, \\ldots, Y_k$ и имеет распределение $N(0, \\sigma)$.\n\n## Основное следствие леммы Фишера\nПусть $X_1, \\ldots, X_n$ независимы и имеют нормальное распределение с параметрами $a$ и $\\sigma^2$. Тогда:\n1. Величина $\\frac{nS^2}{\\sigma^2}$ имеет распределение $\\chi_{n-1}^2$;\n2. $\\bar{X}, S^2$ независимы.\n\n## Доказательство\n\n### Пункт 1\n\nРассмотрим статистику:\n$$\nnS^2 = \\sum_{i=1}^n (X_i - \\overline{X})^2 = \\sum_{i=1}^n X_i^2 - \\overline{X}^2 = \\sum_{i=1}^n X_i^2 - \\left( \\frac{X_1}{\\sqrt{n}} + \\cdots + \\frac{X_n}{\\sqrt{n}} \\right)^2\n$$\n\nПоложим:\n$$\nY_1 = \\frac{X_1}{\\sqrt{n}} + \\cdots + \\frac{X_n}{\\sqrt{n}}\n$$\n\nТогда статистика:\n$$\nnS^2 = \\sum_{i=1}^n X_i^2 - Y_1^2\n$$\n\nПо лемме 2, $nS^2$ распределена как сумма $(n-1)$ квадратов независимых величин, имеющих распределение $N(0, \\sigma)$:\n$$\nnS^2 = \\sum_{i=2}^n Y_i^2, \\quad Y_i \\in N(0, \\sigma)\n$$\n\nТогда:\n$$\n\\frac{nS^2}{\\sigma^2} = \\sum_{i=2}^n \\left( \\frac{Y_i}{\\sigma} \\right)^2 = \\sum_{i=1}^n Z_i^2, \\quad Z_i \\in N(0, 1) = \\chi_{n-1}^2\n$$\n\n### Доказательство пункта 2\n\nРассмотрим:\n$$\nY_1 = \\sqrt{n} \\overline{X} = \\frac{X_1}{\\sqrt{n}} + \\cdots + \\frac{X_n}{\\sqrt{n}}\n$$\n\nТогда:\n$$\nS^2 = \\frac{1}{n} \\sum_{i=2}^n Y_i^2\n$$\n\nПо лемме 1, $Y_i$ с разными индексами независимы, следовательно, $\\overline{X}$ и $S^2$ независимы как функции независимых величин. \n\n### Замечание. \nВ выражении для $S^2$ присутствует $\\overline{X}$, т. е. они функционально зависимы. Тем не менее, по теореме Фишера они являются независимыми случайными величинами. Это свойство характерно только для нормального распределения.\n\n---\n## Теоремы о распределении выборочных характеристик\n\n### Теорема 1 (ДОК)\nПусть $X_1, \\ldots, X_n$ – выборка из распределения $N(a, \\sigma)$ и функция от выборочных среднего и дисперсии $t$ определена равенством\n$$\nt = \\sqrt{n-1} \\frac{\\bar{X} - a}{S}.\n$$\nТогда величина $t$ имеет распределение Стьюдента $T_{n-1}$.\n\n### Доказательство\nПреобразуем величину t\n\nРассмотрим величину:\n$$\nt = \\sqrt{n-1} \\frac{\\bar{X} - a}{S} = \\frac{(\\bar{X} - a) \\sqrt{n}}{\\sigma} \\cdot \\sqrt{\\frac{n-1}{n} \\frac{\\sigma}{S}}\n$$\n\nОбозначим первый сомножитель как:\n$$\n\\xi = \\frac{\\bar{X} - a}{\\sqrt{n}} \\sim N(0,1)\n$$\n\nВторой сомножитель:\n$$\n\\sqrt{\\frac{n-1}{n} \\frac{\\sigma}{S}} = \\sqrt{n-1} \\sqrt{\\frac{\\sigma^2}{nS^2}}\n$$\n\nПо теореме Фишера величина:\n$$\n\\frac{\\sigma^2}{nS^2} \\sim \\chi_{n-1}^2\n$$\n\nТаким образом:\n$$\n\\sqrt{n-1} \\sqrt{\\frac{\\sigma^2}{nS^2}} = \\frac{1}{\\sqrt{\\frac{\\chi_{n-1}^2}{n-1}}}\n$$\n\nТогда:\n$$\nt = \\frac{(\\bar{X} - a) \\sqrt{n}}{\\sigma} \\cdot \\frac{1}{\\sqrt{\\frac{\\chi_{n-1}^2}{n-1}}} = \\frac{\\xi}{\\sqrt{\\frac{\\chi_{n-1}^2}{n-1}}} \\sim T_{n-1}\n$$\n\n## Теорема 2\nПусть $X_1, \\ldots, X_n$ и $Y_1, \\ldots, Y_m$ – независимые выборки из распределения $N(a, \\sigma)$, а $\\bar{X}, \\bar{Y}, S_X^2, S_Y^2$ – выборочные средние и дисперсии. Тогда величина\n$$\nt = \\sqrt{\\frac{mn(m+n-2)}{m+n}} \\cdot \\frac{\\bar{X} - \\bar{Y}}{\\sqrt{nS_X^2 + mS_Y^2}}\n$$\nимеет распределение Стьюдента с $m + n - 2$ степенями свободы.\n\n## Теорема 3 (ДОК)\nПусть $X = (X_1, \\ldots, X_n) \\in N(a, \\sigma)$ и $Y = (Y_1, \\ldots, Y_m) \\in N(a, \\sigma)$ независимы. Тогда\n$$\nZ = \\frac{\\overline{S}_X^2}{\\overline{S}_Y^2} = F_{n-1, m-1},\n$$\nто есть $Z$ распределено по закону Фишера с $n - 1$, $m - 1$ степенями свободы.\n\n### Доказательство\nИз теоремы Фишера следует, что:\n$$\n\\chi_{n-1}^2 = \\frac{(n-1)S^2}{\\sigma^2} \\Rightarrow S^2 = \\frac{\\chi_{n-1}^2\\sigma^2}{n-1}\n$$\n\nТогда:\n$$\nZ = \\frac{\\chi_{n-1}^2\\sigma^2}{\\frac{n-1}{\\chi_{m-1}^2\\sigma^2}} = \\frac{\\chi_{n-1}^2}{\\frac{n-1}{\\chi_{m-1}^2}}\n$$\n## Теорема 4\nПусть $X = (X_1, \\ldots, X_n) \\in N(a_x, \\sigma_1)$ и $Y = (Y_1, \\ldots, Y_m) \\in N(a_y, \\sigma_2)$ независимы. Тогда случайная величина\n$$\nF = \\frac{n(m-1)\\sigma_2^2 S_X^2}{m(n-1)\\sigma_1^2 S_Y^2} = F_{n-1,m-1},\n$$\nто есть $F$ распределено по закону Фишера с $n - 1$, $m - 1$ степенями свободы.",
    "created_at": "2025-03-27T13:50:03.657928"
  },
  {
    "id": "1743061850.826484",
    "subject": "Теория вероятностей и математическая статистика",
    "date": "2025-02-20",
    "title": "Лекция 20.02.2025",
    "content": "## Вариационный ряд\n\nУпорядочим элементы выборки $X_1, \\ldots, X_n$ по возрастанию (операция ранжирования):\n\n$$\nX^*_1 \\leq \\cdots \\leq X^*_n\n$$\n### Вариационный ряд выборки:\n\n  $$\n  X^*_i = \\min \\{X_1, \\ldots, X_n\\}, \\, X^*_n = \\max \\{X_1, \\ldots, X_n\\}\n  $$\n\n  $X^*_i$ — $i$-й член вариационного ряда (вариант) или $i$-я порядковая статистика.\n\n---\n## Частоты и относительные частоты\n\nЧисла $n_i$, показывающие, сколько раз встречаются варианты $x_i$ в ряде наблюдений, называются **частотами**, а отношение их к объему выборки — **относительными частотами**:\n\n$$\n\\omega_i = \\frac{n_i}{n}\n$$\n\n$$\nn = \\sum_{i=1}^k n_i\n$$\n\n$$\n\\sum_{i=1}^k \\omega_i = 1\n$$\n---\n## Статистическое распределение выборки\n\nПример таблицы статистического распределения:\n\n| $x_i$ | 1    | 3    | 9    |\n|-------|------|------|------|\n| $n_i$ | 5    | 4    | 11   |\n| $\\omega_i$ | 0.25 | 0.2  | 0.55 |\n\n---\n## Эмпирическая функция распределения\n\nЭмпирическая функция распределения $F_n(x)$ определяется как:\n\n$$\nF_n(x) = \n\\begin{cases} \n0, & \\text{при } x \\leq x_1^*, \\\\ \n\\frac{k}{n}, & \\text{при } x_k^* < x \\leq x_{k+1}^*, k = 1, \\ldots, n-1, \\\\ \n1, & \\text{при } x > x_n^*.\n\\end{cases}\n$$\n\n### Пример\n\nОбъём выборки: $n = 5 + 4 + 11 = 20$.\n\n| $x_i$ | 1    | 3    | 9    |\n|-------|------|------|------|\n| $n_i$ | 5    | 4    | 11   |\n| $\\omega_i$ | 0.25 | 0.2  | 0.55 |\n\nЭмпирическая функция распределения:\n\n$$\nF_n(x) =\n\\begin{cases} \n0, & \\text{при } x \\leq 1, \\\\\n0.25, & \\text{при } 1 < x \\leq 3, \\\\\n0.45, & \\text{при } 3 < x \\leq 9, \\\\\n1, & \\text{при } x > 9.\n\\end{cases}\n$$\n---\n## Теорема о сходимости эмпирической функции распределения\n\n$F_n(x)$ — эмпирическая функция распределения, построенная по выборке $X = X_1, \\ldots, X_n$, и $F(x)$ — соответствующая теоретическая функция распределения. Тогда для любого $-\\infty < x < \\infty$ и $\\forall \\varepsilon > 0$:\n\n$$\n\\lim_{n \\to \\infty} P(F_n(x) - F(x) < \\varepsilon) = 1.\n$$\n### Доказательство:\n\n$F_n(x)$ — относительная частота события $\\{X < x\\}$ («успеха») в $n$ испытаниях Бернулли с вероятностью «успеха» $F(x)$. По закону больших чисел (теорема Бернулли) относительная частота события в $n$ независимых испытаниях сходится к вероятности этого события при $n \\to \\infty$, т.е. $F_n(x) \\to F(x)$.\n\n---\n## Cвойства эмпирической функции распределения\n\n### Математическое ожидание:\n$$\nMF_n(x) = F_\\xi(x).\n$$\n\n### Доказательство\nДоказательство этого свойства основано на математическом ожидании эмпирической функции:\n$$\nMF_n(x) = M\\left(\\frac{n_x}{n}\\right) = \\frac{M n_x}{n}.\n$$\nТак как $n_x \\sim B(n, F_\\xi(x))$ (биномиальное распределение), то:\n$$\nM n_x = n F_\\xi(x).\n$$\nПодставляя это в предыдущее уравнение, получаем:\n$$\nMF_n(x) = \\frac{n F_\\xi(x)}{n} = F_\\xi(x).\n$$\n---\n### Дисперсия:\n\n$$\nDF_n(x) = \\frac{F_{\\xi}(x)(1 - F_{\\xi}(x))}{n}\n$$\n\n### Доказательство:\n\n$$\nDF_n(x) = D\\left(\\frac{n_x}{n}\\right) = \\frac{D n_x}{n^2} = \\frac{n F_{\\xi}(x)(1 - F_{\\xi}(x))}{n^2} = \\frac{F_{\\xi}(x)(1 - F_{\\xi}(x))}{n}\n$$\n---\n### Асимптотическая нормальность:\n\n\n$$\n\\sqrt{n(F_n(x) - F_{\\xi}(x))} \\sim N(0, \\sqrt{F_{\\xi}(x)(1 - F_{\\xi}(x))}),\n$$\n\nт.е. $F_n(x)$ — асимптотически нормальная оценка для $F_{\\xi}(x)$.\n\n**Доказательство:**\n\n$$\nnF_n(x) = n_x\n$$\nможно представить в виде суммы независимых случайных величин, имеющих распределение Бернулли\n$$\nB(1, F_{\\xi}(x)),\n$$\n\nгде каждое слагаемое — «успех» события $\\{X_i < x\\}$. По центральной предельной теореме для суммы одинаково распределенных независимых случайных величин справедливо:\n\n$$\n\\frac{n(F_n(x) - F_{\\xi}(x))}{\\sqrt{nF_{\\xi}(x)(1 - F_{\\xi}(x))}} \\rightarrow u \\in N(0, 1)\n$$\n\n$$\n\\frac{\\sqrt{n(F_n(x) - F_{\\xi}(x))}}{\\sqrt{F_{\\xi}(x)(1 - F_{\\xi}(x))}} \\rightarrow u \\in N(0, 1)\n$$\n---\n## Теоремы о эмпирической функции распределения\n\n### Теорема Гливенко\n\n$F_n(x)$ — эмпирическая функция распределения, построенная по выборке $X = X_1, \\ldots, X_n$, и $F(x)$ — соответствующая теоретическая функция распределения. Тогда для любого $-\\infty < x < \\infty$:\n\n$$\nP\\left( \\lim_{n \\to \\infty} D_n = 0 \\right) = 1,\n$$\n\nгде\n\n$$\nD_n = D_n(x) = \\sup_{-\\infty < x < \\infty} |F_n(x) - F(x)|.\n$$\n---\n### Теорема Колмогорова\n\nЕсли функция $F(x)$ непрерывна, то при любом фиксированном $t > 0$:\n\n$$\n\\lim_{n \\to \\infty} P \\left( |\\sqrt{n}D_n < t| \\right) = K(t) = \\sum_{j=-\\infty}^{\\infty} (-1)^j e^{-2j^2t^2}.\n$$\n\nЗдесь $K(t)$ — предельная функция распределения Колмогорова (табулированная, можно использовать при $n \\geq 20$).\n\n---\n### Замечание\n\nЕсли задана вероятность $0 < \\alpha < 1$, то при $n \\to \\infty$ с вероятностью, близкой к $\\alpha$, функция $F(x)$ удовлетворяет неравенству:\n\n$$\n|F_n(x) - F(x)| \\leq \\frac{t_\\alpha}{\\sqrt{n}},\n$$\n\nгде величина $t_\\alpha$ находится как корень уравнения $K(t) = \\alpha$. \n\nТаким образом, можно записать:\n\n$$\nF_n(x) - \\frac{t_\\alpha}{\\sqrt{n}} \\leq F(x) \\leq F_n(x) + \\frac{t_\\alpha}{\\sqrt{n}}.\n$$\n\nУчитывая, что $0 \\leq F(x) \\leq 1$, неравенство можно уточнить:\n\n$$\n\\max \\left( 0, F_n(x) - \\frac{t_\\alpha}{\\sqrt{n}} \\right) \\leq F(x) \\leq \\min \\left( F_n(x) + \\frac{t_\\alpha}{\\sqrt{n}}, 1 \\right).\n$$\n\nИнтервал\n\n$$\n\\left[ \\max \\left( 0, F_n(x) - \\frac{t_\\alpha}{\\sqrt{n}} \\right), \\min \\left( F_n(x) + \\frac{t_\\alpha}{\\sqrt{n}}, 1 \\right) \\right]\n$$\n\nназывается **асимптотической $\\alpha$-доверительной зоной** для теоретической функции распределения $F(x)$.\n\n---\n### Теорема Смирнова\n\nПусть $F_n(x)$ и $F_m(x)$ — две эмпирические функции распределения, построенные на основе двух независимых выборок объемов $n$ и $m$ из одного и того же распределения $F_5$, и\n\n$$\nD_{n,m} = \\sup_{-\\infty < x < \\infty} |F_n(x) - F_m(x)|.\n$$\n\nТогда если теоретическая функция $F(x)$ непрерывна, то для любого фиксированного $t > 0$\n\n$$\n\\lim_{n,m \\to \\infty} P\\left(|\\sqrt{nm/(n+m)} D_{n,m} \\leq t| \\right) = K(t)\n$$\n---\n",
    "created_at": "2025-03-27T13:50:50.826484"
  },
  {
    "id": "1743061870.760705",
    "subject": "Теория вероятностей и математическая статистика",
    "date": "2025-02-27",
    "title": "Лекция 27.02.2025",
    "content": "## Свойства выборочного среднего\n\n### Основные свойства\n1. **Выборочная средняя постоянной равна самой постоянной**:\n   $$\n   \\overline{c} = c\n   $$\n\n2. **Изменение вариантов в одно и то же число раз**:\n   Если все варианты увеличить (уменьшить) в одно и то же число раз, то средняя арифметическая увеличится (уменьшится) во столько же раз:\n   $$\n   \\overline{kx} = k \\overline{x}\n   $$\n\n3. **Изменение вариантов на одно и то же число**:\n   Если все варианты увеличить (уменьшить) на одно и то же число, то средняя арифметическая увеличится (уменьшится) на то же число:\n   $$\n   \\overline{x + c} = \\overline{x} + c\n   $$\n\n4. **Средняя арифметическая отклонений от средней равна нулю**:\n   $$\n   \\overline{x - \\overline{x}} = 0\n   $$\n\n### Математическое ожидание выборочного среднего\n- Пусть выборка взята из совокупности с математическим ожиданием $Mξ = a$ и дисперсией $Dξ = σ^2$.\n- Тогда для каждого элемента выборки $X_i$, где $X_{i}$ - независимы:\n  $$\n  MX_i = a, \\quad DX_i = σ^2\n  $$\n- Математическое ожидание выборочного среднего:\n  $$\n  M\\overline{X} = a\n  $$\n\n### Сходимость выборочного среднего\n- При $n \\to \\infty$, выборочное среднее сходится по вероятности к математическому ожиданию:\n  $$\n  \\overline{X} \\xrightarrow{P} a\n  $$\n- По УЗБЧ Колмогорова, сходимость почти наверное:\n  $$\n  \\overline{X} \\xrightarrow{n.n.} a\n  $$\n\n### Центральная предельная теорема\n- При больших $n$, выборочное среднее имеет нормальное распределение:\n  $$\n  \\frac{(\\overline{X} - a) \\sqrt{n}}{\\sigma} \\sim N(0,1)\n  $$\n\n---\n## Свойства выборочной дисперсии\n\n### Основные свойства\n1. **Дисперсия постоянной равна нулю**:\n   $$\n   S_c^2 = 0\n   $$\n\n2. **Дисперсия не изменяется при сдвиге**:\n   $$\n   S_{X + c}^2 = S_X^2\n   $$\n\n3. **Дисперсия при умножении на константу**:\n   $$\n   S_{kX}^2 = k^2 S_X^2\n   $$\n\n4. **Минимальность дисперсии**:\n   Дисперсия минимальна, когда $c = \\overline{X}$:\n   $$\n   S^2 = \\min_{c} \\frac{1}{n} \\sum_{i=1}^n (X_i - c)^2\n   $$\n\n### Математическое ожидание выборочной дисперсии\n- Математическое ожидание выборочной дисперсии:\n  $$\n  MS^2 = \\frac{(n-1)\\mu_{2}}{n}\n  $$\n\n### Дисперсия выборочной дисперсии\n- Для нормального распределения $N(a, \\sigma)$:\n  $$\n  DS^2 = \\frac{2(n-1)}{n^2} \\sigma^4\n  $$\n\n---\n## Другие характеристики выборки\n\n### Структурные средние\n- **Среднее усеченное**: Среднее значение после отбрасывания фиксированного числа предельных значений.\n  $$\n  \\overline{x} = \\frac{\\sum_{i=p+1}^{n-p} x(i)}{n-2p}\n  $$\n\n- **Выборочная мода**: Значение, которое встречается чаще всего.\n  $$\n  n_i (m_0) = \\max_i n_i\n  $$\n\n- **Выборочная медиана**: Среднее значение в ранжированном ряду.\n  $$\n  m_e = X \\left[ \\frac{n}{2} \\right] + 1\n  $$\n### Медиана vs Среднее\n- Медиана менее чувствительна к выбросам, чем среднее.\n- Пример: \"Медианная зарплата\" vs \"Средняя зарплата\".\n\n---\n## Квантили\n\n### Определение\n- **Квантиль** — значение признака, которое делит выборку на две части с заданным соотношением.\n- Выборочная квантиль порядка $q$, $0<q<1$:\n  $$\n  X_q = X_{[nq] + 1}\n  $$\n### Часто используемые квантили\n- **Процентили**: $x_{p/100}, p = 1, ..., 99$.\n- **Децили**: $x_{p/10}, p = 1, ..., 9$.\n- **Квартили**: $x_{p/4}, p = 1, 2, 3$.\n- **Медиана**: $x_{1/2}$.\n\n### Симметричные выборки\nЕсли медиана и среднее близки друг к другу, то выборка называется симметричной: $$|m_{e} - \\overline x| \\leq \\frac{3s}{\\sqrt{ n }}$$\n\n---\n## Показатели вариации\n\n### Вариационный размах\n- Разница между максимальным и минимальным значениями:\n  $$\n  R = x_{\\text{max}} - x_{\\text{min}}\n  $$\n### Среднее квадратическое отклонение\n- Мера разброса данных:\n  $$\n  S = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (X_i - \\overline{X})^2}\n  $$\n### Коэффициент вариации\n- Относительная мера разброса:\n  $$\n  v = \\frac{S}{\\overline{X}} \\cdot 100\\%\n  $$\n\n---\n## Гистограмма и полигон частот\n\n### Гистограмма\n- Ступенчатая фигура, состоящая из прямоугольников, где:\n  - Основания — интервалы.\n    Длина частичного интервала: $h = \\frac{x_{max} -x_{min}}{1+3.322lg(n)}$\n  - Высоты — относительные частоты: $\\frac{n_i}{n h_i}$\n\t$n_{i}$ - частота элементов выборки в под-интервале\n### Полигон частот\n- Ломаная линия, проходящая через середины верхних границ прямоугольников гистограммы.\n### Пример построения гистограммы\n1. Составление вариационного ряда.\n2. Определение числа интервалов и их длины.\n3. Построение интервального ряда и гистограммы.\n---\n### Теорема\nПри $n \\to \\infty$ для любого $j = 1, \\ldots, k$:\n\n$$\n\\frac{n_i}{n h_i} \\cdot h_i \\xrightarrow{P} P(X \\in A_i) = \\int_{A_i} p(x) dx,\n$$\n\nт. е. площадь столбца гистограммы, построенного над произвольным интервалом группировки, с ростом объёма выборки сближается с площадью области под графиком плотности над этим же интервалом.\n\n### Доказательство\nРассмотрим площадь прямоугольника:\n\n$$\n\\frac{n_i}{n h_i} \\cdot h_i = \\frac{n_i}{n}\n$$\n\n- относительная частота попадания выборочных значений в соответствующий интервал.\n\nПо теореме Бернулли при $n \\to \\infty$:\n\n$$\n\\frac{n_i}{n}\n$$\n\nбудет сходиться по вероятности к вероятности попадания значения случайной величины $\\xi$ в соответствующий интервал.\n\n### Следствие\nПлощадь гистограммы равна 1.\n\n---\n## Выборочные моменты и поправки Шеппарда\n\n### Выборочные моменты\n- Приближенные формулы для моментов:\n  $$\n  \\alpha_k \\approx \\frac{1}{n} \\sum_{i=1}^n \\bar{X}_i^k n_i\n  $$\n  $$\n  m_k = \\frac{1}{n} \\sum_{i=1}^n (\\bar{X}_i - \\bar{X})^k n_i\n  $$\n### Поправки Шеппарда\n- Поправки для уменьшения систематических ошибок:\n  $$\n  \\tilde{\\alpha}_2 = \\frac{1}{n} \\sum_{i=1}^n \\bar{X}_i^2 n_i - \\frac{h^2}{12}\n  $$\n  $$\n  \\tilde{m}_2 = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X})^2 n_i - \\frac{h^2}{12}\n  $$\n\n---\n### Пример группировки выборки \n![[Пример 1.png]]![[Пример 2.png]]\n![[Пример 3.png]]![[Пример 4.png]]\n![[Пример 5.png]]\n",
    "created_at": "2025-03-27T13:51:10.760705"
  },
  {
    "id": "1743061907.997116",
    "subject": "Теория вероятностей и математическая статистика",
    "date": "2025-03-11",
    "title": "Практика 11.03.2025",
    "content": "### Задача 1\n\n![[мс1.jpg]]\n### Решение:\n$n_{1}$ = 200, $n_{2}$ = 300 \n$MX_{i} = a$, $DX_{i} = \\sigma^2$, $i = 1, 2$\n$MX = M\\left( \\frac{X_{1} + X_{2}}{2} \\right) = \\frac{1}{2}(MX_{1}+MX_{2}) = \\frac{1}{2} *2a = a$\n$DX = \\frac{1}{2} \\sigma^2$\n---\n### Задача 2\n\n![[мс2.jpg]]\n### Решение:\n1) Составляем уравнение $a + 0.2 + 0.8 - a = 1$\n   Ограничения: \n\t- $a \\geq 0$, \n\t- $0.8 - a \\geq 0$\n2) $MX_{i} = a + 0.6 + 4 - 5a = -4a + 4.6$\n3) $Ma = mX_{1} + 1.15 + \\frac{1}{n-1} \\sum_{i=2}X_{i} =$ \n   $= m(-4a + 4.6) + 1.15 + \\frac{1}{n-1}(n-1)(-4a+4.6) =$\n   $= -4ma +4.6m +1.15-4a+4.6 = a$\n4) $m(-4a+4.6) = 5a-5.75$\n   $m = \\frac{5a-5.75}{-4a+4.6}$\n---\n### Задача 3\n\n![[мс3.jpg]]\n\n### Решение:\n1) $MX_{i} = \\mu i$\n   $DX_i = 1.2i + 1.8i = 3i$\n2) $MX = \\frac{h*\\mu}{h} = \\mu$\n   $DX = \\frac{1}{n^2} * 3ni = \\frac{3i}{n}$\n3) $\\lim_{ n \\to \\infty } DX = \\lim_{ n \\to \\infty } \\frac{3i}{n} = 0$\n---\n### Задача 4\n\n![[мс4.jpg]]\n\n### Решение:\n1) $D\\theta_{1} = \\sigma_{1}^2$\n   $D\\theta_{2} = \\sigma_{2}^2$\n2) $\\theta = \\alpha \\theta_{1} + (1-\\alpha)\\theta_{2}$\n   $D\\theta = \\alpha^2 \\sigma_{1}^2 + (1-\\alpha)^2\\sigma_{2}^2$\n   $(D\\theta)' = 2\\alpha\\sigma_{1}^2 - 2(1-\\alpha)\\sigma_{2}^2$\n   $(D\\theta)'' = 2\\sigma_{1}^2 + 2\\sigma_{2}^2$\n3) Приравниваем первую производную к 0 и получаем $\\frac{\\sigma_{2}^2}{\\sigma_{1}^2 + \\sigma_{2}^2}$\n4) $M\\theta = \\alpha \\theta + (1- \\alpha)\\theta = \\theta$\n---\n### Задача 5\n\n![[]]\n\n### Решение:\n1) $M\\mu_{1} = \\mu + \\alpha \\mu + 3 \\beta \\mu = \\mu$\n   Получаем $\\alpha = 3 \\beta$\n2) $M\\mu_{2} = \\alpha \\mu - \\beta \\mu = \\mu$\n   Получаем $\\alpha - \\beta = 1$\n3) Решаем систему уравнений и получаем $\\alpha = \\frac{3}{2}, \\beta = 1/2$\n4) $D\\mu_{1} = \\sigma^2 + \\alpha^2 \\sigma^2 - 9\\beta^2 \\sigma^2 = \\sigma^2$\n   $D\\mu_{2} = \\alpha^2 \\sigma^2 - \\beta^2 \\sigma^2 = 2\\sigma^2$ \n\n### Задача 6\n\n![[]]\n\n### Решение:\n",
    "created_at": "2025-03-27T13:51:47.997116"
  },
  {
    "id": "1743061963.806961",
    "subject": "Объектно-ориентированный анализ и проектирование",
    "date": "2025-02-05",
    "title": "05.02.25",
    "content": "### *Выстрел*\nUser Story - как игрок могу приказать кораблю сделать выстрел, чтобы запустить ракету\nКоманда - выстрелить\nСобытие - ракета начала движения\nАгрегат - сама игра\n\n---\n### *Класс Game*\nНеобходимо указать игровой объект (космический корабль)\n1) Авторизовать пользователя (может ли он приказывать)\n2) Поиск игрового объекта\n3) Создать необходимый объект (фотонная ракета)\n4) Обновить состояние объекта\nВыстрел (GameItem ID, User ID)\nОбновить состояние игры - возврат от выстрела\n\n---\nДо : - имя объекта\nПосле : - имя потенциального класса\n_ - экземпляр класса\nx - объект уничтожен\n-> - объект создается\ndistrict - приказ\n\n---\n### *Класс Auth*\nGame использует для авторизации выстрела\nНе создается и не умирает в процессе взаимодействия\nВнутри выстрела происходит авторизация - ответ либо да, либо нет\nПроверить возможность стрельбы в данный момент\n\n---\n### *Объект Репозиторий* \nЧасть класса Game - стратегия в IOC контейнере\nХранилище существующих объектов\nПредоставить группа объектов по входящему запросу (Game Item)\nGame хранит Game Item в репозитории\n\n---\nФабрика + IOC\nПричем IOC не вложен в Фабрику\nСоздает фотонную ракету (Game Item)\nВозвращается Game Object\n\n---\n### *Команды*\nОбъект Order - key:value\nAuthCommand(Приказ) - если Game может обработать его -> Execute(), иначе - Исключение\nIAuthOrder\n\n---\n### *Стратегии*\n- Найти игровой объект - стратегия IOC, которая имеет ссылку на словарь Dict:\n\t1) Ключ - строковый ID объекта\n\t2) Значение - сам игровой объект\n\t3) Объекты хранить через тип Object\n\t4) В игровом объекте должна быть строка с ID\n- Выстрел:\n\t1) Стратегия IOC для создания нового игрового объекта\n\t2) Предполагаем, что эта стратегия создает словарь с 1! парой ID\n\t3) Необходимо получить от космического корабля:\n\t    - Cтартовое положение ракеты\n\t    - Направление движения ракеты\n\t    - Модуль скорости ракеты\n\t4) Необходимо установить свойства созданной ракеты в соответствии с параметрами выше - это можно сделать с помощью стратегии IOC, которая меняет состояние игрового объекта\n\t5) Необходимо добавить созданную и проинициализированную ракету в репозиторий игровых объектов - для этого нужна стратегия IOC\n\t6) Создать команду движения для ракеты\n\t7) Добавить эту команду в очередь всех остальных команд",
    "created_at": "2025-03-27T13:52:43.806961"
  },
  {
    "id": "1743062008.873044",
    "subject": "Методы оптимизации для машинного обучения",
    "date": "2025-02-13",
    "title": "Лекция 13.02.25",
    "content": "## Обозначения, определения  \n\nСтандартные обозначения для точек (векторов) $x = (x_1, \\ldots, x_n)$ из $n$-мерного линейного пространства $R^n$.  \n\nНижний индекс в записи $x = (x_1, \\ldots, x_n)$ — это номер координаты точки, верхний индекс в $x^k = (x_1^k, \\ldots, x_n^k)$ обозначает номер точки, верхний индекс «звездочка» характеризует решение задачи оптимизации $x^*$.  \n\nДля одномерной оптимизации $x^*$ — это уже может быть номер точки.  \n\n**Скалярная функция:**  \n$$ J(x) = J(x_1, \\ldots, x_n) $$  \n— это функция от $n$ переменных, определенная для всех $x = (x_1, \\ldots, x_n) \\in R^n$.  \n\nПочему скалярная? Значения функции — это скалярные величины (числа).  \n\n---\n\n## Общая постановка задачи оптимизации  \n\nМодели оптимизации принятия решений (математического программирования) — это модели, для которых характерны следующие черты:  \n— показатель эффективности (целевая функция) является функцией от элементов решения;  \n— ограничительные условия, налагаемые на возможные решения, имеют вид функциональных равенств или неравенств.  \n\n**Аналитическая постановка**  \n\nТребуется найти вектор  \n$$ x = (x_1, \\ldots, x_n) \\in R^n, $$  \nдоставляющий целевой функции  \n$$ f(x) = f(x_1, \\ldots, x_n) $$  \nмаксимум (минимум) на множестве допустимых решений (допустимой области), заданных ненустым множество точек  \n$$ D, \\, D \\subseteq R^n, \\, D = \\{x \\in R^n | \\mathcal{E}_i(x) \\leq 0, \\, i = 1, \\ldots, m\\} $$  \n$$ \\mathcal{E}_i(x) = \\mathcal{E}_i(x_1, \\ldots, x_n), \\, i = 1, \\ldots, m $$  \n\n---\n\n## Основные классы оптимизационных задач  \n\n### Классификация в зависимости от **существования или отсутствия** ограничений:  \n\n**Безусловная оптимизация** – оптимизация без ограничений:  \n   $$ D = R^n $$ **Условная оптимизация** – оптимизация при наличии ограничений:  \n   $$ D \\subset R^n $$  \n\n---\n\n### Классификация по виду функциональных зависимостей:  \n\n1. **Линейное программирование (ЛП)** – линейная целевая функция и линейные функции ограничений (*симплекс-метод*).  \n\n2. **Квадратичное программирование (КП)** – квадратичная целевая функция и линейные функции ограничений (*метод Баранкина-Дорфмана*).  \n\n3. **Выпуклое программирование (ВП)** – выпуклая целевая функция и выпуклые функции ограничений (*метод возможных направлений*).  \n\n4. **Глобальная оптимизация** – среди функций есть невыпуклые функции (*метод тяжелого шарика*).  \n\n---\n\n## Задача максимизации  \n\nЗадача максимизации функции $f(x)$ состоит в отыскании такого вектора $x^* \\in D$, что для всех $x \\in D$ справедливо неравенство  \n$$ f(x^*) \\geq f(x). $$  \n\nИз определения следует, что функция $f(x)$ на векторе $x^*$ принимает наибольшее возможное значение в области допустимых решений.  \n\n---\n\n## Оптимальное решение и оптимальное значение  \n\nВектор $x^* \\in D$ называется оптимальным решением или точкой минимума (максимума).  \nЗначение целевой функции $f(x)$ в точке $x = x^*$ называется  \nоптимальным значением.  \n\nОптимальное значение $f^* = f(x^*)$ всегда единственное, оптимальных решений $X^* \\subseteq D$ может быть множество:  \n$$ X^* = [x^* \\in D] \\quad f^* = f(x^*)] $$  \n\n---\n\n## Локальные и глобальные экстремумы  \n\nГлобальный минимум (максимум) $x^e \\in D$  \n- для всех $x \\in D$ справедливо неравенство $f(x^*) \\leq f(x)$ (для минимума)  \n- для всех $x \\in D$ справедливо неравенство $f(x^*) \\geq f(x)$ (для максимума)  \n\nЛокальный минимум (максимум) $x^e \\in O_x$, $O_x \\subset D$  \n- для всех $x \\in O_x$ справедливо неравенство $f(x^*) \\leq f(x)$ (для минимума)  \n- для всех $x \\in O_x$ справедливо неравенство $f(x^*) \\geq f(x)$ (для максимума)  \n\n---\n\n## Геометрическая интерпретация линии уровня  \n![[Линии уровня.png]]\n\n---\n\n### Два случая разрешимости задачи оптимизации  \n\n- единственное оптимальное решение:  \n  $$ x^* \\in D; $$  \n- множество оптимальных решений:  \n  $$ X^* = [x^* \\in D] \\, f^* = f(x^*)] $$  \nПример для ЛП:  \n  $$ x^* \\in \\{x \\in D | x = a x^{1/4} + (1 - a) x^{2/4}\\} $$  \n\n---\n\n### Два случая неразрешимости задачи оптимизации  \n\n- Пустое множество  \n- Неограниченная целевая функция  \n\n---\n\n## Общая схема итерационных методов  \n\nСтроится последовательность приближений $x^k$, сходящаяся к решению $x^*$:  \n$$ \\lim_{k \\to \\infty} x^k = x^*. $$  \n\nДля заданной точки $x^k$ вычисляются:  \n- вектор направления: $S^k$;  \n- величина шага: $\\alpha_k$ вдоль направления $S^k$;  \n- новая итерационная точка: $x^{k+1} = x^k + \\alpha_k S^k$.  \n\nВо многих алгоритмах вычисление величины шага $\\alpha_k$ реализуется решением задачи минимизации функции $f(x^k \\geq \\alpha S^k)$, зависящей от переменной $\\alpha$. А это – задача одномерного поиска. Поэтому одномерный поиск (одномерная минимизация, скалярная минимизация) является основой алгоритмов для решения задач нелинейного программирования.  \n\n---\n\n## Геометрическая интерпретация общей схемы методов  \n\n![[Общая схема методов.png]]\n---\n## Выпуклые функции  \n\nФункция $f(x)$ называется **выпуклой**, если для любых двух точек $x^1$ и $x^2$ из её области определения и для любого $\\alpha \\in [0, 1]$ выполняется неравенство:  \n$$ f(\\alpha x^1 + (1 - \\alpha) x^2) \\leq \\alpha f(x^1) + (1 - \\alpha) f(x^2). $$  \n\nГеометрически это означает, что отрезок, соединяющий любые две точки на графике функции, лежит выше или на самом графике.  \n\n### Свойства выпуклых функций:  \n1. **Локальный минимум является глобальным:** если функция выпукла, то любая точка локального минимума является также глобальным минимумом.  \n2. **Градиентный спуск сходится к глобальному минимуму:** для выпуклых функций градиентные методы гарантированно находят глобальный минимум.  \n3. **Гессиан положительно полу-определён:** если функция дважды дифференцируема, то её гессиан (матрица вторых производных) является положительно полуопределённой для всех точек области определения.  \n\n---\n\n## Положительно полуопределённая матрица  \n\nМатрица $C$ называется **положительно полуопределённой**, если для любого вектора $x \\neq 0$ выполняется неравенство:  \n$$ x^T C x \\geq 0. $$  \n\n### Свойства положительно полуопределённых матриц:  \n1. **Все собственные значения неотрицательны:** если матрица $C$ положительно полуопределённая, то все её собственные значения $\\lambda_i \\geq 0$.  \n2. **Связь с выпуклыми функциями:** если гессиан функции $f(x)$ (матрица вторых производных) является положительно полуопределённым для всех $x$, то функция $f(x)$ выпукла.  \n3. **Квадратичные формы:** для квадратичной функции $f(x) = \\frac{1}{2} x^T C x + b^T x + c$, матрица $C$ определяет, является ли функция выпуклой. Если $C$ положительно полуопределённая, то функция выпукла.  \n\n### Пример:  \nРассмотрим квадратичную функцию $f(x) = x_1^2 + x_2^2$. Её гессиан:  \n$$ C = \\begin{pmatrix} 2 & 0 \\\\ 0 & 2 \\end{pmatrix}. $$  \nМатрица $C$ положительно определённая, так как все её собственные значения положительны. Следовательно, функция $f(x)$ является выпуклой.  \n\n---\n",
    "created_at": "2025-03-27T13:53:28.873044"
  },
  {
    "id": "1743062036.69688",
    "subject": "Методы оптимизации для машинного обучения",
    "date": "2025-03-13",
    "title": "Лекция 13.03.25",
    "content": "# Безусловная (многомерная) оптимизация\n## План лекции:\n1. Безусловная минимизация: методы нулевого порядка.\n2. Безусловная минимизация: методы первого порядка.\n3. Безусловная минимизация: методы второго порядка.\n---\n## Классификация методов безусловной минимизации\n\nВ зависимости от максимального порядка частных производных минимизируемой функции $f(x)$, вычисление которых требует метод, методы принято делить на следующие классы:\n\n- **Методы нулевого порядка** – методы, использующие лишь вычисления самой функции.\n\n- **Методы первого порядка** – методы, требующие вычисления значений функции и ее первых частных производных.\n\n- **Методы второго порядка** – методы, требующие вычисления значений функции и ее первых и вторых частных производных.\n---\n### Для справки\n**Методы одномерной минимизации**: *дихотомия*, *Фибоначчи* и *золотое сечение* – используют вычисления только значений функции $f(x)$ и относятся к методам **нулевого порядка**.\n\n---\n## Метод циклического покоординатного спуска\n\nПрименяется для минимизации функции нескольких переменных и не требует использования производных.\n\nНаправления спуска – координатные векторы $S^1, ..., S^n$, где $S^j$ – вектор, все компоненты которого, за исключением $j$-й, равны нулю. При поиске по направлению $S^j$ меняется только $j$-я координата точки, все остальные координаты остаются зафиксированными.\n\n![[Спуск.jpg]]\n\n---\n### Алгоритм метода циклического покоординатного спуска\n\n**Подготовительный этап.** Начальная точка $x^1$, $\\varepsilon > 0$ – константа остановки; координатные векторы $S^1, ..., S^n$; начальное значение $y^1 = x^1$, $k = j = 1$.\n\n##### **Основной этап.**\n\n##### **Шаг 1.** \nВычисляем $\\alpha_j$ – оптимальное решение задачи минимизации $f(y^j + \\alpha s^j)$.  \nПолагаем $y^{j+1} = y^j + \\alpha_j s^j$.  \n\nЕсли $j < n$, то заменить $j$ на $j+1$ и вернуться к шагу 1.  \nЕсли $j = n$, то переходим к шагу 2.\n\n##### **Шаг 2.** \nПолагаем $x^{k+1} = y^{n+1}$. Если $\\|x^{k+1} - x^k\\| < \\varepsilon$, то остановиться.  \nВ противном случае, полагаем $y^1 = x^{k+1}$, $j=1$, заменяем $k$ на $k+1$ и переходим к шагу 1.\n\n##### **Сходимость метода**  \nК стационарной точке (точка с нулевым градиентом: $\\nabla f(x) = 0$) гарантируется при условии дифференцируемости минимизируемой функции $f(x)$.\n\n---\n## Метод Хука и Дживса\n\nМетод Хука и Дживса – модификация метода циклического покоординатного спуска, осуществляет два типа спуска – **исследующий поиск** и **поиск по образцу**.\n\n**Поиск по образцу (1)** – это ускоряющий шаг.\n**Исследующий поиск вдоль координатных осей (2)** – это спуск по координатным направлениям.\n\n![[Хук и Дживс (идея).jpg]]\n\n---\n### Алгоритм метода Хука и Дживса (без одномерного спуска)\n\n##### **Подготовительный этап.**\nНачальная точка $x^1$, $\\varepsilon > 0$ – константа остановки; начальная величина шага $\\Delta \\geq \\varepsilon$ ($\\Delta > \\varepsilon$); ускоряющий множитель $\\alpha > 0$; координатные векторы $d^1, ..., d^n$; начальное значение $y^1 = x^1$, $k = j = 1$.\n\n##### **Основной этап.**\n\n##### **Шаг 1.**  \nЕсли $f(y^j + \\Delta d^j) < f(y^j)$, то шаг считается успешным; \nполагаем $y^{j+1} = y^j + \\Delta d^j$ и переходим к шагу 2.  \n\nЕсли $f(y^j + \\Delta d^j) \\geq f(y^j)$, то шаг считается неудачным. В этом случае, если $f(y^j - \\Delta d^j) < f(y^j)$, то полагаем $y^{j+1} = y^j - \\Delta d^j$ и переходим к шагу 2, если же $f(y^j - \\Delta d^j) \\geq f(y^j)$, то полагаем $y^{j+1} = y^j$ и переходим к шагу 2.\n\n##### **Шаг 2.**  \nЕсли $j < n$, то заменяем $j$ на $j+1$ и возвращаемся к шагу 1. В противном случае, переходим к шагу 3, если $f(y^{n+1}) < f(x^k)$, и к шагу 4, если $f(y^{n+1}) \\geq f(x^k)$.\n\n##### **Шаг 3.**  \nПолагаем $x^{k+1} = y^{n+1}$, $y^1 = x^{k+1} + \\alpha (x^{k+1} - x^k)$. Заменяем $k$ на $k+1$, полагаем $j = 1$ и переходим к шагу 1.\n\n##### **Шаг 4.**  \nЕсли $\\Delta \\leq \\varepsilon$, то остановиться; $x^k$ – решение. В противном случае, заменяем $\\Delta$ на $\\Delta/2$. Полагаем $y^1 = x^k$, $x^{k+1} = x^k$, заменяем $k$ на $k+1$, полагаем $j = 1$ и возвращаемся к шагу 1.\n\n---\n### Алгоритм Хука и Дживса (с одномерной минимизацией)\n\n##### **Подготовительный этап.** \nНачальная точка $x^1$, $\\varepsilon > 0$ – константа остановки; координатные векторы $S^1, ..., S^n$; начальное значение $y^1 = x^1$; $k = j = 1$.\n\n##### **Основной этап.**\n\n##### **Шаг 1.** \nВычисляем $\\alpha_j$ – оптимальное решение задачи одномерной минимизации $f(y^j + \\alpha s^j)$  \nи полагаем $y^{j+1} = y^j + \\alpha_j s^j$\n\nЕсли $j < n$, то заменяем $j$ на $j + 1$ и повторяем шаг 1.\n\nЕсли $j = n$, то полагаем $x^{k+1} = y^{n+1}$\n\nЕсли $\\|x^{k+1} - x^k\\| < \\varepsilon$, то остановиться.\n\nВ противном случае переходим к шагу 2.\n\n##### **Шаг 2.** \nПолагаем $d = x^{k+1} - x^k$ и находим $\\tilde{\\alpha}$  – оптимальное решение задачи минимизации $f(x^{k+1} + \\alpha d)$ . Полагаем $y^1 = x^{k+1} + \\tilde{\\alpha} d, j = 1$, заменяем $k$ на $k+1$ и переходим к шагу 1.\n\n![[Хук и Дживс.jpg]]\n\n---\n## Метод Розенброка (метод вращающихся координат)\n\nПрименяется для минимизации функции нескольких переменных и не требует использования производных.\n\nНачиная с текущей точки $x^k$ итеративный поиск осуществляется вдоль $n$ **линейно независимых** $s^1, ..., s^n$, по норме равных единице, **ортогональных направлений**  \n$$(s^i, s^j) = 0 \\quad \\text{для} \\quad i \\neq j.$$\n![[Розенброк (вращение).jpg]]\n\n---\n### Построение новых направлений в методе Розенброка\n\nПосле того, как получена новая точка $x^{k+1} = x^k + \\sum_{j=1}^n a_j s^j$, строится новое множество ортогональных векторов $S^1, ..., S^n$ на основе **множества линейно-независимых векторов** $z^j, j = 1, ..., n$\n\n$$z^j =\n\\begin{cases} \ns^j, & \\text{если } \\alpha_j = 0, \\\\\n\\sum_{i=j}^n \\alpha_i s^i, & \\text{если } \\alpha_j \\neq 0;\n\\end{cases}$$\n\nс помощью **процедуры Грама–Шмидта (ортонормализации):**\n\n$$g^j =\n\\begin{cases} \nz^j, & \\text{если } j = 1, \\\\\nz^j - \\sum_{i=1}^{j-1} (z^j, \\overline{s}^i) \\overline{s}^i, & \\text{если } j \\geq 2;\n\\end{cases}$$\n\n$$\\overline{s}^j = \\frac{g^j}{\\| g^j \\|}$$\n\nНовые направления $\\overline{s}^1, ..., \\overline{s}^n$ линейно независимы, взаимно ортогональны и нормированы.\n\n![[Грамм-Шмитд.jpg]]\n\n---\n### Алгоритм Розенброка (с минимизацией по направлению)\n\n![[Розенброк.jpg]]\n##### **Подготовительный этап.** \nНачальная точка $x^1$, $\\varepsilon > 0$ – константа остановки; координатные векторы $s^1, ..., s^n$; начальное значение $y^1 = x^1$, $k = j = 1$.\n\n##### **Основной этап.**\n\n##### **Шаг 1.**  \nВычисляем $\\alpha_j$ – оптимальное решение задачи минимизации $f(y^j + \\alpha s^j)$ и полагаем $y^{j+1} = y^j + \\alpha_j s^j$. Если $j < n$, то заменяем $j$ на $j + 1$ и повторяем шаг 1.\n\nВ противном случае переходим к шагу 2.\n\n##### **Шаг 2.**  \nПолагаем $x^{k+1} = y^{n+1}$. \nЕсли $\\|x^{k+1} - x^k\\| < \\varepsilon$, то остановиться. \nВ противном случае полагаем $y^1 = x^{k+1}$, $j = 1$, заменяем $k$ на $k+1$ и переходим к шагу 3.\n\n##### **Шаг 3.**  \nСтроим новое множество линейно независимых, по норме равных единице и взаимно ортогональных направлений с помощью **процедуры Грама–Шмидта**. Обозначаем новые направления через $s^1, ..., s^n$ и возвращаемся к шагу 1.\n\n---\n## Градиентные методы\n\n**Направление спуска $S^k$ – это антиградиент функции**  $f(x) \\text{ в точке } x^{k}.$\n\n$$s^k = -\\nabla f(x^k) = - \\left( \\frac{\\partial f}{\\partial x_1}(x^k), \\frac{\\partial f}{\\partial x_2}(x^k), \\ldots, \\frac{\\partial f}{\\partial x_n}(x^k) \\right)$$\n\nнаправление наибольшего убывания функции  $f(x) \\text{ в точке } x^k.$\n\n**Различие градиентных методов**  – в способах выбора величины шага  $\\alpha_k$.\n\n- **Градиентный метод с постоянным шагом,**\n- **Градиентный метод с дроблением шага,**\n- **Метод наискорейшего спуска (градиентный метод с минимизацией по направлению).**\n\n### Два критерия окончания процесса решения:\n- Близость двух последующих приближений $x^k$ и $x^{k+1}$.\n- Близость к нулю градиента $\\nabla f(x)$.\n---\n### Алгоритм градиентного метода с постоянным шагом\n\n##### **Подготовительный этап.** \nНачальная точка $x^1$, $\\varepsilon > 0$ – константа остановки; **постоянная величина шага**  $\\alpha > 0$; $k = 1$.\n\n##### **Основной этап.**\n\n##### **Шаг 1.** \nВычисляем  $s^k = -\\nabla f(x^k)$ и полагаем $\\alpha_k = \\alpha$\n\n##### **Шаг 2.** \nПолагаем $x^{k+1} = x^k + \\alpha_k s^k$\n\n##### **Шаг 3.** \nЕсли  $\\|x^{k+1} - x^k\\| < \\varepsilon,$ то приближенное решение задачи равно $x^{k+1},$ иначе $k := k + 1$ и повторяем основной этап (перейти к шагу 1).\n\n##### **Сходимость метода:**\n\n$f(x)$ – **сильно** выпуклая с константой $\\rho$, дважды непрерывно дифференцируемая функция, градиент удовлетворяет условию Липшица:  \n$$\\|\\nabla f(x) - \\nabla f(y)\\| \\leq \\Lambda \\|x - y\\|.$$  \nТогда градиентный метод с постоянным шагом  \n$$\\alpha \\in (0, \\frac{2}{\\Lambda})$$  \nсходится линейно c \n$$q = \\max\\{|1 - \\alpha \\rho|, |1 - \\alpha \\Lambda|\\}.$$\n\n![[Градиент.jpg]]\n\n---\n### Алгоритм градиентного метода с дроблением шага\n\n#### **Подготовительный этап.** \nНачальная точка $x^1$, $\\varepsilon > 0$ – константа остановки; начальная величина шага $\\bar{\\alpha} > 0; \\, k = 1.$\n\n##### **Основной этап.**\n\n##### **Шаг 1.** \nВычисляем $s^k = -\\nabla f(x^k)$ и полагаем  $\\alpha = \\bar{\\alpha}.$\n\n##### **Шаг 2.** \nПолагаем $x = x^k + \\alpha s^k.$\n\n##### **Шаг 3.** \nЕсли $f(x) < f(x^k),$ то удваиваем шаг: $\\alpha = 2\\alpha.$  \nПерейти к шагу 2. \nПроцесс удвоения продолжаем до тех пор, пока убывание не прекратится, после чего перейти к шагу 5.\n\n##### **Шаг 4.** \nЕсли  $f(x) \\geq f(x^k),$ то уменьшаем шаг: $\\alpha = \\alpha/2.$  \nПерейти к шагу 2. \nПроцесс дробления продолжаем до тех пор, пока не наступит убывание, после чего перейти к шагу 5.\n\n##### **Шаг 5.** \nПолагаем  $x^{k+1} = x.$  \nЕсли $\\|x^{k+1} - x^k\\| < \\varepsilon,$ то приближенное решение задачи равно $x^{k+1},$ иначе $k := k + 1$ и повторяем основной этап (перейти к шагу 1).\n\n##### **Сходимость метода:**\n$f(x)$ – **строго** выпуклая, дважды непрерывно дифференцируемая функция, собственные числа матрицы вторых производных при любом $x$ содержатся в интервале $[m, M]$, $m > 0$.\n\n---\n### Алгоритм метода наискорейшего спуска\n\n##### **Подготовительный этап.** \nНачальная точка $x^1$, $\\varepsilon > 0$ – константа остановки, $k = 1$.\n##### **Основной этап.** \nЕсли $\\|\\nabla f(x^k)\\| < \\varepsilon,$  то остановиться; в противном случае:  \n$$S^k = -\\nabla f(x^k),$$\n$\\alpha_k$ – решение задачи одномерной минимизации $f(x^k + \\alpha S^k) \\text{ при } \\alpha \\geq 0$, $x^{k+1} = x^k + \\alpha_k S^k$,\n$k := k + 1$ и повторяем основной этап.\n\n##### **Сходимость метода:**  \n$f(x)$ – **строго** выпуклая, дважды непрерывно дифференцируемая функция, собственные числа матрицы вторых производных при любом $x$ содержатся в интервале $[m, M]$, $m > 0$.  \n\n##### **Скорость сходимости:** линейная с параметром  \n$$q = \\frac{M - m}{M + m}.$$\n---\n## Масштабирование\n\nЗадача **плохо масштабирована**, если изменения для целевой функции в зависимости от одних координат (в одном направлении) приводят к гораздо большим изменениям в значении целевой функции, чем в другом направлении.\n\n### Пример\n$f(x_1, x_2) = x_1^2 + 100 \\, x_2^2$  чувствительна к $x_2$, но не чувствительна к $x_1$.\n\n![[Масштаб 1.jpg]]\n\nДля $f(x_1, x_2) = x_1^2 + 100 \\, x_2^2$ заменяем $x_2$ на $(10^{-1} \\, x_2)$; $x_1$ – без изменения.  \nПолучаем функцию $g(x_1, x_2) = x_1^2 + 100(10^{-1} \\, x_2)^2 = x_1^2 + x_2^2$.\n\n![[Масштаб 2.jpg]]\n\nАнтиградиент функции $g(x_1, x_2)$ равен антиградиенту $f(x_1, x_2)$ умноженному на диагональную матрицу $D$ с положительными значениями диагоналей $(1, 10^{-2})$.\n\n$$- \\nabla g(x_1, x_2) = -D\\nabla f(x_1, x_2)$$\n![[Масштаб 3.jpg]]\n\n---\n## Метод «тяжелого шарика»\n\n![[Идея тяжелого шарика.jpg]]\n\n---\n\n### Алгоритм поиска минимума методом «тяжелого шарика»\n\n##### **Подготовительный этап.** \nНачальные точки $x^0, x^1$, $\\varepsilon > 0$ – константа остановки; параметры $\\alpha > 0$, $d > 0$; $k = 1$.\n\n##### **Основной этап.**\n\n##### **Шаг 1.** \nВычисляем $s^k = -\\nabla f(x^k)$, вычисляем предполагаемую точку минимума  \n$x$ по формуле  \n\n$$x = x^k + \\alpha s^k + d(x^k - x^{k-1})$$\n\n##### **Шаг 2.** \nЕсли $f(x) < f(x^k)$ и выполняется условие $\\|x - x^k\\| < \\varepsilon$, то решение задачи $x^* = x$; иначе $x^k = x$, $k := k + 1$ и повторяем основной этап (перейти к шагу 1).\n\n##### **Шаг 3.** \nЕсли $f(x) \\geq f(x^k)$, то $x^k = x$, изменяем (уменьшаем) параметры $\\alpha, d$;  $k := k + 1$ и повторяем основной этап (перейти к шагу 1).\n\n---\n### Геометрия метода «тяжелого шарика»\n\n![[Тяжелый шарик.jpg]]\n\n",
    "created_at": "2025-03-27T13:53:56.696880"
  },
  {
    "id": "1743062056.46708",
    "subject": "Методы оптимизации для машинного обучения",
    "date": "2025-03-27",
    "title": "Лекция 27.02.25",
    "content": "## Постановка задачи безусловной оптимизации\n\nМодели оптимизации принятия решений (математического программирования) – это модели, для которых характерны следующие черты:\n\n- Показатель эффективности (целевая функция) является функцией от элементов решения.\n- Ограничений на элементы решений нет.\n\nПусть $f(x) = f(x_1, ..., x_n)$ – скалярная функция n переменных.\n\nТребуется найти вектор $x = (x_1, ..., x_n) \\in \\mathbb{R}^n$, доставляющий целевой функции  \n$f(x) = f(x_1, ..., x_n)$  минимум (максимум) на множестве элементов из $\\mathbb{R}^n$\n\n### Аналитическая постановка\n\n$$ f(x) \\to \\min, \\quad x \\in \\mathbb{R}^n $$\n\n$$ f(x) \\to \\max, \\quad x \\in \\mathbb{R}^n $$\n\n---\n\n## Необходимое условие минимума в безусловной оптимизации\n\nПусть $f(x) = f(x_1, \\ldots, x_n)$ имеет непрерывные **частные производные** $$ \\frac{\\partial f}{\\partial x_i}(x), \\quad i=1,\\ldots,n $$Если $x^*$ – решение задачи безусловной минимизации функции $f(x)$, тогда\n\n$$ \\frac{\\partial f}{\\partial x_i}(x^*) = 0, \\quad i=1,\\ldots,n $$\n\nили в векторной форме $\\nabla f(x^*) = 0$, где\n\n$$ \\nabla f(x^*) = \\left( \\frac{\\partial f}{\\partial x_1}(x^*), \\frac{\\partial f}{\\partial x_2}(x^*), \\ldots, \\frac{\\partial f}{\\partial x_n}(x^*) \\right) $$\n\n— вектор-градиент функции $f(x)$ в точке $x^*$\n\n---\n\n## Выпуклая функция\n\nФункция $f(x)$, определенная для всех $x = (x_1, ..., x_n) \\in \\mathbb{R}^n$, называется **выпуклой**, если для любых $$ x^1, x^2 \\in \\mathbb{R}^n, \\alpha \\in [0,1] $$ справедливо неравенство  \n\n$$ f(\\alpha x^1 + (1 - \\alpha)x^2) \\leq \\alpha f(x^1) + (1 - \\alpha) f(x^2) $$\n\n### Основное свойство выпуклой функции\n\nНеобходимое условие минимума является **достаточным**.\n\n---\n\n## Аналитическое условие выпуклости\n\nПусть $f(x)$ имеет непрерывные вторые частные производные  \n$$ \\frac{\\partial^2 f}{\\partial x_i \\partial x_j} (x), \\quad i, j = 1, \\ldots, n. $$\n\nТогда $f(x)$ выпукла в том и только в том случае, если ее матрица вторых частных производных $F(x)$ неотрицательно определена (положительно полу-определена), то есть выполняется неравенство  \n$$ \\sum_{i,j=1}^n \\frac{\\partial^2 f}{\\partial x_i \\partial x_j} (x) y_i y_j \\geq 0 $$  \nдля всех $$ y = (y_1, \\ldots, y_n) \\in \\mathbb{R}^n $$\n\n---\n\n## Строго выпуклая функция\n\nФункция $f(x)$, определенная для всех $x = (x_1, ..., x_n) \\in \\mathbb{R}^n$, называется **строго выпуклой**, если для любых \n$$ \\text{} x^1, x^2 \\in \\mathbb{R}^n, \\, x^1 \\neq x^2, \\, \\alpha \\in (0,1) $$  \nсправедливо **строгое** неравенство  \n$$ f(\\alpha x^1 + (1 - \\alpha)x^2) < \\alpha f(x^1) + (1 - \\alpha)f(x^2) $$\n\n### Основное свойство строго выпуклой функции\n\nСтрого выпуклая функция имеет **единственную** точку минимума.\n\n---\n\n## Сильно выпуклая функция\n\nФункция $f(x)$, определенная для всех $x = (x_1, ..., x_n) \\in \\mathbb{R}^n$, называется **сильно выпуклой**, если существует такая константа $$ \\rho > 0 $$, что для любых $$ x^1, x^2 \\in \\mathbb{R}^n,\\alpha \\in [0,1]$$ справедливо неравенство  \n$$ f(\\alpha x^1 + (1 - \\alpha)x^2) \\leq \\alpha f(x^1) + (1 - \\alpha) f(x^2) - \\alpha(1 - \\alpha) \\rho \\|x^1 - x^2\\|^2 $$\n\n### Основное свойство сильно выпуклой функции\n\nДля любой точки $x^0$ множество  $X_0 = \\{x \\in \\mathbb{R}^n \\mid f(x) \\leq f(x^0)\\}$  - ограниченное ( и компактное).\n\n---\n\n## Скорость сходимости\n\nНормой вектора $$ z = (z_1, z_2, \\ldots, z_n) $$ называется число вида\n\n$$ \\|z\\| = \\sqrt{z_1^2 + z_2^2 + \\ldots + z_n^2}. $$\n\nЕсли даны два вектора $z^1$ и $z^2$, то число $$ \\|z^2 - z^1\\| $$является **расстоянием** между этими векторами.\n\nМетод спуска имеет **линейную скорость сходимости** (или сходится со **скоростью геометрической прогрессии**), если\n\n$$ \\|x^{k+1} - x^*\\| < q \\|x^k - x^*\\|, $$\n\nгде некоторая константа $$ 0 < q < 1 $$\nСкорость сходимости **сверх-линейная**, если\n\n$$ \\|x^{k+1} - x^*\\| < q_k \\|x^k - x^*\\|,\\ q_k \\to 0 $$\n\nСкорость сходимости имеет **степень $\\beta > 1$** (при $\\beta = 2$ – **квадратичная**), если\n\n$$ \\|x^{k+1} - x^*\\| < C \\|x^k - x^*\\|^\\beta, 0 < C < 1$$\n\n,где $C$ - некоторая константа \n\n---\n## Общая схема итерационных методов\n\nДля заданной точки $x^k$ вычисляются:\n\n- **вектор направления**:  $s^k$\n- **величина шага**:  $\\alpha_k$ вдоль направления $s^k$  \n- **новая итерационная точка**:  $x^{k+1} = x^k + \\alpha_k s^k$\n---\n### Общие условия для выбора направления и величины шага\n\nДля выбора направления $s^k$ при малых $\\alpha > 0$ предъявляются два требования:\n\n- точка $x^{k+1} = x^k + \\alpha s^k$ **допустимая**;\n- значение целевой функции в точке $x^k + \\alpha s^k$ **лучше, чем в точке** $x^k$\n\nДля вычисления величины шага $\\alpha_k$ вдоль направления $s^k$, как правило, решается задача **одномерной минимизации** функции $$ f(x^k + \\alpha s^k) $$, зависящей от переменной $\\alpha$, то есть **задача одномерного спуска**.\n\n---\n### Геометрическая интерпретация\n\n![[Общая схема в условной.jpg]]\n\n---\n### Критерии выпуклости\n- Неотрицательная определенность матрицы $F(x)$ - выпукла ли вообще:\n\t1) Все собственные числа матрицы $F(x)$ неотрицательны\n\t2) Все главные миноры неотрицательны\n- Положительная определенность матрицы $F(x)$ - строго ли выпукла:\n\t3) Все собственные числа матрицы $F(x)$ положительны\n\t4) Все угловые миноры матрицы $F(x)$ положительны\n\n![[Матрица F(x).jpg]]\n\n",
    "created_at": "2025-03-27T13:54:16.467080"
  },
  {
    "id": "1743062145.65952",
    "subject": "Методы оптимизации для машинного обучения",
    "date": "2025-03-06",
    "title": "Практика 06.03.25",
    "content": "## Пример 1\n$$f(x) = x_{1}^2 + 4x_{1}x_{2}+4x_{2}^2+x_{3}^2$$\nНаходим частные производные 1 порядка:\n1) $$\\frac{df}{dx_{1}} =2x_{1} + 4x_{2}$$\n2) $$\\frac{df}{dx_{2}} = 4x_{1}+8x_{2}$$\n3) $$\\frac{df}{dx_{3}} = 2x_{3}$$\nНаходим частные производные 2 порядка:\n4) $$\\frac{d^2f}{dx_{1}^2} = 2$$\n5) $$\\frac{d^2f}{dx_{2}^2} = 8$$\n6)  $$\\frac{d^2f}{dx_{3}^2} = 2$$\nНаходим частные смешанные производные:\n7) $$\\frac{d^2f}{dx_{1}x_{2}} = 4$$\n8) $$\\frac{d^2f}{dx_{2}x_{3}} = 0$$\n9) $$\\frac{d^2f}{dx_{1}x_{3}} = 0$$\nПолучаем матрицу c $$- \\lambda $$\n$$\\begin{bmatrix}  \n2 - \\lambda & 4 & 0 \\\\  \n4 & 8 - \\lambda & 0 \\\\\n0 & 0 & 2 - \\lambda \\\\\n\\end{bmatrix}$$\nИщем корни характеристического многочлена:\n$$ (2-\\lambda)(8-\\lambda) - 16(2-\\lambda) = \\lambda(2-\\lambda)(\\lambda-10)$$\nКорни:\n$$\\lambda_{1} = 0,\\ \\lambda_{2} = 2,\\ \\lambda_{3} = 10$$\n**По первому критерию** функция выпуклая, поскольку $$ \\lambda_{i} \\geq 0$$\n## Пример 2\n$$f(x) = x_{1}^2 + x_{1}x_{2} + 2x_{2}^2 + x_{1} - x_{2}$$\nПолучаем матрицу c $$- \\lambda $$\n$$\\begin{bmatrix}  \n2 - \\lambda & 1 \\\\  \n1 & 4 - \\lambda \\\\\n\\end{bmatrix}$$\nИщем корни характеристического многочлена:\n$$ (2-\\lambda)(4-\\lambda) - 1 = \\lambda^2 - 6\\lambda + 7$$\nКорни:\n$$ \\lambda_{1,2} = 3 \\pm \\sqrt{2}$$\n**По третьему критерию** функция является строго выпуклой\n\nСоставляем систему уравнений из частных производных 1 порядка для точки минимума:\n$$\\begin{cases}\n   2x_{1} + x_{2} + 1 = 0 \\\\\n   4x_{2} + x_{1} - 1 = 0\n \\end{cases}$$\nРезультат:\n$$ \nx_{1} = -\\frac{5}{7}, \\ x_{2} = \\frac{3}{7}\n$$\nОн является точкой минимума функции по **достаточности необходимого условия минимума**\n\n## Пример 3\n\n$$ f(x) = x_{1}^2 - x_{1}x_{2} + x_{2}^2 - x_{2}x_{3} + x_{3}^2$$\nЧастные производные 1 порядка:\n1) $$\\frac{df}{dx_{1}} = 2x_{1} - x_{2}$$\n2) $$\\frac{df}{dx_{2}} = -x_{1} + 2x_{2} - x_{3}$$\n3) $$\\frac{df}{dx_{3}} = - x_{2} + 2x_{3}$$\nЧастные производные 2 порядка:\n1) $$\\frac{d^2f}{dx_{1}^2} = 2$$\n2) $$\\frac{d^2f}{dx_{2}^2} = 2$$\n3) $$\\frac{d^2f}{dx_{3}^2} = 2$$\nСмешанные производные:\n1) $$\\frac{d^2f}{dx_{1}x_{2}} = -1$$\n2) $$\\frac{d^2f}{dx_{2}x_{3}} = -1 $$\n3) $$\\frac{d^2f}{dx_{1}x_{3}} = 0 $$\nПолучаем матрицу c\n$$\\begin{bmatrix}  \n2 & -1 & 0 \\\\  \n-1 & 2 & -1 \\\\\n0 & -1 & 2 \\\\\n\\end{bmatrix}$$\nЗыкина летит вперед планеты :(\n\n## Пример 4\n$$ f(x_{1},x_{2}) = (x_{1}-1)^2 + (x_{2}+3)^2 \\to min $$\nЧтобы построить конкретную линию уровня рассмотрим случай \n$$ f(x_{1}, x_{2}) = 2 ^ 2 =4$$\nЦентр линии уровня:\n$$\n(1, -3)\n$$\nДосчитаем ключевые точки при радиусе = 2:\n$$\n(-1, -3), \\ (1, -1), \\ (3, -3), \\ (1, -5)\n$$\nЛинии уровня (фото):\nРисуем градиент для спонтанной точки\n$$\nx^0 = (1, 1)\n$$\n",
    "created_at": "2025-03-27T13:55:45.659520"
  },
  {
    "id": "1743062233.882296",
    "subject": "Методы оптимизации для машинного обучения",
    "date": "2025-02-07",
    "title": "Практика 07.02.25",
    "content": "### *Транспортная задача в закрытой форме*\nФиктивный пункт запасов - добавить при несоблюдении балансового равенства\nСтоимость перевозки в таком случае тоже будет фиктивная и равна 0\nЧисло базисных переменных = m+n-1, некоторые из них могут быть равны 0\nМетод минимальной стоимости может дать решение лучше, чем метод СЗУ\n\nНачальный опорный план перевозок:\n0 10 0   |10\n0 10 10 |20\n0  0 30  |30\n20 10 0 |30\n20 30 40\nИщем новое базисное решение, если не выполняется условие стоимости: псевдо<= реальной\ndL = (псевдо - реальная) * x, где х - помеченные \"-\" в цикле пересчета\nL = (sum)sum(c_ij * x_ij)\n### Цикл пересчета\nЗамкнутая ломаная, которая начинается в небазисной клетке и проходит через базисные клетки с поворотом в них на 90 градусов. Для каждой небазисной клетки такой цикл один.\nЧередуется с + и - в таблице (сохранение баланса путем перераспределения)\n\n### Пересчет таблицы\nВычитается минимальный отрицательный x\nПереносятся в новую таблицу те значения, которые не были затронуты в прошлой\nДо тех пор пока не наступит условие стоимости\n\n\n\n",
    "created_at": "2025-03-27T13:57:13.882296"
  },
  {
    "id": "1743062256.234386",
    "subject": "Методы оптимизации для машинного обучения",
    "date": "2025-03-14",
    "title": "Практика 14.03.25",
    "content": "### Задача 1\n\n$f(x) = 4x_{1}^2 + (x_{2}-2)^2 \\to min \\ x \\in R^2$\n\nПриравниваем к $const = 16$: \n$4x_{1}^2 + x_{2}^2 - 4x_{2} + 4 = 16$\n$4x_{1}^2 + x_{2}^2 - 4x_{2} = 12$\n\nИщем 5 точек:\n1) Центр: $x^* = (0, 2)$\n2) Вершины: $(0, -2)$, $(0, 6)$, $(-2, 2)$, $(2,2)$\n\n![[Эллипс.png]]\n\n$\\nabla f(x_{1},x_{2}) = \\begin{pmatrix} 8x_{1} \\\\ 2x_{2} - 4 \\\\ \\end{pmatrix}$\n$S = - \\nabla f(x_{1}, x_{2})$\n---\n## Алгоритм с постоянным шагом (неудачная $\\alpha$)\n#### Шаг 0\n$x_{1} -$ произвольное, $x' = (2, 2)$, $\\overline{\\alpha} = \\frac{1}{2}, k = 1$\n$f(x') = 16$\n#### Шаг 1\n$S^k = -\\nabla f(x_{1},x_{2})$\nПодставляем и получаем при $x': S^1 = \\begin{pmatrix} -16 \\\\ 0 \\\\ \\end{pmatrix}$ \n### Шаг 2\n$\\alpha_{k} = \\overline{\\alpha}$\n#### Шаг 3\n$x^{k+1} = x^k + \\alpha_{k}S^k$\n$x^2 = \\begin{pmatrix} 2 \\\\ 2 \\\\ \\end{pmatrix} + \\frac{1}{2}\\begin{pmatrix} -16 \\\\ 0 \\\\ \\end{pmatrix} = \\begin{pmatrix} -6 \\\\ 2 \\\\ \\end{pmatrix}$\n$f(x^{2}) = 144$\n#### Шаг 4\n$S^2 = \\begin{pmatrix} 48 \\\\ 0 \\\\ \\end{pmatrix}$\n$x^3 = \\begin{pmatrix} -6 \\\\ 2 \\\\ \\end{pmatrix} + \\frac{1}{2}\\begin{pmatrix} 48 \\\\ 0 \\\\ \\end{pmatrix} = \\begin{pmatrix} 18 \\\\ 2 \\\\ \\end{pmatrix}$\n#### Шаг 5\n$S^3 = \\begin{pmatrix} -144 \\\\ 0 \\\\ \\end{pmatrix}$\n$x^4 = \\begin{pmatrix} 18 \\\\ 2 \\\\ \\end{pmatrix} + \\frac{1}{2}\\begin{pmatrix} -144 \\\\ 0 \\\\ \\end{pmatrix} = \\begin{pmatrix} -54 \\\\ 2 \\\\ \\end{pmatrix}$\n---\n## Алгоритм с  дроблением шага\n\n#### Шаг 0\n$x_{1} -$ произвольное, $x^1 = (2, 2)$, $\\overline{\\alpha} = \\frac{1}{2}, k = 1$\n$f(x^1) = 16$\n\n#### Шаг 1\n$S^k = -\\nabla f(x_{1},x_{2})$\nПодставляем и получаем при $x': S^1 = \\begin{pmatrix} -16 \\\\ 0 \\\\ \\end{pmatrix}$ \n#### Шаг 2\n$x = x^1 + \\overline{\\alpha}S^1 = \\begin{pmatrix} -6 \\\\ 2 \\\\ \\end{pmatrix}$\n$f(x) = 144$\n\n#### Шаг 3 (не выполняется)\n$f(x) < f(x^1)$\n\n#### Шаг 4 (выполняется)\n$f(x) \\geq f(x^1) \\implies \\alpha = \\overline{\\frac{\\alpha}{2}} = \\frac{1}{4}$\n\n#### Шаг 5\n$x = x^1 + \\overline{\\alpha}S^1 = \\begin{pmatrix} -2 \\\\ 2 \\\\ \\end{pmatrix}$\n$f(x) = 16 \\implies f(x) = f(x^1)$\n\n#### Шаг 6 (не выполняется, см. Шаг 3)\n\n#### Шаг 7 (выполняется)\n$f(x) \\geq f(x^1) \\implies \\alpha = \\overline{\\frac{\\alpha}{2}} = \\frac{1}{8}$\n\n#### Шаг 8\n$x = x^1 + \\overline{\\alpha}S^1 = \\begin{pmatrix} 0 \\\\ 2 \\\\ \\end{pmatrix}$\n$f(x) = 0 \\implies f(x) < f(x_{1})$\n\n#### Шаг 9\n$x^{k+1} = x = \\begin{pmatrix} 0 \\\\ 2 \\\\ \\end{pmatrix}$\n$||x^{k+1} - x^k|| = ||\\begin{pmatrix} 0 \\\\ 2 \\\\ \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 2 \\\\ \\end{pmatrix}|| = 2$\n#### Шаг 10\nНорма $= 2 > \\epsilon$\n$S^2 = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\end{pmatrix}$\n$x = x^2 + \\overline{\\alpha}S^2 = \\begin{pmatrix} 0 \\\\ 2 \\\\ \\end{pmatrix}$\n$f(x_{2}) = f(x) = 0$\nЕсли точка не меняется - значит, это точка минимума\n$\\dots$\n---\n### Метод наискорейшего спуска\n#### Шаг 0\n$x_{1} -$ произвольное, $x^1 = (2, 2)$, $\\overline{\\alpha} = \\frac{1}{2}, k = 1$\n$f(x^1) = 16$\n",
    "created_at": "2025-03-27T13:57:36.234386"
  }
]