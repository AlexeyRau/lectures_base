[
  {
    "id": "1743060933.131187",
    "subject": "Машинное обучение и большие данные",
    "date": "2025-02-21",
    "title": "Лекция 21.02.25",
    "content": "## Меры центральной тенденции\n\n### **Пример**: \n\nРассмотрим показатели силы ветра по шкале Бофорта:  \n$[0, 2, 2, 1, 1, 3, 3, 1, 1, 0, 0, 1, 2]$ \n\nВариационный ряд по возрастанию:  \n$[0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 3, 3]$  \n\nМедиана (Me) = 1 – тихий ветер.\n\n| Сила ветра | Описание |\n|------------|----------|\n| 0 Галлов  | Штиль    |\n| 1 Галл    | Тихий ветер |\n| 2 Галла   | Лёгкий ветер |\n| 3 Галла   | Слабый ветер |\n| 4 Галла   | Умеренный ветер |\n| 5 Галлов  | Свежий ветер |\n| 6 Галлов  | Сильный ветер |\n| 7 Галлов  | Крепкий ветер |\n| 8 Галлов  | Очень крепкий ветер |\n| 9 Галлов  | Шторм |\n| 10 Галлов | Сильный шторм |\n| 11 Галлов | Жестокий шторм |\n| 12 Галлов | Ураган |\n\n---\n\n## Проблема выбросов\n\n**Выбросы** – это значения, которые значительно отличаются от остальных данных. Они могут негативно повлиять на среднее значение, но медиана более устойчива к выбросам.\n\n- **Выбросы** могут быть вызваны ошибками в данных или редкими событиями.\n- Для обнаружения выбросов можно использовать визуализацию или сравнение медианы и моды.\n\n---\n\n## Меры разброса (изменчивости)\n\nМеры разброса описывают, насколько данные отклоняются от центральной тенденции.\n\n- **Max** – максимальное значение.\n- **Min** – минимальное значение.\n- **Range (Размах)** = Max - Min.\n- **Midrange** = (Max + Min) / 2.\n- **Inter-Quartile Range (IQR)** – межквартильный размах.\n- **Дисперсия** и **Стандартное отклонение**.\n\n---\n\n### Размах и квартильный размах\n\n**Размах** – разность между максимальным и минимальным значениями:  \n$$ R = X_{\\text{max}} - X_{\\text{min}} $$\n\n**Квартильный размах** – разница между третьим и первым квартилями:  \n$$ IQR = Q_3 - Q_1 $$\n\nКвартили делят данные на четыре равные части:\n-  $(Q_1) \\text{ - 25\\% квартиль}$\n- $( Q_2 ) \\text{ - 50\\% квартиль}$\n- $( Q_1 ) \\text{ - 25\\% квартиль}$\n\n---\n\n## Дисперсия и стандартное отклонение\n\n**Дисперсия** измеряет отклонение значений от среднего:  \n$$ \\sigma^2 = \\frac{\\sum_{i=1}^n (x_i - \\overline{x})^2}{n} $$\n\n**Стандартное отклонение** – это квадратный корень из дисперсии:  \n$$ \\sigma = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\overline{x})^2}{n}} $$\n\nСтандартное отклонение выражает изменчивость в исходных единицах измерения.\n\n---\n\n## Нормальное распределение\n\nНормальное распределение (колоколообразная кривая) характеризуется следующими свойствами:\n- Около 68% данных лежат в пределах одного стандартного отклонения от среднего:  \n  $$ \\overline{x} \\pm 1\\sigma $$\n- Около 95% данных лежат в пределах двух стандартных отклонений:  \n  $$ \\overline{x} \\pm 2\\sigma $$\n- Около 99.7% данных лежат в пределах трех стандартных отклонений:  \n  $$ \\overline{x} \\pm 3\\sigma $$\n\n---\n\n## Нормализация и стандартизация данных\n\n**Нормализация** – преобразование данных к диапазону $[0, 1]$:  \n$$ X_{\\text{norm}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} $$\n\n**Стандартизация** – преобразование данных к среднему значению 0 и стандартному отклонению 1:  \n$$ Z = \\frac{x_i - \\overline{x}}{s} $$\n\n---\n\n## Визуализация данных\n\n### Гистограмма\nГистограмма показывает распределение данных по интервалам (бинам). Она используется для непрерывных переменных.\n\n### Диаграмма рассеяния (Scatter Plot)\nДиаграмма рассеяния показывает взаимосвязь между двумя переменными. Она помогает выявить корреляции.\n\n### Тепловая карта (Heat Map)\nТепловая карта используется для визуализации корреляций между переменными. Коэффициент корреляции находится в интервале $[-1, 1]$.\n\n---\n\n## Обработка пропущенных значений\n\n1. **Удаление пропущенных значений** – если пропусков мало.\n2. **Замена средним значением** – если нет выбросов.\n3. **Замена медианой** – если есть выбросы.\n4. **Замена модой** – для категориальных данных.\n5. **Регрессия** – для прогнозирования пропущенных значений.\n\n---\n\n## Ключевые идеи\n\n- **Среднее значение** – типичное значение в наборе данных, но не устойчиво к выбросам.\n- **Медиана** – центральное значение, устойчиво к выбросам.\n- **Мода** – наиболее часто встречающееся значение.\n- **Дисперсия** и **стандартное отклонение** – меры разброса данных.\n- **Нормализация** и **стандартизация** – важные этапы предобработки данных.\n- **Визуализация** – помогает понять структуру и взаимосвязи в данных.\n\n---\n\n## Инструменты визуализации\n\n- **Matplotlib** – библиотека для построения графиков.\n- **Seaborn** – библиотека для статистической визуализации.\n\n---\n\n## Пример кода для стандартизации и нормализации\n\n```python\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nimport numpy as np\n\n# Пример данных\ndata = np.array([[1, 2], [3, 4], [5, 6]])\n\n# Стандартизация\nscaler = StandardScaler()\nstandardized_data = scaler.fit_transform(data)\n\n# Нормализация\nminmax_scaler = MinMaxScaler()\nscaled_data = minmax_scaler.fit_transform(data)\n\nprint(\"Стандартизированные данные:\\n\", standardized_data)\nprint(\"Нормализованные данные:\\n\", scaled_data)",
    "created_at": "2025-03-27T13:35:33.131187"
  },
  {
    "id": "1743060984.569695",
    "subject": "Машинное обучение и большие данные",
    "date": "2025-03-14",
    "title": "Лекция 14.03.25",
    "content": "## Введение в линейную регрессию\n\nЛинейная регрессия — это метод моделирования зависимости между зависимой переменной (откликом) и одной или несколькими независимыми переменными (предикторами). \n\nОсновная цель — построить математическую модель, которая наилучшим образом описывает связь между переменными.\n\n### Общее уравнение линейной регрессии:\n$$ y = ax + b $$\n\nВ Data Science и машинном обучении это уравнение часто записывается как:\n$$ Y = b_0 + b_1 X $$\n\n### Описание параметров:\n- $y$ или $Y$ — переменная ответа (зависимая переменная).\n- $x$ или $X$ — предикторная переменная (независимая переменная).\n- $a$ и $b$ или $b_0$ и $b_1$ — коэффициенты регрессии:\n\t  - $b_0$ — пересечение (intercept).\n\t  - $b_1$ — наклон (slope).\n\n## Ключевые термины\n\n- **Отклик (Response)**: Переменная, которую пытаемся предсказать. Cинонимы: зависимая переменная, целевая переменная.\n  \n- **Независимая переменная (Independent Variable)**: Переменная, используемая для предсказания отклика. Синонимы: предиктор, признак.\n  \n- **Запись (Record)**: Вектор, состоящий из значений предикторов и значений отклика для одного элемента данных. Синонимы: строка, случай, пример\n\n- **Пересечение (Intercept)**: Предсказанное значение, когда $X = 0$. Синонимы: $b_0$, точка пересечения.\n  \n- **Коэффициент регрессии (Regression Coefficient)**: Наклон регрессионной прямой. Синонимы: наклон, $b_1$, вес.\n\n## Линейные модели регрессии\n\nЛинейные модели регрессии используются для прогнозирования непрерывных величин. В зависимости от количества признаков, выделяют:\n\n- **Парная регрессия** (однофакторная):\n  $$ y = w x + b $$\n\n- **Множественная регрессия** (многофакторная):\n  $$ y = w_1 x_1 + w_2 x_2 + \\ldots + w_n x_n + b $$\n\n### Постановка задачи регрессии\n\nДано: коллекция размеченных данных $\\{(x_i, y_i)\\}_{i=1}^N$, где $N$ — размер коллекции, $x_i$ — $D$-мерный вектор признаков образца $i = 1, ..., N$, $y_i$ — действительное целевое значение.\n\nТребуется: построить модель $f_{w,b}(x)$, которая является линейной комбинацией признаков:\n$$ f_{w,b}(x) = xw + b $$\n\nгде:\n- $w$ — вектор параметров (весов).\n- $b$ — свободный коэффициент (сдвиг).\n\n## Метод наименьших квадратов (МНК)\n\nМетод наименьших квадратов (МНК) используется для нахождения оптимальных параметров линейной регрессии, минимизирующих сумму квадратов ошибок (остатков):\n\n$$ \\min_{w, b} \\sum_{i=1}^n (w^T x_i + b - y_i)^2 $$\n\n### Функция потерь (MSE)\nСреднеквадратичная ошибка (MSE) вычисляется как:\n$$ MSE = \\frac{1}{m} \\sum_{i=1}^m (y_i - \\hat{y}_i)^2 $$\n\n### Коэффициент детерминации $R^2$\nКоэффициент детерминации $R^2$ показывает, насколько хорошо модель объясняет дисперсию данных:\n$$ R^2 = 1 - \\frac{\\sum_{i=1}^n (\\hat{y}_i - y_i)^2}{\\sum_{i=1}^n (\\bar{y}_i - y_i)^2} $$\n\n## Регуляризация\n\nРегуляризация используется для предотвращения переобучения модели. Основные виды регуляризации:\n\n1. **L1-регуляризация (Lasso)**:\n   $$ \\min_{w, b} \\left[ \\frac{1}{N} \\sum_{i=1}^N (y_i - f_{w,b}(X_i))^2 + \\lambda \\sum_{j=1}^D |w_j| \\right] $$\n\n2. **L2-регуляризация (Ridge)**:\n   $$ \\min_{w, b} \\left[ \\frac{1}{N} \\sum_{i=1}^N (y_i - f_{w,b}(X_i))^2 + \\lambda \\sum_{j=1}^D w_j^2 \\right] $$\n\n3. **Elastic Net** (комбинация L1 и L2):\n   $$ \\min_{w, b} \\left[ \\frac{1}{N} \\sum_{i=1}^N (y_i - f_{w,b}(X_i))^2 + \\lambda_1 \\sum_{j=1}^D |w_j| + \\lambda_2 \\sum_{j=1}^D w_j^2 \\right] $$\n\n## Метрики качества модели\n\nДля оценки качества модели регрессии используются следующие метрики:\n\n- **MAE (Mean Absolute Error)**:\n  $$ MAE = \\frac{1}{m} \\sum_{i=1}^m |y_i - \\hat{y}_i| $$\n\n- **RMSE (Root Mean Squared Error)**:\n  $$ RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2} $$\n\n- **MAPE (Mean Absolute Percentage Error)**:\n  $$ MAPE = \\frac{1}{n} \\sum_{i=1}^n \\frac{|y_i - \\hat{y}_i|}{|y_i|} \\times 100\\% $$\n",
    "created_at": "2025-03-27T13:36:24.569695"
  },
  {
    "id": "1743061009.628635",
    "subject": "Машинное обучение и большие данные",
    "date": "2025-02-14",
    "title": "Лекция 14.02.25",
    "content": "## Ключевые идеи\n- **EDA** — итеративный процесс, который может потребовать возвращения к предыдущим этапам по мере получения новых инсайтов из данных.\n  - Это означает, что EDA не является линейным процессом. По мере анализа данных могут возникать новые вопросы, которые потребуют повторного изучения уже пройденных этапов.\n- Основная цель EDA — лучше понять данные и подготовить их для последующего анализа или моделирования.\n  - EDA помогает выявить закономерности, аномалии и взаимосвязи в данных, что важно для построения точных моделей.\n---\n## Размер и размерность данных\n- **Размер данных** относится к числу объектов данных.\n  - Например, в датасете с информацией о студентах размер данных будет равен количеству студентов.\n- **Размерность данных** относится к числу атрибутов (признаков).\n  - Например, если у каждого студента есть возраст, пол и оценка, то размерность данных будет равна 3.\n---\n## Типы данных\n### Числовые (Numeric)\n- Выражены числами (например, возраст, вес, размер обуви).\n  - **Непрерывные**: неограниченное число вариантов (например, возраст, вес, давление).\n    - Эти данные могут принимать любое значение в пределах определенного диапазона.\n  - **Дискретные**: ограниченное число вариантов (например, размер обуви, количество учеников).\n    - Эти данные принимают только целые значения.\n\n### Категориальные\n- Выражены словами (например, цвет глаз, пол, группа крови).\n  - **Порядковые (Ordinal)**: имеют иерархию (например, настроение, оценка качества обслуживания).\n    - Эти данные можно упорядочить, но разница между значениями не количественная.\n  - **Номинальные (Nominal)**: не имеют иерархии (например, цвет глаз, группа крови).\n    - Эти данные представляют собой просто категории без какого-либо порядка.\n---\n## Порядковые данные (Ordinal Data)\n- Значения имеют осмысленный порядок (например, оценки: A, B, C, D; размеры порций: маленькие, средние, большие).\n  - Например, оценка \"A\" лучше, чем \"B\", но мы не можем точно сказать, насколько лучше.\n- Количественной разницы между двумя уровнями не обнаружено.\n  - Это означает, что разница между \"A\" и \"B\" может быть не такой же, как между \"B\" и \"C\".\n---\n## Выбор признаков\n- Удаление признаков, не имеющих отношения к задаче (например, Student ID).\n  - Например, если мы анализируем доходы студентов, то идентификатор студента (Student ID) не имеет значения для анализа.\n  - Удаление лишних признаков помогает упростить модель и улучшить её производительность.\n---\n## Основы описательной статистики\n### Меры центральной тенденции\n- Данные распределены вокруг этого «центра».\n  - Эти меры помогают понять, где находится \"центр\" данных.\n- Вычисляется для каждого атрибута.\n- Три распространенных типа:\n  - **Среднее**: сумма всех значений, деленная на их количество.\n  - **Мода**: наиболее часто встречающееся значение.\n  - **Медиана**: значение, которое находится в середине упорядоченного набора данных.\n- Эти меры не дают информации относительно экстремальных значений в распределении данных и разброса данных.\n  - Например, среднее может быть искажено выбросами.\n\n### Среднее арифметическое\n- Чувствительно к выбросам.\n  - Например, если в данных есть одно очень большое значение, среднее будет значительно выше, чем большинство значений в наборе данных.\n---\n## Этапы EDA\n1. **Изучение распределения данных.**\n   - Это помогает понять, как данные распределены и есть ли в них выбросы.\n2. **Обработка пропущенных значений** (наиболее распространенная проблема в каждом датасете).\n   - Пропущенные значения могут быть заполнены средним значением, медианой или удалены.\n3. **Обработка отклонений.**\n   - Выбросы могут быть удалены или заменены на более репрезентативные значения.\n4. **Удаление дубликатов данных.**\n   - Дубликаты могут исказить результаты анализа.\n5. **Кодирование категориальных переменных.**\n   - Категориальные данные могут быть преобразованы в числовые для использования в моделях.\n6. **Нормализация и масштабирование.**\n   - Это помогает привести данные к одному масштабу, что важно для многих алгоритмов машинного обучения.\n---\n## Двумерный анализ\n- Исследует отношения между двумя переменными.\n  - Например, можно исследовать связь между возрастом и доходом.\n- Помогает находить корреляции, отношения и зависимости между парами переменных.\n- Используемые методы:\n  - **Диаграммы рассеяния**: визуализация зависимости между двумя переменными.\n  - **Регрессионный анализ**: позволяет предсказать одну переменную на основе другой.\n  - **Корреляционный анализ**: измеряет силу и направление связи между переменными.\n---\n## Предварительная обработка данных\n- **Очистка данных**: удаление дубликатов, обработка пропущенных значений, исправление ошибок в данных.\n  - Это важный этап, так как качество данных напрямую влияет на качество анализа.\n- **Преобразование данных**: изменение формата данных, кодирование категориальных переменных, нормализация или стандартизация числовых признаков.\n  - Например, категориальные данные могут быть преобразованы в числовые с помощью one-hot encoding.\n---\n## Визуализация данных\n- Построение графиков и диаграмм для визуального представления данных.\n  - **Гистограммы**: показывают распределение данных.\n  - **Ящики с усами (Box Plots)**: показывают медиану, квартили и выбросы.\n  - **Диаграммы рассеяния (Scatter Plots)**: показывают связь между двумя переменными.\n  - **Тепловые карты (HeatMaps)**: показывают интенсивность значений в таблице данных.\n- Визуализация помогает выявить закономерности, аномалии и взаимосвязи между переменными.\n  - Например, тепловая карта может показать, какие переменные наиболее сильно коррелируют друг с другом.\n---\n",
    "created_at": "2025-03-27T13:36:49.628635"
  },
  {
    "id": "1743061044.507574",
    "subject": "Машинное обучение и большие данные",
    "date": "2025-03-07",
    "title": "Лекция 07.03.25",
    "content": "### Что такое машинное обучение?\nМашинное обучение (ML) — это процесс разработки алгоритмов или моделей, которые могут предсказывать значения целевой переменной (Y) для новых объектов (x) на основе обучающих данных.\n\n### Модель алгоритма\n- Модель алгоритма$a$— это параметрическое семейство функций$g : X \\to Y$или $g(x, \\theta)$, где $\\theta \\in \\Theta$— параметры.\n- Процесс подбора оптимальной функции$g$и параметров$\\theta$называется **настройкой** или **обучением** алгоритма.\n\n### Признаковое описание объекта\n- Признаки объекта $x$ можно записать в виде вектора: $x_1, ..., x_d$.\n- Признаковое пространство определяется количеством признаков. Например, три признака образуют трехмерное пространство.\n\n### Обучающая выборка\n- Обучающая выборка — это набор объектов $x$, для которых известны значения целевой переменной $y$.\n- Обучающая выборка может быть разделена на:\n  - Тренировочную выборку (для обучения модели).\n  - Валидационную выборку (для настройки гиперпараметров).\n  - Тестовую выборку (для оценки качества модели).\n\n---\n## Обучение с учителем (Supervised Learning)\n\n### Основные понятия\n- В обучении с учителем данные представлены в виде пар $(x_i, y_i)$, где $x_i$— вектор признаков, а $y_i$ — метка.\n- Метка $y_i$ может быть элементом конечного множества классов, вещественным числом или более сложной структурой.\n\n### Цель обучения с учителем\n- Цель — создать модель, которая принимает вектор признаков $x$ и возвращает информацию, позволяющую определить метку для этого вектора.\n- Пример: модель, предсказывающая вероятность заболевания раком на основе характеристик пациента.\n\n### Оценщики (Estimators)\n- В Scikit-Learn модели называются **оценщиками**.\n- Основные методы оценщиков:\n  - `fit(X, y)` — обучение модели.\n  - `predict(X)` — предсказание на новых данных.\n  - `transform(X)` — преобразование данных.\n\n---\n## Классификация и регрессия\n\n### Классификация (Classification)\n- **Целевая переменная (target)** — дискретная.\n- Прогнозируется метка класса из заранее определенного списка.\n- Примеры:\n  - Бинарная классификация: $Y = \\{0, 1\\}$ (да/нет, положительный/отрицательный).\n  - Многоклассовая классификация: $Y = \\{0, 1, 2, ...\\}$.\n\n### Регрессия (Regression)\n- **Целевая переменная (target)** — непрерывная.\n- Прогнозируется вещественное число.\n- Пример: предсказание стоимости квартиры на основе её характеристик.\n\n---\n## Процесс машинного обучения\n\n### Этапы машинного обучения\n1. **Обучение алгоритма**:\n   - Модель изучает данные с известными ответами и учится делать предсказания.\n   - Цель: минимизировать отклонение предсказаний $a(x)$ от правильных ответов $y$.\n\n2. **Применение алгоритма**:\n   - Обученная модель используется для предсказания на новых данных.\n   - Пример: предсказание стоимости квартир, для которых неизвестна цена.\n\n### Разделение данных\n- Данные делятся на:\n  - Обучающую выборку $(X_{train}, y_{train})$.\n  - Тестовую выборку $(X_{test}, y_{test})$.\n- Модель обучается на обучающей выборке и оценивается на тестовой.\n\n---\n## Гиперпараметры и проблемы машинного обучения\n\n### Гиперпараметры модели\n- **Гиперпараметры** — параметры, задаваемые до начала обучения (например, шаг градиентного спуска).\n- **Параметры модели** — параметры, оптимизируемые в процессе обучения (например, веса в нейронной сети).\n\n### Основные проблемы ML\n1. **Недостаточный объем обучающей выборки**.\n2. **Пропуски в данных**.\n3. **Переобучение (Overfitting)**:\n   - Модель слишком сложна и хорошо работает на обучающих данных, но плохо на новых.\n4. **Недобучение (Underfitting)**:\n   - Модель слишком проста и не может хорошо описать данные.\n\n---\n## Линейная регрессия\n\n### Понятие линейной регрессии\n- Линейная регрессия — это статистический метод, моделирующий линейную зависимость между зависимой переменной и одной или несколькими независимыми переменными.\n- Математически линейная зависимость представляет прямую линию на графике.\n\n### Регрессионный анализ\n- **Регрессионный анализ** — инструмент для установления модели отношений между переменными.\n- **Линейная регрессия**: зависимая и независимая переменные связаны через уравнение, где показатель степени равен 1.\n- **Нелинейная регрессия**: описывает нелинейные отношения, где показатель степени любой переменной не равен 1.\n\n### Типы регрессии\n- **Простая линейная регрессия**: одна независимая переменная.\n- **Множественная линейная регрессия**: несколько независимых переменных.\n- **Полиномиальная регрессия**: нелинейная зависимость, описываемая полиномом.\n\n---\n",
    "created_at": "2025-03-27T13:37:24.507574"
  }
]