Дата,Название лекции,Содержание лекции
"2025-02-05","первая лекция оаип","### *Выстрел*
User Story - как игрок могу приказать кораблю сделать выстрел, чтобы запустить ракету
Команда - выстрелить
Событие - ракета начала движения
Агрегат - сама игра

---
### *Класс Game*
Необходимо указать игровой объект (космический корабль)
1) Авторизовать пользователя (может ли он приказывать)
2) Поиск игрового объекта
3) Создать необходимый объект (фотонная ракета)
4) Обновить состояние объекта
Выстрел (GameItem ID, User ID)
Обновить состояние игры - возврат от выстрела

---
До : - имя объекта
После : - имя потенциального класса
_ - экземпляр класса
x - объект уничтожен
-> - объект создается
district - приказ

---
### *Класс Auth*
Game использует для авторизации выстрела
Не создается и не умирает в процессе взаимодействия
Внутри выстрела происходит авторизация - ответ либо да, либо нет
Проверить возможность стрельбы в данный момент

---
### *Объект Репозиторий* 
Часть класса Game - стратегия в IOC контейнере
Хранилище существующих объектов
Предоставить группа объектов по входящему запросу (Game Item)
Game хранит Game Item в репозитории

---
Фабрика + IOC
Причем IOC не вложен в Фабрику
Создает фотонную ракету (Game Item)
Возвращается Game Object

---
### *Команды*
Объект Order - key:value
AuthCommand(Приказ) - если Game может обработать его -> Execute(), иначе - Исключение
IAuthOrder

---
### *Стратегии*
- Найти игровой объект - стратегия IOC, которая имеет ссылку на словарь Dict:
	1) Ключ - строковый ID объекта
	2) Значение - сам игровой объект
	3) Объекты хранить через тип Object
	4) В игровом объекте должна быть строка с ID
- Выстрел:
	1) Стратегия IOC для создания нового игрового объекта
	2) Предполагаем, что эта стратегия создает словарь с 1! парой ID
	3) Необходимо получить от космического корабля:
	    - Cтартовое положение ракеты
	    - Направление движения ракеты
	    - Модуль скорости ракеты
	4) Необходимо установить свойства созданной ракеты в соответствии с параметрами выше - это можно сделать с помощью стратегии IOC, которая меняет состояние игрового объекта
	5) Необходимо добавить созданную и проинициализированную ракету в репозиторий игровых объектов - для этого нужна стратегия IOC
	6) Создать команду движения для ракеты
	7) Добавить эту команду в очередь всех остальных команд"
"2025-10-11","вторая лекция оаип","### *Выстрел*
User Story - как игрок могу приказать кораблю сделать выстрел, чтобы запустить ракету
Команда - выстрелить
Событие - ракета начала движения
Агрегат - сама игра

---
### *Класс Game*
Необходимо указать игровой объект (космический корабль)
1) Авторизовать пользователя (может ли он приказывать)
2) Поиск игрового объекта
3) Создать необходимый объект (фотонная ракета)
4) Обновить состояние объекта
Выстрел (GameItem ID, User ID)
Обновить состояние игры - возврат от выстрела

---
До : - имя объекта
После : - имя потенциального класса
_ - экземпляр класса
x - объект уничтожен
-> - объект создается
district - приказ

---
### *Класс Auth*
Game использует для авторизации выстрела
Не создается и не умирает в процессе взаимодействия
Внутри выстрела происходит авторизация - ответ либо да, либо нет
Проверить возможность стрельбы в данный момент

---
### *Объект Репозиторий* 
Часть класса Game - стратегия в IOC контейнере
Хранилище существующих объектов
Предоставить группа объектов по входящему запросу (Game Item)
Game хранит Game Item в репозитории

---
Фабрика + IOC
Причем IOC не вложен в Фабрику
Создает фотонную ракету (Game Item)
Возвращается Game Object

---
### *Команды*
Объект Order - key:value
AuthCommand(Приказ) - если Game может обработать его -> Execute(), иначе - Исключение
IAuthOrder

---
### *Стратегии*
- Найти игровой объект - стратегия IOC, которая имеет ссылку на словарь Dict:
	1) Ключ - строковый ID объекта
	2) Значение - сам игровой объект
	3) Объекты хранить через тип Object
	4) В игровом объекте должна быть строка с ID
- Выстрел:
	1) Стратегия IOC для создания нового игрового объекта
	2) Предполагаем, что эта стратегия создает словарь с 1! парой ID
	3) Необходимо получить от космического корабля:
	    - Cтартовое положение ракеты
	    - Направление движения ракеты
	    - Модуль скорости ракеты
	4) Необходимо установить свойства созданной ракеты в соответствии с параметрами выше - это можно сделать с помощью стратегии IOC, которая меняет состояние игрового объекта
	5) Необходимо добавить созданную и проинициализированную ракету в репозиторий игровых объектов - для этого нужна стратегия IOC
	6) Создать команду движения для ракеты
	7) Добавить эту команду в очередь всех остальных команд"
"2025-03-06","Лекция такая то","# Статистическое оценивание параметров
---
## Неравенство Рао – Крамера

### Плотность распределения
Плотность распределения $p(x, \theta)$ может быть определена как:
$$
p(x, \theta) = 
\begin{cases} 
p(x, \theta), & \text{если распределение непрерывно,} \\
P_\theta (X = y), & \text{если распределение дискретно.}
\end{cases}
$$

### Вклад выборки
Вклад выборки в информационное количество Фишера выражается через производную логарифма плотности распределения:
$$
\frac{\partial \ln p(x_1, x_2, \dots, x_n, \theta)}{\partial \theta}
$$
### Вклад одного элемента выборки:
$$
\frac{\partial \ln p(x,\theta)}{\partial \theta}
$$
### Информационное количество Фишера о параметре $\theta$:
$$
I = M \left( \frac{\partial \ln p(x_1, x_2, \dots, x_n, \theta)}{\partial \theta} \right)^2
$$

### Условия регулярности
1. Множество значений $x$, для которых $p(x; \theta) \neq 0$, не зависит от $\theta$.
2. $p(x; \theta)$ дифференцируема по параметру $\theta$.
3. Оцениваемая функция $\tau(\theta)$ дифференцируема по параметру $\theta$.

### Неравенство Рао – Крамера
При выполнении условий регулярности для любой оценки выполняется неравенство:
$$
D\hat\theta \geq \frac{1}{I}
$$
### Доказательство

1. По свойству плотности:

$$
\int p \, dx = 1
$$

2. Из несмещенности оценки:

$$
\int \hat{\theta} p \, dx = M \hat{\theta} = \theta
$$

3. Продифференцируем (1) по параметру:

$$
\int \frac{\partial p}{\partial \theta} \, dx = 0
$$

4. Домножим (3) на $\theta$:

$$
\int \theta \frac{\partial p}{\partial \theta} \, dx = 0
$$

5. Продифференцируем (2) по параметру:

$$
\int \hat{\theta}\frac{\partial p}{\partial \theta} \, dx = 1
$$

6. Вычтем (5) - (4):

$$
\int (\hat{\theta} - \theta) \frac{\partial p}{\partial \theta} \, dx = 1
$$

7. Подставим выражение для $\partial p / \partial \theta$:

$$
\int (\hat{\theta} - \theta) \frac{\partial \ln p}{\partial \theta} \, p \, dx = 1
$$

8. Из курса теории вероятностей известно:

$$
M [(\varphi_1 - M \varphi_1)(\varphi_2 - M \varphi_2)] = \text{cov}(\varphi_1, \varphi_2)
$$

9. Из свойства коэффициента корреляции вытекает неравенство Коши – Буняковского:

$$
| \text{cov} (\varphi_1, \varphi_2) | \leq \sqrt{D \varphi_1 D \varphi_2}
$$

$$
1 \leq \sqrt{D \hat\theta D \left( \frac{\partial \ln p}{\partial \theta} \right)} \Rightarrow D \hat{\theta} \geq \frac{1}{D \left( \frac{\partial \ln p}{\partial \theta} \right)}
$$

$$
D \left( \frac{\partial \ln p}{\partial \theta} \right) = M \left( \frac{\partial \ln p}{\partial \theta} \right)^2 - M^2 \left( \frac{\partial \ln p}{\partial \theta} \right) = M \left( \frac{\partial \ln p}{\partial \theta} \right)^2 = I
$$

$$
\Rightarrow D \hat{\theta} \geq \frac{1}{I}
$$

---

### Следствие: 
Равенство достигается, если $\theta^*$ и $\partial \ln p / \partial \theta$ линейно зависимы.

---
## Формы информационного количества Фишера

### Тождество для информационного количества:
$$
\frac{\partial \ln p}{\partial \theta} = \frac{\partial p}{\partial \theta} \frac{1}{p}
$$

### Вторая производная логарифма плотности:
$$
\frac{\partial^2 \ln p}{\partial \theta^2} = -\frac{1}{p^2} \left( \frac{\partial p}{\partial \theta} \right)^2 + \frac{1}{p} \frac{\partial^2 p}{\partial \theta^2}
$$

### Информационное количество Фишера через вторую производную:
$$
I = -M \left( \frac{\partial^2 \ln p}{\partial \theta^2} \right)
$$

### Формы для одномерной плотности $p(x, \theta)$:
$$
I = nM \left( \frac{\partial \ln p(x, \theta)}{\partial \theta} \right)^2
$$
$$
I = -nM \left( \frac{\partial^2 \ln p(x, \theta)}{\partial \theta^2} \right)
$$
---
## Эффективные оценки

### Определение эффективной оценки
Несмещенная оценка $\hat{\theta}$ параметра $\theta$ называется **эффективной**, если для любого $\theta \in \Theta$ выполняется:
$$
D\hat{\theta} = \frac{1}{I}
$$
### Замечание.
Если оценка является эффективной, то она является оптимальной.
Обратное не верно

### Пример эффективной оценки
Для распределения Пуассона $P_\lambda$ оценка $\hat{\lambda} = \bar{X}$ является эффективной, так как:
$$
D\bar{X} = \frac{\lambda}{n} = \frac{1}{I}
$$
### Показатель эффективности
Показатель эффективности несмещенной оценки $\hat{\theta}$ параметра $\theta$ называется число:
$$
e(\hat{\theta}) = \frac{1}{I \cdot D\hat{\theta}},\ 0<e(\hat\theta) \leq 1
$$
Для эффективных оценок $e(\hat{\theta}) = 1$.

---
## Метод максимального правдоподобия
Пусть $\xi$ - непрерывная случайная величина с плотностью $p(x, \theta)$, где $\theta$ - неизвестный параметр..

Тогда плотность распределения вектора $\overline{X}$:
$$
p(x_{1}\dots x_{n},\theta) = p(x_{1}, \theta)* \dots * p(x_{n}, \theta)
$$
### Функция правдоподобия
Для непрерывной случайной величины функция, рассматриваемая при фиксированных $(x_{1}\dots x_{n})$ как функция параметра $\theta$, называется функцией правдоподобия:
$$
L(x_1, \dots, x_n, \theta) = p(x_1, \theta) \cdot \dots \cdot p(x_n, \theta)
$$

Для дискретной случайной величины:
$$
L(x_1, \dots, x_n, \theta) = P(\xi = x_1) \cdot \dots \cdot P(\xi = x_n)
$$
### Суть метода
Метод максимального правдоподобия заключается в нахождении значения параметра $\theta$, которое максимизирует функцию правдоподобия $L(x_1, \dots, x_n, \theta)$.

### Оценка максимального правдоподобия (о.м.п.)
Оценка $\theta^*$, обеспечивающая по параметру $\theta$ максимум функции правдоподобия, называется **оценкой максимального правдоподобия** параметра $\theta$ (о.м.п.).

### Уравнение правдоподобия
Для нахождения оценки максимального правдоподобия решается уравнение:
$$
\frac{\partial \ln L}{\partial \theta} = 0
$$

После нахождения критической точки необходимо проверить, что это точка максимума:
$$
\frac{\partial^2 \ln L}{\partial \theta^2} < 0
$$

---
### Пример (нерегулярная модель)
Найти о.м.п. параметра $\theta = (a, b)$ в равномерном распределении $R[a, b]$.

### Решение
Функция правдоподобия для равномерного распределения:
$$
L(X, \theta) = \prod_{i=1}^n \frac{1}{b-a} = \frac{1}{(b-a)^n}
$$

Так как функция $L$ монотонна по $a$ и $b$, наибольшее значение достигается при минимально возможном значении $b$ и максимально возможном значении $a$.

Таким образом, оценки максимального правдоподобия для параметров $a$ и $b$:
$$
\hat{a} = x_{max} = x^*_1, \quad \hat{b} = x_{min} = x^*_n
$$

---
## Некоторые свойства оценок максимального правдоподобия

### Cвойство 1

Для нахождения оценки максимального правдоподобия (о.м.п.) можно выбирать наиболее удобную параметризацию, а о.м.п. получать затем с помощью соответствующих преобразований. Это свойство выражается следующим образом:

$$
\overline{\tau(\theta)} = \tau(\overline{\theta})
$$

### Пример:
Найдем о.м.п. параметра $\alpha^3$ в распределении $\Gamma_{\alpha,\beta}$ при известном $\beta$.

**Решение:**

$$
\alpha^3 = (\hat{\alpha})^3 = \left( \frac{\beta}{\overline{X}} \right)^3
$$

### Свойство 2 

Оценки максимального правдоподобия обладают следующими асимптотическими свойствами:

- **Асимптотическая несмещенность:** Оценки максимального правдоподобия становятся несмещенными при увеличении объема выборки.
- **Состоятельность:** Оценки максимального правдоподобия сходятся по вероятности к истинному значению параметра при увеличении объема выборки.
- **Асимптотическая нормальность:** При некоторых дополнительных условиях оценки максимального правдоподобия асимптотически нормальны.

### Свойство 3

Если оценки максимального правдоподобия асимптотически нормальны, то они также асимптотически эффективны, то есть:

$$
D\hat{\theta} \rightarrow \frac{1}{I}
$$

где $I$ — информационное количество Фишера.

---

## Метод моментов

Метод моментов заключается в приравнивании выборочных моментов к соответствующим теоретическим моментам распределения случайной величины $\xi$.

- Если распределение имеет **один параметр**, то составляют уравнение:

$$
M\xi = \bar{X}
$$

- Если распределение имеет **два параметра**, то составляют систему уравнений:

$$
\begin{cases} 
M\xi = \bar{X}, \\ 
D\xi = S^2
\end{cases}
$$
---
## Пример: оценки параметров для распределений

### 1. Показательное распределение

Для показательного распределения:

$$
M\xi = \frac{1}{\lambda} = \bar{X} \Rightarrow \lambda = \frac{1}{\bar{X}}
$$

### 2. Равномерное распределение

Для равномерного распределения:

$$
\begin{cases}
M\xi = \frac{a+b}{2} = \bar{X}, \\
D\xi = \frac{(b-a)^2}{12} = S^2
\end{cases}
$$

### 3. Нормальное распределение

Для нормального распределения оценки параметров находятся аналогичным образом.

---
## Некоторые свойства оценок метода моментов

### Теорема:
Пусть $\hat{\theta} = g(a_1, \ldots, a_k)$ — оценка параметра $\theta$, полученная по методу моментов, причем функция $g^{-1}$ непрерывна. Тогда $\hat{\theta}$ состоятельна.

### Доказательство:

Если $\hat{\theta} = g(a_1, \ldots, a_k)$, то $\theta = g(\alpha_1, \ldots, \alpha_k)$.

По свойству выборочных моментов:

$$
a_i \xrightarrow{P} \alpha_i \quad \text{при} \quad n \to \infty
$$

Тогда по теореме о сходимости по вероятности:

$$
g(a_1, \ldots, a_k) \xrightarrow{P} g(\alpha_1, \ldots, \alpha_k)
$$

Откуда:

$$
\hat{\theta} \xrightarrow{P} \theta
$$

---

## Метод наименьших
Оценка метода наименьших квадратов определяется из условия минимизации суммы квадратов отклонений выборочных данных от определяемой оценки.

### Пример:
Найти оценку метода наименьших квадратов $\hat{\theta}$ для генеральной средней $\theta = \bar{x}_0$.

**Решение:**

Минимизируем сумму квадратов отклонений:

$$
u = \sum_{i=1}^n (x_i - \theta)^2 \to \min
$$

Продифференцируем по $\theta$ и приравняем к нулю:

$$
\frac{du}{d\theta} = -2\sum_{i=1}^n (x_i - \theta) = 0 \implies \sum_{i=1}^n x_i - \theta n = 0 \implies \hat{\theta} = \frac{1}{n} \sum_{i=1}^n x_i = \bar{X}
$$

Таким образом, оценка метода наименьших квадратов для генеральной средней равна выборочному среднему $\bar{X}$."
